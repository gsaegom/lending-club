{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1115ed0f",
   "metadata": {},
   "source": [
    "# Predicting Sub-grade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b99e0",
   "metadata": {},
   "source": [
    "The next step is to predict the sub-grade assigned to a loan. Because we are\n",
    "going to use the same dataset as for the grade, we will just read the pickle\n",
    "file we previously generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "77ae53dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from pycaret.regression import *\n",
    "from sklearn.metrics import classification_report, confusion_matrix,make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from script import make_mi_scores, plot_mi_scores, max_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a04bcc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d98a87",
   "metadata": {},
   "source": [
    "Instead of using 35 different classes, we are going to convert them to numbers from 1 to 35 and treat this as an ordinal regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a20cc900",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_grade_mapping = {\n",
    "    'A1': 1, 'A2': 2, 'A3': 3, 'A4': 4, 'A5': 5,\n",
    "    'B1': 6, 'B2': 7, 'B3': 8, 'B4': 9, 'B5': 10,\n",
    "    'C1': 11, 'C2': 12, 'C3': 13, 'C4': 14, 'C5': 15,\n",
    "    'D1': 16, 'D2': 17, 'D3': 18, 'D4': 19, 'D5': 20,\n",
    "    'E1': 21, 'E2': 22, 'E3': 23, 'E4': 24, 'E5': 25,\n",
    "    'F1': 26, 'F2': 27, 'F3': 28, 'F4': 29, 'F5': 30,\n",
    "    'G1': 31, 'G2': 32, 'G3': 33, 'G4': 34, 'G5': 35\n",
    "}\n",
    "\n",
    "df['Sub_Grade_Numerical'] = df['Sub_Grade'].map(sub_grade_mapping).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b3e65a",
   "metadata": {},
   "source": [
    "For the EDA part, we are going to choose the 10 features with the highest mutual information score, just like we did for Grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "47be0af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = df.sample(frac=0.05, random_state=42).dropna()\n",
    "mi_scores = make_mi_scores(\n",
    "    sample.drop(['Sub_Grade', 'Interest_Rate', 'id', 'Sub_Grade_Numerical'],\n",
    "                axis=1)\n",
    "    .select_dtypes\n",
    "    (include=np.number), sample['Sub_Grade'])\n",
    "\n",
    "plt.figure(dpi=100, figsize=(10, 10))\n",
    "plot_mi_scores(mi_scores.nlargest(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c531bfb",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c465767",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 18\u001B[0m\n\u001B[0;32m     15\u001B[0m sns\u001B[38;5;241m.\u001B[39mkdeplot(x\u001B[38;5;241m=\u001B[39mdf[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSub_Grade_Numerical\u001B[39m\u001B[38;5;124m'\u001B[39m],hue\u001B[38;5;241m=\u001B[39mdf[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTerm_In_Months\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mstr\u001B[39m), ax\u001B[38;5;241m=\u001B[39maxes[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m3\u001B[39m])\n\u001B[0;32m     16\u001B[0m axes[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m3\u001B[39m]\u001B[38;5;241m.\u001B[39mset_title(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTerm in Months by Subgrade\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 18\u001B[0m \u001B[43msns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlineplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTotal_Bank_Card_Limit\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSub_Grade_Numerical\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m            \u001B[49m\u001B[43max\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxes\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m axes[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mset_title(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTotal Bank Card Limit Distribution\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     22\u001B[0m sns\u001B[38;5;241m.\u001B[39mlineplot(data\u001B[38;5;241m=\u001B[39mdf, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRevolving_Account_Ratio\u001B[39m\u001B[38;5;124m'\u001B[39m, x\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSub_Grade_Numerical\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m     23\u001B[0m             ax\u001B[38;5;241m=\u001B[39maxes[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\seaborn\\relational.py:645\u001B[0m, in \u001B[0;36mlineplot\u001B[1;34m(data, x, y, hue, size, style, units, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001B[0m\n\u001B[0;32m    642\u001B[0m color \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolor\u001B[39m\u001B[38;5;124m\"\u001B[39m, kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    643\u001B[0m kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolor\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m _default_color(ax\u001B[38;5;241m.\u001B[39mplot, hue, color, kwargs)\n\u001B[1;32m--> 645\u001B[0m \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43max\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    646\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ax\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\seaborn\\relational.py:441\u001B[0m, in \u001B[0;36m_LinePlotter.plot\u001B[1;34m(self, ax, kws)\u001B[0m\n\u001B[0;32m    438\u001B[0m     grouped \u001B[38;5;241m=\u001B[39m sub_data\u001B[38;5;241m.\u001B[39mgroupby(orient, sort\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msort)\n\u001B[0;32m    439\u001B[0m     \u001B[38;5;66;03m# Could pass as_index=False instead of reset_index,\u001B[39;00m\n\u001B[0;32m    440\u001B[0m     \u001B[38;5;66;03m# but that fails on a corner case with older pandas.\u001B[39;00m\n\u001B[1;32m--> 441\u001B[0m     sub_data \u001B[38;5;241m=\u001B[39m \u001B[43mgrouped\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43magg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mother\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mreset_index()\n\u001B[0;32m    442\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    443\u001B[0m     sub_data[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mother\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mnan\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1549\u001B[0m, in \u001B[0;36mGroupBy.apply\u001B[1;34m(self, func, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1547\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode.chained_assignment\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   1548\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1549\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_python_apply_general\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_selected_obj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1550\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   1551\u001B[0m         \u001B[38;5;66;03m# gh-20949\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m         \u001B[38;5;66;03m# try again, with .apply acting as a filtering\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1556\u001B[0m         \u001B[38;5;66;03m# fails on *some* columns, e.g. a numeric operation\u001B[39;00m\n\u001B[0;32m   1557\u001B[0m         \u001B[38;5;66;03m# on a string grouper column\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_group_selection_context():\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1601\u001B[0m, in \u001B[0;36mGroupBy._python_apply_general\u001B[1;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001B[0m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[0;32m   1565\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_python_apply_general\u001B[39m(\n\u001B[0;32m   1566\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1571\u001B[0m     is_agg: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1572\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NDFrameT:\n\u001B[0;32m   1573\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1574\u001B[0m \u001B[38;5;124;03m    Apply function f in python space\u001B[39;00m\n\u001B[0;32m   1575\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1599\u001B[0m \u001B[38;5;124;03m        data after applying f\u001B[39;00m\n\u001B[0;32m   1600\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1601\u001B[0m     values, mutated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrouper\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1602\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m not_indexed_same \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1603\u001B[0m         not_indexed_same \u001B[38;5;241m=\u001B[39m mutated \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmutated\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:839\u001B[0m, in \u001B[0;36mBaseGrouper.apply\u001B[1;34m(self, f, data, axis)\u001B[0m\n\u001B[0;32m    837\u001B[0m \u001B[38;5;66;03m# group might be modified\u001B[39;00m\n\u001B[0;32m    838\u001B[0m group_axes \u001B[38;5;241m=\u001B[39m group\u001B[38;5;241m.\u001B[39maxes\n\u001B[1;32m--> 839\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgroup\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    840\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mutated \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_indexed_like(res, group_axes, axis):\n\u001B[0;32m    841\u001B[0m     mutated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1532\u001B[0m, in \u001B[0;36mGroupBy.apply.<locals>.f\u001B[1;34m(g)\u001B[0m\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[0;32m   1530\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf\u001B[39m(g):\n\u001B[0;32m   1531\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m np\u001B[38;5;241m.\u001B[39merrstate(\u001B[38;5;28mall\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1532\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(g, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\seaborn\\_statistics.py:513\u001B[0m, in \u001B[0;36mEstimateAggregator.__call__\u001B[1;34m(self, data, var)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mci\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    512\u001B[0m     units \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munits\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m--> 513\u001B[0m     boots \u001B[38;5;241m=\u001B[39m bootstrap(vals, units\u001B[38;5;241m=\u001B[39munits, func\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mestimator, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mboot_kws)\n\u001B[0;32m    514\u001B[0m     err_min, err_max \u001B[38;5;241m=\u001B[39m _percentile_interval(boots, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39merror_level)\n\u001B[0;32m    516\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mSeries({var: estimate, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvar\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mmin\u001B[39m\u001B[38;5;124m\"\u001B[39m: err_min, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvar\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124mmax\u001B[39m\u001B[38;5;124m\"\u001B[39m: err_max})\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\seaborn\\algorithms.py:98\u001B[0m, in \u001B[0;36mbootstrap\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     96\u001B[0m     resampler \u001B[38;5;241m=\u001B[39m integers(\u001B[38;5;241m0\u001B[39m, n, n, dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mintp)  \u001B[38;5;66;03m# intp is indexing dtype\u001B[39;00m\n\u001B[0;32m     97\u001B[0m     sample \u001B[38;5;241m=\u001B[39m [a\u001B[38;5;241m.\u001B[39mtake(resampler, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m args]\n\u001B[1;32m---> 98\u001B[0m     boot_dist\u001B[38;5;241m.\u001B[39mappend(f(\u001B[38;5;241m*\u001B[39msample, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfunc_kwargs))\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(boot_dist)\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mmean\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3432\u001B[0m, in \u001B[0;36mmean\u001B[1;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[0;32m   3429\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3430\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m mean(axis\u001B[38;5;241m=\u001B[39maxis, dtype\u001B[38;5;241m=\u001B[39mdtype, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m-> 3432\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _methods\u001B[38;5;241m.\u001B[39m_mean(a, axis\u001B[38;5;241m=\u001B[39maxis, dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[0;32m   3433\u001B[0m                       out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\numpy\\core\\_methods.py:181\u001B[0m, in \u001B[0;36m_mean\u001B[1;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[0;32m    178\u001B[0m         is_float16_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    180\u001B[0m ret \u001B[38;5;241m=\u001B[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001B[38;5;241m=\u001B[39mwhere)\n\u001B[1;32m--> 181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43misinstance\u001B[39;49m(ret, mu\u001B[38;5;241m.\u001B[39mndarray):\n\u001B[0;32m    182\u001B[0m     ret \u001B[38;5;241m=\u001B[39m um\u001B[38;5;241m.\u001B[39mtrue_divide(\n\u001B[0;32m    183\u001B[0m             ret, rcount, out\u001B[38;5;241m=\u001B[39mret, casting\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124munsafe\u001B[39m\u001B[38;5;124m'\u001B[39m, subok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_float16_result \u001B[38;5;129;01mand\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x1000 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(20, 10))\n",
    "\n",
    "sns.kdeplot(data=df, x='Sub_Grade_Numerical', ax=axes[0, 0])\n",
    "axes[0, 1].set_title('Subgrade Distribution')\n",
    "\n",
    "sns.lineplot(data=df, y='Risk_Score', x='Sub_Grade_Numerical',\n",
    "             ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Risk Score Distribution')\n",
    "\n",
    "sns.lineplot(data=df, y='Total_Open_Buy', x='Sub_Grade_Numerical',\n",
    "             ax=axes[0, 2])\n",
    "axes[0, 2].set_title('Total Open Buy Distribution')\n",
    "\n",
    "sns.kdeplot(x=df['Sub_Grade_Numerical'], hue=df['Term_In_Months'].astype(str),\n",
    "            ax=axes[0, 3])\n",
    "axes[0, 3].set_title('Term in Months by Subgrade')\n",
    "\n",
    "sns.lineplot(data=df, y='Total_Bank_Card_Limit', x='Sub_Grade_Numerical',\n",
    "             ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Total Bank Card Limit Distribution')\n",
    "\n",
    "sns.lineplot(data=df, y='Revolving_Account_Ratio', x='Sub_Grade_Numerical',\n",
    "             ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Revolving Account Ratio Distribution')\n",
    "\n",
    "sns.lineplot(data=df, y='Percent_Bankcard_Accounts_Greater_75',\n",
    "             x='Sub_Grade_Numerical',\n",
    "             ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Percent Bankcard Accounts Greater than 75 Distribution')\n",
    "\n",
    "sns.lineplot(data=df, y='Loan_Amount', x='Sub_Grade_Numerical',\n",
    "             ax=axes[1, 3])\n",
    "axes[1, 3].set_title('Loan Amount Distribution')\n",
    "\n",
    "sns.lineplot(data=df, y='Total_Revolving_High_Limit', x='Sub_Grade_Numerical',\n",
    "\n",
    "             ax=axes[2, 0])\n",
    "axes[2, 0].set_title('Total Revolving High Limit Distribution')\n",
    "\n",
    "sns.lineplot(data=df, y='Revolving_Account_Ratio', x='Sub_Grade_Numerical',\n",
    "             ax=axes[2, 1])\n",
    "axes[2, 1].set_title('Revolving Account Ratio Distribution')\n",
    "\n",
    "sns.lineplot(data=df, y='Debt_To_Income_Ratio', x='Sub_Grade_Numerical',\n",
    "             ax=axes[2, 2])\n",
    "axes[2, 2].set_title('Debt to Income Ratio Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2393f36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='Sub_Grade_Numerical', ylabel='Density'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef2c1313",
   "metadata": {},
   "source": [
    "## Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3f185",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f23c8ca2",
   "metadata": {},
   "source": [
    "## Statistical Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e4cc02",
   "metadata": {},
   "source": [
    "Since this is the last step of our project, we can assume we have already predicted the loan grade. Therefore, we will keep it as a feature for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf0304a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_grade_mapping = {\n",
    "    'A1': 1, 'A2': 2, 'A3': 3, 'A4': 4, 'A5': 5,\n",
    "    'B1': 6, 'B2': 7, 'B3': 8, 'B4': 9, 'B5': 10,\n",
    "    'C1': 11, 'C2': 12, 'C3': 13, 'C4': 14, 'C5': 15,\n",
    "    'D1': 16, 'D2': 17, 'D3': 18, 'D4': 19, 'D5': 20,\n",
    "    'E1': 21, 'E2': 22, 'E3': 23, 'E4': 24, 'E5': 25,\n",
    "    'F1': 26, 'F2': 27, 'F3': 28, 'F4': 29, 'F5': 30,\n",
    "    'G1': 31, 'G2': 32, 'G3': 33, 'G4': 34, 'G5': 35\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15904dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['id', 'Interest_Rate', 'Sub_Grade'], axis=1)\n",
    "y = df['Sub_Grade']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edc91fc",
   "metadata": {},
   "source": [
    "Even though we numerified the subgrade, we still want it to be evenly represented in our dataset splits. Therefore, we are going to use stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "73ef87b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8164c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pycaret, X_rest, y_pycaret, y_rest = train_test_split(X_train, y_train,\n",
    "                                                        train_size=1 / 7,\n",
    "                                                        stratify=y_train,\n",
    "                                                        random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4535073",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pycaret = y_pycaret.map(sub_grade_mapping).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "769e8c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.map(sub_grade_mapping).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1920058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.map(sub_grade_mapping).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17612c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = setup(pd.concat([X_pycaret, y_pycaret], axis=1), target='Sub_Grade',\n",
    "          session_id=123, experiment_name='tune_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8acc8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_cdf24_row10_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_cdf24\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cdf24_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_cdf24_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_cdf24_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_cdf24_row0_col1\" class=\"data row0 col1\" >123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_cdf24_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_cdf24_row1_col1\" class=\"data row1 col1\" >Sub_Grade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_cdf24_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_cdf24_row2_col1\" class=\"data row2 col1\" >Regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_cdf24_row3_col0\" class=\"data row3 col0\" >Original data shape</td>\n",
       "      <td id=\"T_cdf24_row3_col1\" class=\"data row3 col1\" >(1582467, 104)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_cdf24_row4_col0\" class=\"data row4 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_cdf24_row4_col1\" class=\"data row4 col1\" >(1582467, 130)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_cdf24_row5_col0\" class=\"data row5 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_cdf24_row5_col1\" class=\"data row5 col1\" >(1107726, 130)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_cdf24_row6_col0\" class=\"data row6 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_cdf24_row6_col1\" class=\"data row6 col1\" >(474741, 130)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_cdf24_row7_col0\" class=\"data row7 col0\" >Numeric features</td>\n",
       "      <td id=\"T_cdf24_row7_col1\" class=\"data row7 col1\" >98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_cdf24_row8_col0\" class=\"data row8 col0\" >Categorical features</td>\n",
       "      <td id=\"T_cdf24_row8_col1\" class=\"data row8 col1\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_cdf24_row9_col0\" class=\"data row9 col0\" >Rows with missing values</td>\n",
       "      <td id=\"T_cdf24_row9_col1\" class=\"data row9 col1\" >22.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_cdf24_row10_col0\" class=\"data row10 col0\" >Preprocess</td>\n",
       "      <td id=\"T_cdf24_row10_col1\" class=\"data row10 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_cdf24_row11_col0\" class=\"data row11 col0\" >Imputation type</td>\n",
       "      <td id=\"T_cdf24_row11_col1\" class=\"data row11 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_cdf24_row12_col0\" class=\"data row12 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_cdf24_row12_col1\" class=\"data row12 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_cdf24_row13_col0\" class=\"data row13 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_cdf24_row13_col1\" class=\"data row13 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_cdf24_row14_col0\" class=\"data row14 col0\" >Maximum one-hot encoding</td>\n",
       "      <td id=\"T_cdf24_row14_col1\" class=\"data row14 col1\" >25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_cdf24_row15_col0\" class=\"data row15 col0\" >Encoding method</td>\n",
       "      <td id=\"T_cdf24_row15_col1\" class=\"data row15 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_cdf24_row16_col0\" class=\"data row16 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_cdf24_row16_col1\" class=\"data row16 col1\" >KFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_cdf24_row17_col0\" class=\"data row17 col0\" >Fold Number</td>\n",
       "      <td id=\"T_cdf24_row17_col1\" class=\"data row17 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_cdf24_row18_col0\" class=\"data row18 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_cdf24_row18_col1\" class=\"data row18 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_cdf24_row19_col0\" class=\"data row19 col0\" >Use GPU</td>\n",
       "      <td id=\"T_cdf24_row19_col1\" class=\"data row19 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_cdf24_row20_col0\" class=\"data row20 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_cdf24_row20_col1\" class=\"data row20 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_cdf24_row21_col0\" class=\"data row21 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_cdf24_row21_col1\" class=\"data row21 col1\" >final_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cdf24_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_cdf24_row22_col0\" class=\"data row22 col0\" >USI</td>\n",
       "      <td id=\"T_cdf24_row22_col1\" class=\"data row22 col1\" >35fb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16bb5a236d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_826cd_row10_col0, #T_826cd_row10_col1, #T_826cd_row10_col2, #T_826cd_row10_col3, #T_826cd_row10_col4, #T_826cd_row10_col5 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_826cd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_826cd_level0_col0\" class=\"col_heading level0 col0\" >MAE</th>\n",
       "      <th id=\"T_826cd_level0_col1\" class=\"col_heading level0 col1\" >MSE</th>\n",
       "      <th id=\"T_826cd_level0_col2\" class=\"col_heading level0 col2\" >RMSE</th>\n",
       "      <th id=\"T_826cd_level0_col3\" class=\"col_heading level0 col3\" >R2</th>\n",
       "      <th id=\"T_826cd_level0_col4\" class=\"col_heading level0 col4\" >RMSLE</th>\n",
       "      <th id=\"T_826cd_level0_col5\" class=\"col_heading level0 col5\" >MAPE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_826cd_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_826cd_row0_col0\" class=\"data row0 col0\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row0_col1\" class=\"data row0 col1\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row0_col2\" class=\"data row0 col2\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row0_col3\" class=\"data row0 col3\" >0.0000</td>\n",
       "      <td id=\"T_826cd_row0_col4\" class=\"data row0 col4\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row0_col5\" class=\"data row0 col5\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_826cd_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_826cd_row1_col0\" class=\"data row1 col0\" >1.1421</td>\n",
       "      <td id=\"T_826cd_row1_col1\" class=\"data row1 col1\" >1.7755</td>\n",
       "      <td id=\"T_826cd_row1_col2\" class=\"data row1 col2\" >1.3325</td>\n",
       "      <td id=\"T_826cd_row1_col3\" class=\"data row1 col3\" >0.9549</td>\n",
       "      <td id=\"T_826cd_row1_col4\" class=\"data row1 col4\" >0.1794</td>\n",
       "      <td id=\"T_826cd_row1_col5\" class=\"data row1 col5\" >0.1803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_826cd_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_826cd_row2_col0\" class=\"data row2 col0\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row2_col1\" class=\"data row2 col1\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row2_col2\" class=\"data row2 col2\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row2_col3\" class=\"data row2 col3\" >0.0000</td>\n",
       "      <td id=\"T_826cd_row2_col4\" class=\"data row2 col4\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row2_col5\" class=\"data row2 col5\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_826cd_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_826cd_row3_col0\" class=\"data row3 col0\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row3_col1\" class=\"data row3 col1\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row3_col2\" class=\"data row3 col2\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row3_col3\" class=\"data row3 col3\" >0.0000</td>\n",
       "      <td id=\"T_826cd_row3_col4\" class=\"data row3 col4\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row3_col5\" class=\"data row3 col5\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_826cd_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_826cd_row4_col0\" class=\"data row4 col0\" >1.1431</td>\n",
       "      <td id=\"T_826cd_row4_col1\" class=\"data row4 col1\" >1.7796</td>\n",
       "      <td id=\"T_826cd_row4_col2\" class=\"data row4 col2\" >1.3340</td>\n",
       "      <td id=\"T_826cd_row4_col3\" class=\"data row4 col3\" >0.9552</td>\n",
       "      <td id=\"T_826cd_row4_col4\" class=\"data row4 col4\" >0.1793</td>\n",
       "      <td id=\"T_826cd_row4_col5\" class=\"data row4 col5\" >0.1805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_826cd_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_826cd_row5_col0\" class=\"data row5 col0\" >1.1392</td>\n",
       "      <td id=\"T_826cd_row5_col1\" class=\"data row5 col1\" >1.7693</td>\n",
       "      <td id=\"T_826cd_row5_col2\" class=\"data row5 col2\" >1.3302</td>\n",
       "      <td id=\"T_826cd_row5_col3\" class=\"data row5 col3\" >0.9552</td>\n",
       "      <td id=\"T_826cd_row5_col4\" class=\"data row5 col4\" >0.1792</td>\n",
       "      <td id=\"T_826cd_row5_col5\" class=\"data row5 col5\" >0.1793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_826cd_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_826cd_row6_col0\" class=\"data row6 col0\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row6_col1\" class=\"data row6 col1\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row6_col2\" class=\"data row6 col2\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row6_col3\" class=\"data row6 col3\" >0.0000</td>\n",
       "      <td id=\"T_826cd_row6_col4\" class=\"data row6 col4\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row6_col5\" class=\"data row6 col5\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_826cd_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_826cd_row7_col0\" class=\"data row7 col0\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row7_col1\" class=\"data row7 col1\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row7_col2\" class=\"data row7 col2\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row7_col3\" class=\"data row7 col3\" >0.0000</td>\n",
       "      <td id=\"T_826cd_row7_col4\" class=\"data row7 col4\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row7_col5\" class=\"data row7 col5\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_826cd_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_826cd_row8_col0\" class=\"data row8 col0\" >1.1464</td>\n",
       "      <td id=\"T_826cd_row8_col1\" class=\"data row8 col1\" >1.7847</td>\n",
       "      <td id=\"T_826cd_row8_col2\" class=\"data row8 col2\" >1.3359</td>\n",
       "      <td id=\"T_826cd_row8_col3\" class=\"data row8 col3\" >0.9552</td>\n",
       "      <td id=\"T_826cd_row8_col4\" class=\"data row8 col4\" >0.1815</td>\n",
       "      <td id=\"T_826cd_row8_col5\" class=\"data row8 col5\" >0.1828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_826cd_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_826cd_row9_col0\" class=\"data row9 col0\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row9_col1\" class=\"data row9 col1\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row9_col2\" class=\"data row9 col2\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row9_col3\" class=\"data row9 col3\" >0.0000</td>\n",
       "      <td id=\"T_826cd_row9_col4\" class=\"data row9 col4\" >-0.0000</td>\n",
       "      <td id=\"T_826cd_row9_col5\" class=\"data row9 col5\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_826cd_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_826cd_row10_col0\" class=\"data row10 col0\" >0.4571</td>\n",
       "      <td id=\"T_826cd_row10_col1\" class=\"data row10 col1\" >0.7109</td>\n",
       "      <td id=\"T_826cd_row10_col2\" class=\"data row10 col2\" >0.5333</td>\n",
       "      <td id=\"T_826cd_row10_col3\" class=\"data row10 col3\" >0.3821</td>\n",
       "      <td id=\"T_826cd_row10_col4\" class=\"data row10 col4\" >0.0719</td>\n",
       "      <td id=\"T_826cd_row10_col5\" class=\"data row10 col5\" >0.0723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_826cd_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_826cd_row11_col0\" class=\"data row11 col0\" >0.5598</td>\n",
       "      <td id=\"T_826cd_row11_col1\" class=\"data row11 col1\" >0.8707</td>\n",
       "      <td id=\"T_826cd_row11_col2\" class=\"data row11 col2\" >0.6531</td>\n",
       "      <td id=\"T_826cd_row11_col3\" class=\"data row11 col3\" >0.4679</td>\n",
       "      <td id=\"T_826cd_row11_col4\" class=\"data row11 col4\" >0.0881</td>\n",
       "      <td id=\"T_826cd_row11_col5\" class=\"data row11 col5\" >0.0885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16bfcc446d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(memory=Memory(location=None),\n",
       "          steps=[('numerical_imputer',\n",
       "                  TransformerWrapper(include=['Number_Trades_24_Months',\n",
       "                                              'Annual_Income', 'Total_Open_Buy',\n",
       "                                              'Number_Delinquent_2_Years',\n",
       "                                              'Delinquent_Amount',\n",
       "                                              'Debt_To_Income_Ratio',\n",
       "                                              'Employment_Length_In_Years',\n",
       "                                              'Loan_Amount', 'Risk_Score',\n",
       "                                              'Inquiries_6_Months',\n",
       "                                              'Mortgage_Accounts', 'Open_Trades',\n",
       "                                              'Revolving_Acco...\n",
       "                                              'Verification_Status'],\n",
       "                                     transformer=OneHotEncoder(cols=['Grade',\n",
       "                                                                     'Home_Ownership',\n",
       "                                                                     'Loan_Purpose',\n",
       "                                                                     'Verification_Status'],\n",
       "                                                               handle_missing='return_nan',\n",
       "                                                               use_cat_names=True))),\n",
       "                 ('rest_encoding',\n",
       "                  TransformerWrapper(include=['State'],\n",
       "                                     transformer=TargetEncoder(cols=['State'],\n",
       "                                                               handle_missing='return_nan'))),\n",
       "                 ('trained_model', LGBMRegressor(n_jobs=-1, random_state=123))]),\n",
       " 'model_subgrade_final.pkl')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = setup(pd.concat([X_train, y_train], axis=1), target='Sub_Grade',\n",
    "          session_id=123, experiment_name='final_model')\n",
    "final_model = create_model('lightgbm')\n",
    "save_model(final_model, 'model_subgrade_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e28ff341",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "id already present in metrics dataframe.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[68], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43madd_metric\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmmae\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMMAE\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmake_scorer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmax_mae\u001B[49m\u001B[43m,\u001B[49m\u001B[43mgreater_is_better\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgreater_is_better\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\utils\\generic.py:964\u001B[0m, in \u001B[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    962\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m globals_d[name] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    963\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(message)\n\u001B[1;32m--> 964\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\regression\\functional.py:2387\u001B[0m, in \u001B[0;36madd_metric\u001B[1;34m(id, name, score_func, greater_is_better, **kwargs)\u001B[0m\n\u001B[0;32m   2340\u001B[0m \u001B[38;5;129m@check_if_global_is_not_none\u001B[39m(\u001B[38;5;28mglobals\u001B[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001B[0;32m   2341\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_metric\u001B[39m(\n\u001B[0;32m   2342\u001B[0m     \u001B[38;5;28mid\u001B[39m: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2346\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2347\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mSeries:\n\u001B[0;32m   2348\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2349\u001B[0m \u001B[38;5;124;03m    Adds a custom metric to be used in the experiment.\u001B[39;00m\n\u001B[0;32m   2350\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2384\u001B[0m \n\u001B[0;32m   2385\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _CURRENT_EXPERIMENT\u001B[38;5;241m.\u001B[39madd_metric(\n\u001B[0;32m   2388\u001B[0m         \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mid\u001B[39m,\n\u001B[0;32m   2389\u001B[0m         name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   2390\u001B[0m         score_func\u001B[38;5;241m=\u001B[39mscore_func,\n\u001B[0;32m   2391\u001B[0m         greater_is_better\u001B[38;5;241m=\u001B[39mgreater_is_better,\n\u001B[0;32m   2392\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2393\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\regression\\oop.py:2651\u001B[0m, in \u001B[0;36mRegressionExperiment.add_metric\u001B[1;34m(self, id, name, score_func, greater_is_better, **kwargs)\u001B[0m\n\u001B[0;32m   2604\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_metric\u001B[39m(\n\u001B[0;32m   2605\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   2606\u001B[0m     \u001B[38;5;28mid\u001B[39m: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2610\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2611\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mSeries:\n\u001B[0;32m   2612\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2613\u001B[0m \u001B[38;5;124;03m    Adds a custom metric to be used in the experiment.\u001B[39;00m\n\u001B[0;32m   2614\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2648\u001B[0m \n\u001B[0;32m   2649\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2651\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39madd_metric(\n\u001B[0;32m   2652\u001B[0m         \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mid\u001B[39m,\n\u001B[0;32m   2653\u001B[0m         name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   2654\u001B[0m         score_func\u001B[38;5;241m=\u001B[39mscore_func,\n\u001B[0;32m   2655\u001B[0m         target\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpred\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2656\u001B[0m         greater_is_better\u001B[38;5;241m=\u001B[39mgreater_is_better,\n\u001B[0;32m   2657\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2658\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:4580\u001B[0m, in \u001B[0;36m_SupervisedExperiment.add_metric\u001B[1;34m(self, id, name, score_func, target, greater_is_better, multiclass, **kwargs)\u001B[0m\n\u001B[0;32m   4577\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msetup() needs to be ran first.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   4579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mid\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_all_metrics:\n\u001B[1;32m-> 4580\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid already present in metrics dataframe.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   4582\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ml_usecase \u001B[38;5;241m==\u001B[39m MLUsecase\u001B[38;5;241m.\u001B[39mCLASSIFICATION:\n\u001B[0;32m   4583\u001B[0m     new_metric \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   4584\u001B[0m         pycaret\u001B[38;5;241m.\u001B[39mcontainers\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mclassification\u001B[38;5;241m.\u001B[39mClassificationMetricContainer(\n\u001B[0;32m   4585\u001B[0m             \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mid\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4596\u001B[0m         )\n\u001B[0;32m   4597\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: id already present in metrics dataframe."
     ]
    }
   ],
   "source": [
    "add_metric('mmae', 'MMAE', make_scorer(max_mae,greater_is_better=False), greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "64e638f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "id already present in metrics dataframe.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[66], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscript\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dummy_metric\n\u001B[1;32m----> 3\u001B[0m \u001B[43madd_metric\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdummy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdummy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmake_scorer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdummy_metric\u001B[49m\u001B[43m,\u001B[49m\u001B[43mgreater_is_better\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgreater_is_better\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\utils\\generic.py:964\u001B[0m, in \u001B[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    962\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m globals_d[name] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    963\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(message)\n\u001B[1;32m--> 964\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\regression\\functional.py:2387\u001B[0m, in \u001B[0;36madd_metric\u001B[1;34m(id, name, score_func, greater_is_better, **kwargs)\u001B[0m\n\u001B[0;32m   2340\u001B[0m \u001B[38;5;129m@check_if_global_is_not_none\u001B[39m(\u001B[38;5;28mglobals\u001B[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001B[0;32m   2341\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_metric\u001B[39m(\n\u001B[0;32m   2342\u001B[0m     \u001B[38;5;28mid\u001B[39m: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2346\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2347\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mSeries:\n\u001B[0;32m   2348\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2349\u001B[0m \u001B[38;5;124;03m    Adds a custom metric to be used in the experiment.\u001B[39;00m\n\u001B[0;32m   2350\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2384\u001B[0m \n\u001B[0;32m   2385\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _CURRENT_EXPERIMENT\u001B[38;5;241m.\u001B[39madd_metric(\n\u001B[0;32m   2388\u001B[0m         \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mid\u001B[39m,\n\u001B[0;32m   2389\u001B[0m         name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   2390\u001B[0m         score_func\u001B[38;5;241m=\u001B[39mscore_func,\n\u001B[0;32m   2391\u001B[0m         greater_is_better\u001B[38;5;241m=\u001B[39mgreater_is_better,\n\u001B[0;32m   2392\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2393\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\regression\\oop.py:2651\u001B[0m, in \u001B[0;36mRegressionExperiment.add_metric\u001B[1;34m(self, id, name, score_func, greater_is_better, **kwargs)\u001B[0m\n\u001B[0;32m   2604\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madd_metric\u001B[39m(\n\u001B[0;32m   2605\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   2606\u001B[0m     \u001B[38;5;28mid\u001B[39m: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2610\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2611\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m pd\u001B[38;5;241m.\u001B[39mSeries:\n\u001B[0;32m   2612\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2613\u001B[0m \u001B[38;5;124;03m    Adds a custom metric to be used in the experiment.\u001B[39;00m\n\u001B[0;32m   2614\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2648\u001B[0m \n\u001B[0;32m   2649\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2651\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39madd_metric(\n\u001B[0;32m   2652\u001B[0m         \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mid\u001B[39m,\n\u001B[0;32m   2653\u001B[0m         name\u001B[38;5;241m=\u001B[39mname,\n\u001B[0;32m   2654\u001B[0m         score_func\u001B[38;5;241m=\u001B[39mscore_func,\n\u001B[0;32m   2655\u001B[0m         target\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpred\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   2656\u001B[0m         greater_is_better\u001B[38;5;241m=\u001B[39mgreater_is_better,\n\u001B[0;32m   2657\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   2658\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:4580\u001B[0m, in \u001B[0;36m_SupervisedExperiment.add_metric\u001B[1;34m(self, id, name, score_func, target, greater_is_better, multiclass, **kwargs)\u001B[0m\n\u001B[0;32m   4577\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msetup() needs to be ran first.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   4579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mid\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_all_metrics:\n\u001B[1;32m-> 4580\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid already present in metrics dataframe.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   4582\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ml_usecase \u001B[38;5;241m==\u001B[39m MLUsecase\u001B[38;5;241m.\u001B[39mCLASSIFICATION:\n\u001B[0;32m   4583\u001B[0m     new_metric \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   4584\u001B[0m         pycaret\u001B[38;5;241m.\u001B[39mcontainers\u001B[38;5;241m.\u001B[39mmetrics\u001B[38;5;241m.\u001B[39mclassification\u001B[38;5;241m.\u001B[39mClassificationMetricContainer(\n\u001B[0;32m   4585\u001B[0m             \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mid\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4596\u001B[0m         )\n\u001B[0;32m   4597\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: id already present in metrics dataframe."
     ]
    }
   ],
   "source": [
    "from script import dummy_metric\n",
    "\n",
    "add_metric('dummy', 'dummy', make_scorer(dummy_metric,greater_is_better=False), greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "04da97e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_Scorer' object has no attribute '__name__'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\IPython\\core\\formatters.py:706\u001B[0m, in \u001B[0;36mPlainTextFormatter.__call__\u001B[1;34m(self, obj)\u001B[0m\n\u001B[0;32m    699\u001B[0m stream \u001B[38;5;241m=\u001B[39m StringIO()\n\u001B[0;32m    700\u001B[0m printer \u001B[38;5;241m=\u001B[39m pretty\u001B[38;5;241m.\u001B[39mRepresentationPrinter(stream, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mverbose,\n\u001B[0;32m    701\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_width, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnewline,\n\u001B[0;32m    702\u001B[0m     max_seq_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_seq_length,\n\u001B[0;32m    703\u001B[0m     singleton_pprinters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msingleton_printers,\n\u001B[0;32m    704\u001B[0m     type_pprinters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtype_printers,\n\u001B[0;32m    705\u001B[0m     deferred_pprinters\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeferred_printers)\n\u001B[1;32m--> 706\u001B[0m \u001B[43mprinter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpretty\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    707\u001B[0m printer\u001B[38;5;241m.\u001B[39mflush()\n\u001B[0;32m    708\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m stream\u001B[38;5;241m.\u001B[39mgetvalue()\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\IPython\\lib\\pretty.py:410\u001B[0m, in \u001B[0;36mRepresentationPrinter.pretty\u001B[1;34m(self, obj)\u001B[0m\n\u001B[0;32m    407\u001B[0m                         \u001B[38;5;28;01mreturn\u001B[39;00m meth(obj, \u001B[38;5;28mself\u001B[39m, cycle)\n\u001B[0;32m    408\u001B[0m                 \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mobject\u001B[39m \\\n\u001B[0;32m    409\u001B[0m                         \u001B[38;5;129;01mand\u001B[39;00m callable(\u001B[38;5;28mcls\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__repr__\u001B[39m\u001B[38;5;124m'\u001B[39m)):\n\u001B[1;32m--> 410\u001B[0m                     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_repr_pprint\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcycle\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    412\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_pprint(obj, \u001B[38;5;28mself\u001B[39m, cycle)\n\u001B[0;32m    413\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\IPython\\lib\\pretty.py:778\u001B[0m, in \u001B[0;36m_repr_pprint\u001B[1;34m(obj, p, cycle)\u001B[0m\n\u001B[0;32m    776\u001B[0m \u001B[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001B[39;00m\n\u001B[0;32m    777\u001B[0m \u001B[38;5;66;03m# Find newlines and replace them with p.break_()\u001B[39;00m\n\u001B[1;32m--> 778\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mrepr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    779\u001B[0m lines \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39msplitlines()\n\u001B[0;32m    780\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m p\u001B[38;5;241m.\u001B[39mgroup():\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py:1063\u001B[0m, in \u001B[0;36mDataFrame.__repr__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1060\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m buf\u001B[38;5;241m.\u001B[39mgetvalue()\n\u001B[0;32m   1062\u001B[0m repr_params \u001B[38;5;241m=\u001B[39m fmt\u001B[38;5;241m.\u001B[39mget_dataframe_repr_params()\n\u001B[1;32m-> 1063\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_string(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mrepr_params)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py:1244\u001B[0m, in \u001B[0;36mDataFrame.to_string\u001B[1;34m(self, buf, columns, col_space, header, index, na_rep, formatters, float_format, sparsify, index_names, justify, max_rows, max_cols, show_dimensions, decimal, line_width, min_rows, max_colwidth, encoding)\u001B[0m\n\u001B[0;32m   1225\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m option_context(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisplay.max_colwidth\u001B[39m\u001B[38;5;124m\"\u001B[39m, max_colwidth):\n\u001B[0;32m   1226\u001B[0m     formatter \u001B[38;5;241m=\u001B[39m fmt\u001B[38;5;241m.\u001B[39mDataFrameFormatter(\n\u001B[0;32m   1227\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1228\u001B[0m         columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1242\u001B[0m         decimal\u001B[38;5;241m=\u001B[39mdecimal,\n\u001B[0;32m   1243\u001B[0m     )\n\u001B[1;32m-> 1244\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfmt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_string\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1245\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbuf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1246\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1247\u001B[0m \u001B[43m        \u001B[49m\u001B[43mline_width\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mline_width\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1248\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1136\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_string\u001B[1;34m(self, buf, encoding, line_width)\u001B[0m\n\u001B[0;32m   1133\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mformats\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstring\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StringFormatter\n\u001B[0;32m   1135\u001B[0m string_formatter \u001B[38;5;241m=\u001B[39m StringFormatter(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt, line_width\u001B[38;5;241m=\u001B[39mline_width)\n\u001B[1;32m-> 1136\u001B[0m string \u001B[38;5;241m=\u001B[39m \u001B[43mstring_formatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1137\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m save_to_buffer(string, buf\u001B[38;5;241m=\u001B[39mbuf, encoding\u001B[38;5;241m=\u001B[39mencoding)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\string.py:30\u001B[0m, in \u001B[0;36mStringFormatter.to_string\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto_string\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m---> 30\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_string_representation\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt\u001B[38;5;241m.\u001B[39mshould_show_dimensions:\n\u001B[0;32m     32\u001B[0m         text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([text, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt\u001B[38;5;241m.\u001B[39mdimensions_info])\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\string.py:45\u001B[0m, in \u001B[0;36mStringFormatter._get_string_representation\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt\u001B[38;5;241m.\u001B[39mframe\u001B[38;5;241m.\u001B[39mempty:\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_empty_info_line\n\u001B[1;32m---> 45\u001B[0m strcols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_strcols\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mline_width \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;66;03m# no need to wrap around just print the whole frame\u001B[39;00m\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madj\u001B[38;5;241m.\u001B[39madjoin(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39mstrcols)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\string.py:36\u001B[0m, in \u001B[0;36mStringFormatter._get_strcols\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_strcols\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]]:\n\u001B[1;32m---> 36\u001B[0m     strcols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfmt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_strcols\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt\u001B[38;5;241m.\u001B[39mis_truncated:\n\u001B[0;32m     38\u001B[0m         strcols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_insert_dot_separators(strcols)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:617\u001B[0m, in \u001B[0;36mDataFrameFormatter.get_strcols\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    613\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_strcols\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]]:\n\u001B[0;32m    614\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    615\u001B[0m \u001B[38;5;124;03m    Render a DataFrame to a list of columns (as lists of strings).\u001B[39;00m\n\u001B[0;32m    616\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 617\u001B[0m     strcols \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_strcols_without_index\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    619\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex:\n\u001B[0;32m    620\u001B[0m         str_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_formatted_index(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtr_frame)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:883\u001B[0m, in \u001B[0;36mDataFrameFormatter._get_strcols_without_index\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    879\u001B[0m cheader \u001B[38;5;241m=\u001B[39m str_columns[i]\n\u001B[0;32m    880\u001B[0m header_colwidth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\n\u001B[0;32m    881\u001B[0m     \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcol_space\u001B[38;5;241m.\u001B[39mget(c, \u001B[38;5;241m0\u001B[39m)), \u001B[38;5;241m*\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madj\u001B[38;5;241m.\u001B[39mlen(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m cheader)\n\u001B[0;32m    882\u001B[0m )\n\u001B[1;32m--> 883\u001B[0m fmt_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat_col\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    884\u001B[0m fmt_values \u001B[38;5;241m=\u001B[39m _make_fixed_width(\n\u001B[0;32m    885\u001B[0m     fmt_values, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjustify, minimum\u001B[38;5;241m=\u001B[39mheader_colwidth, adj\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madj\n\u001B[0;32m    886\u001B[0m )\n\u001B[0;32m    888\u001B[0m max_len \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mmax\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madj\u001B[38;5;241m.\u001B[39mlen(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m fmt_values), header_colwidth)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:897\u001B[0m, in \u001B[0;36mDataFrameFormatter.format_col\u001B[1;34m(self, i)\u001B[0m\n\u001B[0;32m    895\u001B[0m frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtr_frame\n\u001B[0;32m    896\u001B[0m formatter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_formatter(i)\n\u001B[1;32m--> 897\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mformat_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    898\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    899\u001B[0m \u001B[43m    \u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    900\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfloat_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    901\u001B[0m \u001B[43m    \u001B[49m\u001B[43mna_rep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mna_rep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    902\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcol_space\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    903\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecimal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecimal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    904\u001B[0m \u001B[43m    \u001B[49m\u001B[43mleading_space\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    905\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1328\u001B[0m, in \u001B[0;36mformat_array\u001B[1;34m(values, formatter, float_format, na_rep, digits, space, justify, decimal, leading_space, quoting)\u001B[0m\n\u001B[0;32m   1313\u001B[0m     digits \u001B[38;5;241m=\u001B[39m get_option(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisplay.precision\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1315\u001B[0m fmt_obj \u001B[38;5;241m=\u001B[39m fmt_klass(\n\u001B[0;32m   1316\u001B[0m     values,\n\u001B[0;32m   1317\u001B[0m     digits\u001B[38;5;241m=\u001B[39mdigits,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1325\u001B[0m     quoting\u001B[38;5;241m=\u001B[39mquoting,\n\u001B[0;32m   1326\u001B[0m )\n\u001B[1;32m-> 1328\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfmt_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1359\u001B[0m, in \u001B[0;36mGenericArrayFormatter.get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1358\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_result\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m-> 1359\u001B[0m     fmt_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_format_strings\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1360\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _make_fixed_width(fmt_values, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjustify)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1422\u001B[0m, in \u001B[0;36mGenericArrayFormatter._format_strings\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1420\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(vals):\n\u001B[0;32m   1421\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_float_type[i] \u001B[38;5;129;01mand\u001B[39;00m leading_space:\n\u001B[1;32m-> 1422\u001B[0m         fmt_values\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_format(v)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1423\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m is_float_type[i]:\n\u001B[0;32m   1424\u001B[0m         fmt_values\u001B[38;5;241m.\u001B[39mappend(float_format(v))\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1402\u001B[0m, in \u001B[0;36mGenericArrayFormatter._format_strings.<locals>._format\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m   1399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(x)\n\u001B[0;32m   1400\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1401\u001B[0m     \u001B[38;5;66;03m# object dtype\u001B[39;00m\n\u001B[1;32m-> 1402\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(\u001B[43mformatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\printing.py:232\u001B[0m, in \u001B[0;36mpprint_thing\u001B[1;34m(thing, _nest_lvl, escape_chars, default_escapes, quote_strings, max_seq_items)\u001B[0m\n\u001B[0;32m    230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mas_escaped_string(thing)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 232\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mas_escaped_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthing\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\printing.py:208\u001B[0m, in \u001B[0;36mpprint_thing.<locals>.as_escaped_string\u001B[1;34m(thing, escape_chars)\u001B[0m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    206\u001B[0m     escape_chars \u001B[38;5;241m=\u001B[39m escape_chars \u001B[38;5;129;01mor\u001B[39;00m ()\n\u001B[1;32m--> 208\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mthing\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m escape_chars:\n\u001B[0;32m    210\u001B[0m     result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mreplace(c, translate[c])\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:211\u001B[0m, in \u001B[0;36m_BaseScorer.__repr__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    207\u001B[0m response_method_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, response_method=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_method\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    208\u001B[0m kwargs_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mv\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_kwargs\u001B[38;5;241m.\u001B[39mitems()])\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m--> 211\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmake_scorer(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_score_func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msign_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    212\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse_method_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mkwargs_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    213\u001B[0m )\n",
      "\u001B[1;31mAttributeError\u001B[0m: '_Scorer' object has no attribute '__name__'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'_Scorer' object has no attribute '__name__'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\IPython\\core\\formatters.py:342\u001B[0m, in \u001B[0;36mBaseFormatter.__call__\u001B[1;34m(self, obj)\u001B[0m\n\u001B[0;32m    340\u001B[0m     method \u001B[38;5;241m=\u001B[39m get_real_method(obj, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_method)\n\u001B[0;32m    341\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m method \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 342\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    343\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    344\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\core\\frame.py:1105\u001B[0m, in \u001B[0;36mDataFrame._repr_html_\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1083\u001B[0m     show_dimensions \u001B[38;5;241m=\u001B[39m get_option(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisplay.show_dimensions\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1085\u001B[0m     formatter \u001B[38;5;241m=\u001B[39m fmt\u001B[38;5;241m.\u001B[39mDataFrameFormatter(\n\u001B[0;32m   1086\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1087\u001B[0m         columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1103\u001B[0m         decimal\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1104\u001B[0m     )\n\u001B[1;32m-> 1105\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfmt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrameRenderer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_html\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnotebook\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1110\u001B[0m, in \u001B[0;36mDataFrameRenderer.to_html\u001B[1;34m(self, buf, encoding, classes, notebook, border, table_id, render_links)\u001B[0m\n\u001B[0;32m   1101\u001B[0m Klass \u001B[38;5;241m=\u001B[39m NotebookFormatter \u001B[38;5;28;01mif\u001B[39;00m notebook \u001B[38;5;28;01melse\u001B[39;00m HTMLFormatter\n\u001B[0;32m   1103\u001B[0m html_formatter \u001B[38;5;241m=\u001B[39m Klass(\n\u001B[0;32m   1104\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt,\n\u001B[0;32m   1105\u001B[0m     classes\u001B[38;5;241m=\u001B[39mclasses,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1108\u001B[0m     render_links\u001B[38;5;241m=\u001B[39mrender_links,\n\u001B[0;32m   1109\u001B[0m )\n\u001B[1;32m-> 1110\u001B[0m string \u001B[38;5;241m=\u001B[39m \u001B[43mhtml_formatter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m save_to_buffer(string, buf\u001B[38;5;241m=\u001B[39mbuf, encoding\u001B[38;5;241m=\u001B[39mencoding)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\html.py:77\u001B[0m, in \u001B[0;36mHTMLFormatter.to_string\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mto_string\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m---> 77\u001B[0m     lines \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m lines):\n\u001B[0;32m     79\u001B[0m         lines \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mstr\u001B[39m(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m lines]\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\html.py:632\u001B[0m, in \u001B[0;36mNotebookFormatter.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<div>\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite_style()\n\u001B[1;32m--> 632\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m</div>\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    634\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39melements\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\html.py:83\u001B[0m, in \u001B[0;36mHTMLFormatter.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrender\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m---> 83\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_write_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshould_show_dimensions:\n\u001B[0;32m     86\u001B[0m         by \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mchr\u001B[39m(\u001B[38;5;241m215\u001B[39m)  \u001B[38;5;66;03m# \u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\html.py:258\u001B[0m, in \u001B[0;36mHTMLFormatter._write_table\u001B[1;34m(self, indent)\u001B[0m\n\u001B[0;32m    255\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt\u001B[38;5;241m.\u001B[39mheader \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshow_row_idx_names:\n\u001B[0;32m    256\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write_header(indent \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindent_delta)\n\u001B[1;32m--> 258\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_write_body\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindent_delta\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    260\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m</table>\u001B[39m\u001B[38;5;124m\"\u001B[39m, indent)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\html.py:406\u001B[0m, in \u001B[0;36mHTMLFormatter._write_body\u001B[1;34m(self, indent)\u001B[0m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_write_body\u001B[39m(\u001B[38;5;28mself\u001B[39m, indent: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    405\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<tbody>\u001B[39m\u001B[38;5;124m\"\u001B[39m, indent)\n\u001B[1;32m--> 406\u001B[0m     fmt_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_formatted_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    408\u001B[0m     \u001B[38;5;66;03m# write values\u001B[39;00m\n\u001B[0;32m    409\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt\u001B[38;5;241m.\u001B[39mindex \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframe\u001B[38;5;241m.\u001B[39mindex, MultiIndex):\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\html.py:596\u001B[0m, in \u001B[0;36mNotebookFormatter._get_formatted_values\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    595\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_formatted_values\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]]:\n\u001B[1;32m--> 596\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {i: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfmt\u001B[38;5;241m.\u001B[39mformat_col(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mncols)}\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\html.py:596\u001B[0m, in \u001B[0;36m<dictcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    595\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_formatted_values\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]]:\n\u001B[1;32m--> 596\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {i: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfmt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat_col\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mncols)}\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:897\u001B[0m, in \u001B[0;36mDataFrameFormatter.format_col\u001B[1;34m(self, i)\u001B[0m\n\u001B[0;32m    895\u001B[0m frame \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtr_frame\n\u001B[0;32m    896\u001B[0m formatter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_formatter(i)\n\u001B[1;32m--> 897\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mformat_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    898\u001B[0m \u001B[43m    \u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miloc\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    899\u001B[0m \u001B[43m    \u001B[49m\u001B[43mformatter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    900\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfloat_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    901\u001B[0m \u001B[43m    \u001B[49m\u001B[43mna_rep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mna_rep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    902\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcol_space\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    903\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecimal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecimal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    904\u001B[0m \u001B[43m    \u001B[49m\u001B[43mleading_space\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    905\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1328\u001B[0m, in \u001B[0;36mformat_array\u001B[1;34m(values, formatter, float_format, na_rep, digits, space, justify, decimal, leading_space, quoting)\u001B[0m\n\u001B[0;32m   1313\u001B[0m     digits \u001B[38;5;241m=\u001B[39m get_option(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisplay.precision\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1315\u001B[0m fmt_obj \u001B[38;5;241m=\u001B[39m fmt_klass(\n\u001B[0;32m   1316\u001B[0m     values,\n\u001B[0;32m   1317\u001B[0m     digits\u001B[38;5;241m=\u001B[39mdigits,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1325\u001B[0m     quoting\u001B[38;5;241m=\u001B[39mquoting,\n\u001B[0;32m   1326\u001B[0m )\n\u001B[1;32m-> 1328\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfmt_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_result\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1359\u001B[0m, in \u001B[0;36mGenericArrayFormatter.get_result\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1358\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_result\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[1;32m-> 1359\u001B[0m     fmt_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_format_strings\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1360\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _make_fixed_width(fmt_values, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjustify)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1422\u001B[0m, in \u001B[0;36mGenericArrayFormatter._format_strings\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1420\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(vals):\n\u001B[0;32m   1421\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_float_type[i] \u001B[38;5;129;01mand\u001B[39;00m leading_space:\n\u001B[1;32m-> 1422\u001B[0m         fmt_values\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_format(v)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1423\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m is_float_type[i]:\n\u001B[0;32m   1424\u001B[0m         fmt_values\u001B[38;5;241m.\u001B[39mappend(float_format(v))\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py:1402\u001B[0m, in \u001B[0;36mGenericArrayFormatter._format_strings.<locals>._format\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m   1399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(x)\n\u001B[0;32m   1400\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1401\u001B[0m     \u001B[38;5;66;03m# object dtype\u001B[39;00m\n\u001B[1;32m-> 1402\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(\u001B[43mformatter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\printing.py:232\u001B[0m, in \u001B[0;36mpprint_thing\u001B[1;34m(thing, _nest_lvl, escape_chars, default_escapes, quote_strings, max_seq_items)\u001B[0m\n\u001B[0;32m    230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mas_escaped_string(thing)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 232\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mas_escaped_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthing\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    234\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pandas\\io\\formats\\printing.py:208\u001B[0m, in \u001B[0;36mpprint_thing.<locals>.as_escaped_string\u001B[1;34m(thing, escape_chars)\u001B[0m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    206\u001B[0m     escape_chars \u001B[38;5;241m=\u001B[39m escape_chars \u001B[38;5;129;01mor\u001B[39;00m ()\n\u001B[1;32m--> 208\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mthing\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    209\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m escape_chars:\n\u001B[0;32m    210\u001B[0m     result \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mreplace(c, translate[c])\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:211\u001B[0m, in \u001B[0;36m_BaseScorer.__repr__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    207\u001B[0m response_method_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, response_method=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_response_method\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    208\u001B[0m kwargs_string \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mv\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_kwargs\u001B[38;5;241m.\u001B[39mitems()])\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m--> 211\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmake_scorer(\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_score_func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msign_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    212\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresponse_method_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mkwargs_string\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    213\u001B[0m )\n",
      "\u001B[1;31mAttributeError\u001B[0m: '_Scorer' object has no attribute '__name__'"
     ]
    }
   ],
   "source": [
    "get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c12fcf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>18:36:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Fitting 10 Folds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Lasso Regression</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                \n",
       "                                                                \n",
       "Initiated  . . . . . . . . . . . . . . . . . .          18:36:46\n",
       "Status     . . . . . . . . . . . . . . . . . .  Fitting 10 Folds\n",
       "Estimator  . . . . . . . . . . . . . . . . . .  Lasso Regression"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8d497 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_8d497_row0_col0, #T_8d497_row0_col1, #T_8d497_row0_col2, #T_8d497_row0_col3, #T_8d497_row0_col4, #T_8d497_row0_col5, #T_8d497_row0_col6, #T_8d497_row0_col7, #T_8d497_row0_col8, #T_8d497_row0_col9 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8d497\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8d497_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_8d497_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th id=\"T_8d497_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n",
       "      <th id=\"T_8d497_level0_col3\" class=\"col_heading level0 col3\" >RMSE</th>\n",
       "      <th id=\"T_8d497_level0_col4\" class=\"col_heading level0 col4\" >R2</th>\n",
       "      <th id=\"T_8d497_level0_col5\" class=\"col_heading level0 col5\" >RMSLE</th>\n",
       "      <th id=\"T_8d497_level0_col6\" class=\"col_heading level0 col6\" >MAPE</th>\n",
       "      <th id=\"T_8d497_level0_col7\" class=\"col_heading level0 col7\" >MMAE</th>\n",
       "      <th id=\"T_8d497_level0_col8\" class=\"col_heading level0 col8\" >dummy</th>\n",
       "      <th id=\"T_8d497_level0_col9\" class=\"col_heading level0 col9\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8d497_level0_row0\" class=\"row_heading level0 row0\" >lr</th>\n",
       "      <td id=\"T_8d497_row0_col0\" class=\"data row0 col0\" >Linear Regression</td>\n",
       "      <td id=\"T_8d497_row0_col1\" class=\"data row0 col1\" >1.1720</td>\n",
       "      <td id=\"T_8d497_row0_col2\" class=\"data row0 col2\" >1.8576</td>\n",
       "      <td id=\"T_8d497_row0_col3\" class=\"data row0 col3\" >1.3629</td>\n",
       "      <td id=\"T_8d497_row0_col4\" class=\"data row0 col4\" >0.9533</td>\n",
       "      <td id=\"T_8d497_row0_col5\" class=\"data row0 col5\" >0.1895</td>\n",
       "      <td id=\"T_8d497_row0_col6\" class=\"data row0 col6\" >0.1949</td>\n",
       "      <td id=\"T_8d497_row0_col7\" class=\"data row0 col7\" >0.0000</td>\n",
       "      <td id=\"T_8d497_row0_col8\" class=\"data row0 col8\" >0.0000</td>\n",
       "      <td id=\"T_8d497_row0_col9\" class=\"data row0 col9\" >2.6940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1de6cfe3460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m best_subgrade_model \u001B[38;5;241m=\u001B[39m \u001B[43mcompare_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msvm\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdummy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgbr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMMAE\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\utils\\generic.py:964\u001B[0m, in \u001B[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    962\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m globals_d[name] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    963\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(message)\n\u001B[1;32m--> 964\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\regression\\functional.py:805\u001B[0m, in \u001B[0;36mcompare_models\u001B[1;34m(include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, engine, verbose, parallel)\u001B[0m\n\u001B[0;32m    670\u001B[0m \u001B[38;5;129m@check_if_global_is_not_none\u001B[39m(\u001B[38;5;28mglobals\u001B[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001B[0;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompare_models\u001B[39m(\n\u001B[0;32m    672\u001B[0m     include: Optional[List[Union[\u001B[38;5;28mstr\u001B[39m, Any]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    687\u001B[0m     parallel: Optional[ParallelBackend] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    688\u001B[0m ):\n\u001B[0;32m    689\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    690\u001B[0m \u001B[38;5;124;03m    This function trains and evaluates performance of all estimators available in the\u001B[39;00m\n\u001B[0;32m    691\u001B[0m \u001B[38;5;124;03m    model library using cross validation. The output of this function is a score grid\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    803\u001B[0m \n\u001B[0;32m    804\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_CURRENT_EXPERIMENT\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompare_models\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    806\u001B[0m \u001B[43m        \u001B[49m\u001B[43minclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    807\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    808\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    809\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    810\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcross_validation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_validation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    811\u001B[0m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    812\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_select\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_select\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    813\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbudget_time\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbudget_time\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    814\u001B[0m \u001B[43m        \u001B[49m\u001B[43mturbo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mturbo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    815\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    816\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    817\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    818\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexperiment_custom_tags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexperiment_custom_tags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    819\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    820\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    821\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparallel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparallel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    822\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\regression\\oop.py:1122\u001B[0m, in \u001B[0;36mRegressionExperiment.compare_models\u001B[1;34m(self, include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, engine, verbose, parallel)\u001B[0m\n\u001B[0;32m   1119\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_engine(estimator\u001B[38;5;241m=\u001B[39mestimator, engine\u001B[38;5;241m=\u001B[39meng, severity\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1121\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1122\u001B[0m     return_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompare_models\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1123\u001B[0m \u001B[43m        \u001B[49m\u001B[43minclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1124\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1125\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1126\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1127\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcross_validation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_validation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1128\u001B[0m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1129\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_select\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_select\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1130\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbudget_time\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbudget_time\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[43m        \u001B[49m\u001B[43mturbo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mturbo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1132\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1133\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1134\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1135\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexperiment_custom_tags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexperiment_custom_tags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1136\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1137\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparallel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparallel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1138\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcaller_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcaller_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1142\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m engine \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1143\u001B[0m         \u001B[38;5;66;03m# Reset the models back to the default engines\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:794\u001B[0m, in \u001B[0;36m_SupervisedExperiment.compare_models\u001B[1;34m(self, include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, probability_threshold, verbose, parallel, caller_params)\u001B[0m\n\u001B[0;32m    791\u001B[0m results_columns_to_ignore \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mObject\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mruntime\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcutoff\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    793\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 794\u001B[0m     model, model_fit_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcreate_model_args)\n\u001B[0;32m    795\u001B[0m     model_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpull(pop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[0;32m    797\u001B[0m         np\u001B[38;5;241m.\u001B[39msum(\n\u001B[0;32m    798\u001B[0m             model_results\u001B[38;5;241m.\u001B[39mdrop(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    802\u001B[0m         \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m    803\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:1533\u001B[0m, in \u001B[0;36m_SupervisedExperiment._create_model\u001B[1;34m(self, estimator, fold, round, cross_validation, predict, fit_kwargs, groups, refit, probability_threshold, experiment_custom_tags, verbose, system, add_to_model_list, X_train_data, y_train_data, metrics, display, model_only, return_train_score, error_score, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m model, model_fit_time\n\u001B[0;32m   1531\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\u001B[1;32m-> 1533\u001B[0m model, model_fit_time, model_results, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_model_with_cv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1534\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1535\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_X\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_X\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1536\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_y\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1537\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfit_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1538\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1539\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1540\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1541\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1542\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrefit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrefit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[43m    \u001B[49m\u001B[43msystem\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1544\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisplay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisplay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1545\u001B[0m \u001B[43m    \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1546\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1547\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1549\u001B[0m \u001B[38;5;66;03m# end runtime\u001B[39;00m\n\u001B[0;32m   1550\u001B[0m runtime_end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:1126\u001B[0m, in \u001B[0;36m_SupervisedExperiment._create_model_with_cv\u001B[1;34m(self, model, data_X, data_y, fit_kwargs, round, cv, groups, metrics, refit, system, display, error_score, return_train_score)\u001B[0m\n\u001B[0;32m   1124\u001B[0m     model_fit_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   1125\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m redirect_output(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger):\n\u001B[1;32m-> 1126\u001B[0m         scores \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1127\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpipeline_with_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1128\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata_X\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1129\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1130\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1132\u001B[0m \u001B[43m            \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetrics_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1133\u001B[0m \u001B[43m            \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1134\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1135\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1136\u001B[0m \u001B[43m            \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1137\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1139\u001B[0m model_fit_end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   1140\u001B[0m model_fit_time \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(model_fit_end \u001B[38;5;241m-\u001B[39m model_fit_start)\u001B[38;5;241m.\u001B[39mround(\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:430\u001B[0m, in \u001B[0;36mcross_validate\u001B[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001B[39;00m\n\u001B[0;32m    428\u001B[0m \u001B[38;5;66;03m# independent, and that it is pickle-able.\u001B[39;00m\n\u001B[0;32m    429\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[1;32m--> 430\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    431\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    435\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscorer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscorers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    436\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    437\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    438\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    439\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    440\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    441\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscore_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscorer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    442\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    443\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_times\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_estimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_estimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    445\u001B[0m \u001B[43m        \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    446\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    447\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\n\u001B[0;32m    448\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    450\u001B[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001B[0;32m    452\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     64\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     66\u001B[0m )\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\joblib\\parallel.py:1952\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1946\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   1947\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   1948\u001B[0m \u001B[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001B[39;00m\n\u001B[0;32m   1949\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   1950\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1952\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\joblib\\parallel.py:1595\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1592\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1594\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1595\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1597\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1598\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1599\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1600\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1601\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\joblib\\parallel.py:1707\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1702\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1703\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[0;32m   1704\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   1705\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[0;32m   1706\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[1;32m-> 1707\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1708\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1710\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[0;32m   1711\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[0;32m   1712\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "best_subgrade_model = compare_models(exclude=['svm', 'dummy', 'gbr'],\n",
    "                                     sort='MMAE')\n",
    "# save_model(best_subgrade_model,'model_subgrade_untuned_mmae')\n",
    "# tuned_best_subgrade_model, tuner=tune_model(best_subgrade_model,optimize='MMAE',tuner_verbose=3,return_tuner=True)\n",
    "# save_model(tuned_best_subgrade_model,'model_subgrade_tuned_mmae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7e02f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>23:23:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Fitting 10 Folds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Random Forest Regressor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       \n",
       "                                                                       \n",
       "Initiated  . . . . . . . . . . . . . . . . . .                 23:23:40\n",
       "Status     . . . . . . . . . . . . . . . . . .         Fitting 10 Folds\n",
       "Estimator  . . . . . . . . . . . . . . . . . .  Random Forest Regressor"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2cc63 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_2cc63_row0_col0, #T_2cc63_row0_col1, #T_2cc63_row0_col2, #T_2cc63_row0_col3, #T_2cc63_row0_col4, #T_2cc63_row0_col5, #T_2cc63_row0_col6, #T_2cc63_row0_col7, #T_2cc63_row0_col8, #T_2cc63_row0_col9, #T_2cc63_row1_col0, #T_2cc63_row1_col1, #T_2cc63_row1_col2, #T_2cc63_row1_col3, #T_2cc63_row1_col4, #T_2cc63_row1_col5, #T_2cc63_row1_col6, #T_2cc63_row1_col7, #T_2cc63_row1_col8, #T_2cc63_row1_col9, #T_2cc63_row2_col0, #T_2cc63_row2_col1, #T_2cc63_row2_col2, #T_2cc63_row2_col3, #T_2cc63_row2_col4, #T_2cc63_row2_col5, #T_2cc63_row2_col6, #T_2cc63_row2_col7, #T_2cc63_row2_col8, #T_2cc63_row2_col9, #T_2cc63_row3_col0, #T_2cc63_row3_col1, #T_2cc63_row3_col2, #T_2cc63_row3_col3, #T_2cc63_row3_col4, #T_2cc63_row3_col5, #T_2cc63_row3_col6, #T_2cc63_row3_col7, #T_2cc63_row3_col8, #T_2cc63_row3_col9, #T_2cc63_row4_col0, #T_2cc63_row4_col1, #T_2cc63_row4_col2, #T_2cc63_row4_col3, #T_2cc63_row4_col4, #T_2cc63_row4_col5, #T_2cc63_row4_col6, #T_2cc63_row4_col7, #T_2cc63_row4_col8, #T_2cc63_row4_col9, #T_2cc63_row5_col0, #T_2cc63_row5_col1, #T_2cc63_row5_col2, #T_2cc63_row5_col3, #T_2cc63_row5_col4, #T_2cc63_row5_col5, #T_2cc63_row5_col6, #T_2cc63_row5_col7, #T_2cc63_row5_col8, #T_2cc63_row5_col9, #T_2cc63_row6_col0, #T_2cc63_row6_col1, #T_2cc63_row6_col2, #T_2cc63_row6_col3, #T_2cc63_row6_col4, #T_2cc63_row6_col5, #T_2cc63_row6_col6, #T_2cc63_row6_col7, #T_2cc63_row6_col8, #T_2cc63_row6_col9, #T_2cc63_row7_col0, #T_2cc63_row7_col1, #T_2cc63_row7_col2, #T_2cc63_row7_col3, #T_2cc63_row7_col4, #T_2cc63_row7_col5, #T_2cc63_row7_col6, #T_2cc63_row7_col7, #T_2cc63_row7_col8, #T_2cc63_row7_col9, #T_2cc63_row8_col0, #T_2cc63_row8_col1, #T_2cc63_row8_col2, #T_2cc63_row8_col3, #T_2cc63_row8_col4, #T_2cc63_row8_col5, #T_2cc63_row8_col6, #T_2cc63_row8_col7, #T_2cc63_row8_col8, #T_2cc63_row8_col9, #T_2cc63_row9_col0, #T_2cc63_row9_col1, #T_2cc63_row9_col2, #T_2cc63_row9_col3, #T_2cc63_row9_col4, #T_2cc63_row9_col5, #T_2cc63_row9_col6, #T_2cc63_row9_col7, #T_2cc63_row9_col8, #T_2cc63_row9_col9, #T_2cc63_row10_col0, #T_2cc63_row10_col1, #T_2cc63_row10_col2, #T_2cc63_row10_col3, #T_2cc63_row10_col4, #T_2cc63_row10_col5, #T_2cc63_row10_col6, #T_2cc63_row10_col7, #T_2cc63_row10_col8, #T_2cc63_row10_col9, #T_2cc63_row11_col0, #T_2cc63_row11_col1, #T_2cc63_row11_col2, #T_2cc63_row11_col3, #T_2cc63_row11_col4, #T_2cc63_row11_col5, #T_2cc63_row11_col6, #T_2cc63_row11_col7, #T_2cc63_row11_col8, #T_2cc63_row11_col9 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2cc63\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2cc63_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_2cc63_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th id=\"T_2cc63_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n",
       "      <th id=\"T_2cc63_level0_col3\" class=\"col_heading level0 col3\" >RMSE</th>\n",
       "      <th id=\"T_2cc63_level0_col4\" class=\"col_heading level0 col4\" >R2</th>\n",
       "      <th id=\"T_2cc63_level0_col5\" class=\"col_heading level0 col5\" >RMSLE</th>\n",
       "      <th id=\"T_2cc63_level0_col6\" class=\"col_heading level0 col6\" >MAPE</th>\n",
       "      <th id=\"T_2cc63_level0_col7\" class=\"col_heading level0 col7\" >MMAE</th>\n",
       "      <th id=\"T_2cc63_level0_col8\" class=\"col_heading level0 col8\" >EVS</th>\n",
       "      <th id=\"T_2cc63_level0_col9\" class=\"col_heading level0 col9\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2cc63_level0_row0\" class=\"row_heading level0 row0\" >lr</th>\n",
       "      <td id=\"T_2cc63_row0_col0\" class=\"data row0 col0\" >Linear Regression</td>\n",
       "      <td id=\"T_2cc63_row0_col1\" class=\"data row0 col1\" >1.1720</td>\n",
       "      <td id=\"T_2cc63_row0_col2\" class=\"data row0 col2\" >1.8576</td>\n",
       "      <td id=\"T_2cc63_row0_col3\" class=\"data row0 col3\" >1.3629</td>\n",
       "      <td id=\"T_2cc63_row0_col4\" class=\"data row0 col4\" >0.9533</td>\n",
       "      <td id=\"T_2cc63_row0_col5\" class=\"data row0 col5\" >0.1895</td>\n",
       "      <td id=\"T_2cc63_row0_col6\" class=\"data row0 col6\" >0.1949</td>\n",
       "      <td id=\"T_2cc63_row0_col7\" class=\"data row0 col7\" >0.0000</td>\n",
       "      <td id=\"T_2cc63_row0_col8\" class=\"data row0 col8\" >0.9533</td>\n",
       "      <td id=\"T_2cc63_row0_col9\" class=\"data row0 col9\" >3.4760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2cc63_level0_row1\" class=\"row_heading level0 row1\" >lasso</th>\n",
       "      <td id=\"T_2cc63_row1_col0\" class=\"data row1 col0\" >Lasso Regression</td>\n",
       "      <td id=\"T_2cc63_row1_col1\" class=\"data row1 col1\" >3.6868</td>\n",
       "      <td id=\"T_2cc63_row1_col2\" class=\"data row1 col2\" >22.3707</td>\n",
       "      <td id=\"T_2cc63_row1_col3\" class=\"data row1 col3\" >4.7297</td>\n",
       "      <td id=\"T_2cc63_row1_col4\" class=\"data row1 col4\" >0.4379</td>\n",
       "      <td id=\"T_2cc63_row1_col5\" class=\"data row1 col5\" >0.4618</td>\n",
       "      <td id=\"T_2cc63_row1_col6\" class=\"data row1 col6\" >0.5867</td>\n",
       "      <td id=\"T_2cc63_row1_col7\" class=\"data row1 col7\" >0.0000</td>\n",
       "      <td id=\"T_2cc63_row1_col8\" class=\"data row1 col8\" >0.4379</td>\n",
       "      <td id=\"T_2cc63_row1_col9\" class=\"data row1 col9\" >8.9540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2cc63_level0_row2\" class=\"row_heading level0 row2\" >ridge</th>\n",
       "      <td id=\"T_2cc63_row2_col0\" class=\"data row2 col0\" >Ridge Regression</td>\n",
       "      <td id=\"T_2cc63_row2_col1\" class=\"data row2 col1\" >1.1719</td>\n",
       "      <td id=\"T_2cc63_row2_col2\" class=\"data row2 col2\" >1.8575</td>\n",
       "      <td id=\"T_2cc63_row2_col3\" class=\"data row2 col3\" >1.3629</td>\n",
       "      <td id=\"T_2cc63_row2_col4\" class=\"data row2 col4\" >0.9533</td>\n",
       "      <td id=\"T_2cc63_row2_col5\" class=\"data row2 col5\" >0.1895</td>\n",
       "      <td id=\"T_2cc63_row2_col6\" class=\"data row2 col6\" >0.1949</td>\n",
       "      <td id=\"T_2cc63_row2_col7\" class=\"data row2 col7\" >0.0000</td>\n",
       "      <td id=\"T_2cc63_row2_col8\" class=\"data row2 col8\" >0.9533</td>\n",
       "      <td id=\"T_2cc63_row2_col9\" class=\"data row2 col9\" >1.3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2cc63_level0_row3\" class=\"row_heading level0 row3\" >en</th>\n",
       "      <td id=\"T_2cc63_row3_col0\" class=\"data row3 col0\" >Elastic Net</td>\n",
       "      <td id=\"T_2cc63_row3_col1\" class=\"data row3 col1\" >3.5791</td>\n",
       "      <td id=\"T_2cc63_row3_col2\" class=\"data row3 col2\" >21.2938</td>\n",
       "      <td id=\"T_2cc63_row3_col3\" class=\"data row3 col3\" >4.6145</td>\n",
       "      <td id=\"T_2cc63_row3_col4\" class=\"data row3 col4\" >0.4650</td>\n",
       "      <td id=\"T_2cc63_row3_col5\" class=\"data row3 col5\" >0.4508</td>\n",
       "      <td id=\"T_2cc63_row3_col6\" class=\"data row3 col6\" >0.5670</td>\n",
       "      <td id=\"T_2cc63_row3_col7\" class=\"data row3 col7\" >0.0000</td>\n",
       "      <td id=\"T_2cc63_row3_col8\" class=\"data row3 col8\" >0.4650</td>\n",
       "      <td id=\"T_2cc63_row3_col9\" class=\"data row3 col9\" >9.9580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2cc63_level0_row4\" class=\"row_heading level0 row4\" >lar</th>\n",
       "      <td id=\"T_2cc63_row4_col0\" class=\"data row4 col0\" >Least Angle Regression</td>\n",
       "      <td id=\"T_2cc63_row4_col1\" class=\"data row4 col1\" >4278.6178</td>\n",
       "      <td id=\"T_2cc63_row4_col2\" class=\"data row4 col2\" >483353936.5085</td>\n",
       "      <td id=\"T_2cc63_row4_col3\" class=\"data row4 col3\" >11210.3347</td>\n",
       "      <td id=\"T_2cc63_row4_col4\" class=\"data row4 col4\" >-12167758.0035</td>\n",
       "      <td id=\"T_2cc63_row4_col5\" class=\"data row4 col5\" >3.3020</td>\n",
       "      <td id=\"T_2cc63_row4_col6\" class=\"data row4 col6\" >599.2209</td>\n",
       "      <td id=\"T_2cc63_row4_col7\" class=\"data row4 col7\" >0.0000</td>\n",
       "      <td id=\"T_2cc63_row4_col8\" class=\"data row4 col8\" >-12165471.6844</td>\n",
       "      <td id=\"T_2cc63_row4_col9\" class=\"data row4 col9\" >1.3180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2cc63_level0_row5\" class=\"row_heading level0 row5\" >llar</th>\n",
       "      <td id=\"T_2cc63_row5_col0\" class=\"data row5 col0\" >Lasso Least Angle Regression</td>\n",
       "      <td id=\"T_2cc63_row5_col1\" class=\"data row5 col1\" >3.6868</td>\n",
       "      <td id=\"T_2cc63_row5_col2\" class=\"data row5 col2\" >22.3707</td>\n",
       "      <td id=\"T_2cc63_row5_col3\" class=\"data row5 col3\" >4.7297</td>\n",
       "      <td id=\"T_2cc63_row5_col4\" class=\"data row5 col4\" >0.4379</td>\n",
       "      <td id=\"T_2cc63_row5_col5\" class=\"data row5 col5\" >0.4618</td>\n",
       "      <td id=\"T_2cc63_row5_col6\" class=\"data row5 col6\" >0.5867</td>\n",
       "      <td id=\"T_2cc63_row5_col7\" class=\"data row5 col7\" >0.0000</td>\n",
       "      <td id=\"T_2cc63_row5_col8\" class=\"data row5 col8\" >0.4379</td>\n",
       "      <td id=\"T_2cc63_row5_col9\" class=\"data row5 col9\" >1.3970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2cc63_level0_row6\" class=\"row_heading level0 row6\" >omp</th>\n",
       "      <td id=\"T_2cc63_row6_col0\" class=\"data row6 col0\" >Orthogonal Matching Pursuit</td>\n",
       "      <td id=\"T_2cc63_row6_col1\" class=\"data row6 col1\" >4.5831</td>\n",
       "      <td id=\"T_2cc63_row6_col2\" class=\"data row6 col2\" >33.8224</td>\n",
       "      <td id=\"T_2cc63_row6_col3\" class=\"data row6 col3\" >5.8141</td>\n",
       "      <td id=\"T_2cc63_row6_col4\" class=\"data row6 col4\" >0.1503</td>\n",
       "      <td id=\"T_2cc63_row6_col5\" class=\"data row6 col5\" >0.5760</td>\n",
       "      <td id=\"T_2cc63_row6_col6\" class=\"data row6 col6\" >0.8491</td>\n",
       "      <td id=\"T_2cc63_row6_col7\" class=\"data row6 col7\" >0.0000</td>\n",
       "      <td id=\"T_2cc63_row6_col8\" class=\"data row6 col8\" >0.1503</td>\n",
       "      <td id=\"T_2cc63_row6_col9\" class=\"data row6 col9\" >1.3470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2cc63_level0_row7\" class=\"row_heading level0 row7\" >br</th>\n",
       "      <td id=\"T_2cc63_row7_col0\" class=\"data row7 col0\" >Bayesian Ridge</td>\n",
       "      <td id=\"T_2cc63_row7_col1\" class=\"data row7 col1\" >1.1719</td>\n",
       "      <td id=\"T_2cc63_row7_col2\" class=\"data row7 col2\" >1.8576</td>\n",
       "      <td id=\"T_2cc63_row7_col3\" class=\"data row7 col3\" >1.3629</td>\n",
       "      <td id=\"T_2cc63_row7_col4\" class=\"data row7 col4\" >0.9533</td>\n",
       "      <td id=\"T_2cc63_row7_col5\" class=\"data row7 col5\" >0.1895</td>\n",
       "      <td id=\"T_2cc63_row7_col6\" class=\"data row7 col6\" >0.1949</td>\n",
       "      <td id=\"T_2cc63_row7_col7\" class=\"data row7 col7\" >0.0000</td>\n",
       "      <td id=\"T_2cc63_row7_col8\" class=\"data row7 col8\" >0.9533</td>\n",
       "      <td id=\"T_2cc63_row7_col9\" class=\"data row7 col9\" >2.1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2cc63_level0_row8\" class=\"row_heading level0 row8\" >par</th>\n",
       "      <td id=\"T_2cc63_row8_col0\" class=\"data row8 col0\" >Passive Aggressive Regressor</td>\n",
       "      <td id=\"T_2cc63_row8_col1\" class=\"data row8 col1\" >8.6709</td>\n",
       "      <td id=\"T_2cc63_row8_col2\" class=\"data row8 col2\" >200.5125</td>\n",
       "      <td id=\"T_2cc63_row8_col3\" class=\"data row8 col3\" >13.1302</td>\n",
       "      <td id=\"T_2cc63_row8_col4\" class=\"data row8 col4\" >-4.0462</td>\n",
       "      <td id=\"T_2cc63_row8_col5\" class=\"data row8 col5\" >0.8404</td>\n",
       "      <td id=\"T_2cc63_row8_col6\" class=\"data row8 col6\" >1.4758</td>\n",
       "      <td id=\"T_2cc63_row8_col7\" class=\"data row8 col7\" >0.0000</td>\n",
       "      <td id=\"T_2cc63_row8_col8\" class=\"data row8 col8\" >-2.6881</td>\n",
       "      <td id=\"T_2cc63_row8_col9\" class=\"data row8 col9\" >1.9740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2cc63_level0_row9\" class=\"row_heading level0 row9\" >huber</th>\n",
       "      <td id=\"T_2cc63_row9_col0\" class=\"data row9 col0\" >Huber Regressor</td>\n",
       "      <td id=\"T_2cc63_row9_col1\" class=\"data row9 col1\" >5.3909</td>\n",
       "      <td id=\"T_2cc63_row9_col2\" class=\"data row9 col2\" >79.5219</td>\n",
       "      <td id=\"T_2cc63_row9_col3\" class=\"data row9 col3\" >8.8061</td>\n",
       "      <td id=\"T_2cc63_row9_col4\" class=\"data row9 col4\" >-0.9977</td>\n",
       "      <td id=\"T_2cc63_row9_col5\" class=\"data row9 col5\" >0.6229</td>\n",
       "      <td id=\"T_2cc63_row9_col6\" class=\"data row9 col6\" >0.8508</td>\n",
       "      <td id=\"T_2cc63_row9_col7\" class=\"data row9 col7\" >0.0000</td>\n",
       "      <td id=\"T_2cc63_row9_col8\" class=\"data row9 col8\" >-0.9705</td>\n",
       "      <td id=\"T_2cc63_row9_col9\" class=\"data row9 col9\" >8.1330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2cc63_level0_row10\" class=\"row_heading level0 row10\" >knn</th>\n",
       "      <td id=\"T_2cc63_row10_col0\" class=\"data row10 col0\" >K Neighbors Regressor</td>\n",
       "      <td id=\"T_2cc63_row10_col1\" class=\"data row10 col1\" >4.7613</td>\n",
       "      <td id=\"T_2cc63_row10_col2\" class=\"data row10 col2\" >36.7593</td>\n",
       "      <td id=\"T_2cc63_row10_col3\" class=\"data row10 col3\" >6.0629</td>\n",
       "      <td id=\"T_2cc63_row10_col4\" class=\"data row10 col4\" >0.0764</td>\n",
       "      <td id=\"T_2cc63_row10_col5\" class=\"data row10 col5\" >0.5831</td>\n",
       "      <td id=\"T_2cc63_row10_col6\" class=\"data row10 col6\" >0.8191</td>\n",
       "      <td id=\"T_2cc63_row10_col7\" class=\"data row10 col7\" >0.0000</td>\n",
       "      <td id=\"T_2cc63_row10_col8\" class=\"data row10 col8\" >0.0768</td>\n",
       "      <td id=\"T_2cc63_row10_col9\" class=\"data row10 col9\" >6.8260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2cc63_level0_row11\" class=\"row_heading level0 row11\" >dt</th>\n",
       "      <td id=\"T_2cc63_row11_col0\" class=\"data row11 col0\" >Decision Tree Regressor</td>\n",
       "      <td id=\"T_2cc63_row11_col1\" class=\"data row11 col1\" >1.4943</td>\n",
       "      <td id=\"T_2cc63_row11_col2\" class=\"data row11 col2\" >3.6378</td>\n",
       "      <td id=\"T_2cc63_row11_col3\" class=\"data row11 col3\" >1.9073</td>\n",
       "      <td id=\"T_2cc63_row11_col4\" class=\"data row11 col4\" >0.9086</td>\n",
       "      <td id=\"T_2cc63_row11_col5\" class=\"data row11 col5\" >0.2573</td>\n",
       "      <td id=\"T_2cc63_row11_col6\" class=\"data row11 col6\" >0.2286</td>\n",
       "      <td id=\"T_2cc63_row11_col7\" class=\"data row11 col7\" >0.0000</td>\n",
       "      <td id=\"T_2cc63_row11_col8\" class=\"data row11 col8\" >0.9086</td>\n",
       "      <td id=\"T_2cc63_row11_col9\" class=\"data row11 col9\" >3.5090</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16b84c2eca0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[39], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m best_subgrade_model \u001B[38;5;241m=\u001B[39m \u001B[43mcompare_models\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msvm\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdummy\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgbr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mMMAE\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m save_model(best_subgrade_model,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel_subgrade_untuned_mmae\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m tuned_best_subgrade_model, tuner\u001B[38;5;241m=\u001B[39mtune_model(best_subgrade_model,optimize\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMMAE\u001B[39m\u001B[38;5;124m'\u001B[39m,tuner_verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,return_tuner\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\utils\\generic.py:964\u001B[0m, in \u001B[0;36mcheck_if_global_is_not_none.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    962\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m globals_d[name] \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    963\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(message)\n\u001B[1;32m--> 964\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\regression\\functional.py:805\u001B[0m, in \u001B[0;36mcompare_models\u001B[1;34m(include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, engine, verbose, parallel)\u001B[0m\n\u001B[0;32m    670\u001B[0m \u001B[38;5;129m@check_if_global_is_not_none\u001B[39m(\u001B[38;5;28mglobals\u001B[39m(), _CURRENT_EXPERIMENT_DECORATOR_DICT)\n\u001B[0;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompare_models\u001B[39m(\n\u001B[0;32m    672\u001B[0m     include: Optional[List[Union[\u001B[38;5;28mstr\u001B[39m, Any]]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    687\u001B[0m     parallel: Optional[ParallelBackend] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    688\u001B[0m ):\n\u001B[0;32m    689\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    690\u001B[0m \u001B[38;5;124;03m    This function trains and evaluates performance of all estimators available in the\u001B[39;00m\n\u001B[0;32m    691\u001B[0m \u001B[38;5;124;03m    model library using cross validation. The output of this function is a score grid\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    803\u001B[0m \n\u001B[0;32m    804\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 805\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_CURRENT_EXPERIMENT\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompare_models\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    806\u001B[0m \u001B[43m        \u001B[49m\u001B[43minclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    807\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    808\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    809\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    810\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcross_validation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_validation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    811\u001B[0m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    812\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_select\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_select\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    813\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbudget_time\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbudget_time\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    814\u001B[0m \u001B[43m        \u001B[49m\u001B[43mturbo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mturbo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    815\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    816\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    817\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    818\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexperiment_custom_tags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexperiment_custom_tags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    819\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    820\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    821\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparallel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparallel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    822\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\regression\\oop.py:1122\u001B[0m, in \u001B[0;36mRegressionExperiment.compare_models\u001B[1;34m(self, include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, engine, verbose, parallel)\u001B[0m\n\u001B[0;32m   1119\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_engine(estimator\u001B[38;5;241m=\u001B[39mestimator, engine\u001B[38;5;241m=\u001B[39meng, severity\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124merror\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1121\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1122\u001B[0m     return_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompare_models\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1123\u001B[0m \u001B[43m        \u001B[49m\u001B[43minclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1124\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1125\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1126\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1127\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcross_validation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_validation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1128\u001B[0m \u001B[43m        \u001B[49m\u001B[43msort\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1129\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_select\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_select\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1130\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbudget_time\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbudget_time\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[43m        \u001B[49m\u001B[43mturbo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mturbo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1132\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1133\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1134\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1135\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexperiment_custom_tags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexperiment_custom_tags\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1136\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1137\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparallel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparallel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1138\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcaller_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcaller_params\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m   1142\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m engine \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1143\u001B[0m         \u001B[38;5;66;03m# Reset the models back to the default engines\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:794\u001B[0m, in \u001B[0;36m_SupervisedExperiment.compare_models\u001B[1;34m(self, include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, probability_threshold, verbose, parallel, caller_params)\u001B[0m\n\u001B[0;32m    791\u001B[0m results_columns_to_ignore \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mObject\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mruntime\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcutoff\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    793\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 794\u001B[0m     model, model_fit_time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcreate_model_args)\n\u001B[0;32m    795\u001B[0m     model_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpull(pop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    796\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m (\n\u001B[0;32m    797\u001B[0m         np\u001B[38;5;241m.\u001B[39msum(\n\u001B[0;32m    798\u001B[0m             model_results\u001B[38;5;241m.\u001B[39mdrop(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    802\u001B[0m         \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m    803\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:1533\u001B[0m, in \u001B[0;36m_SupervisedExperiment._create_model\u001B[1;34m(self, estimator, fold, round, cross_validation, predict, fit_kwargs, groups, refit, probability_threshold, experiment_custom_tags, verbose, system, add_to_model_list, X_train_data, y_train_data, metrics, display, model_only, return_train_score, error_score, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m model, model_fit_time\n\u001B[0;32m   1531\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\u001B[1;32m-> 1533\u001B[0m model, model_fit_time, model_results, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_model_with_cv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1534\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1535\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_X\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_X\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1536\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_y\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1537\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfit_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1538\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mround\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1539\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1540\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1541\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1542\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrefit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrefit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1543\u001B[0m \u001B[43m    \u001B[49m\u001B[43msystem\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1544\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdisplay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisplay\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1545\u001B[0m \u001B[43m    \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1546\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1547\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1549\u001B[0m \u001B[38;5;66;03m# end runtime\u001B[39;00m\n\u001B[0;32m   1550\u001B[0m runtime_end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\pycaret\\internal\\pycaret_experiment\\supervised_experiment.py:1126\u001B[0m, in \u001B[0;36m_SupervisedExperiment._create_model_with_cv\u001B[1;34m(self, model, data_X, data_y, fit_kwargs, round, cv, groups, metrics, refit, system, display, error_score, return_train_score)\u001B[0m\n\u001B[0;32m   1124\u001B[0m     model_fit_start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   1125\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m redirect_output(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger):\n\u001B[1;32m-> 1126\u001B[0m         scores \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1127\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpipeline_with_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1128\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata_X\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1129\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdata_y\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1130\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1132\u001B[0m \u001B[43m            \u001B[49m\u001B[43mscoring\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetrics_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1133\u001B[0m \u001B[43m            \u001B[49m\u001B[43mparams\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfit_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1134\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1135\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1136\u001B[0m \u001B[43m            \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1137\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1139\u001B[0m model_fit_end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   1140\u001B[0m model_fit_time \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(model_fit_end \u001B[38;5;241m-\u001B[39m model_fit_start)\u001B[38;5;241m.\u001B[39mround(\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    211\u001B[0m         )\n\u001B[0;32m    212\u001B[0m     ):\n\u001B[1;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    223\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:430\u001B[0m, in \u001B[0;36mcross_validate\u001B[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001B[39;00m\n\u001B[0;32m    428\u001B[0m \u001B[38;5;66;03m# independent, and that it is pickle-able.\u001B[39;00m\n\u001B[0;32m    429\u001B[0m parallel \u001B[38;5;241m=\u001B[39m Parallel(n_jobs\u001B[38;5;241m=\u001B[39mn_jobs, verbose\u001B[38;5;241m=\u001B[39mverbose, pre_dispatch\u001B[38;5;241m=\u001B[39mpre_dispatch)\n\u001B[1;32m--> 430\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mparallel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    431\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_fit_and_score\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    432\u001B[0m \u001B[43m        \u001B[49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    433\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    434\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    435\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscorer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscorers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    436\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    437\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    438\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    439\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparameters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    440\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfit_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mestimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    441\u001B[0m \u001B[43m        \u001B[49m\u001B[43mscore_params\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrouted_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscorer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    442\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_train_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_train_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    443\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_times\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    444\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_estimator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_estimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    445\u001B[0m \u001B[43m        \u001B[49m\u001B[43merror_score\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_score\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    446\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    447\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\n\u001B[0;32m    448\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    450\u001B[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001B[0;32m    452\u001B[0m \u001B[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001B[39;00m\n\u001B[0;32m    453\u001B[0m \u001B[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001B[39;00m\n\u001B[0;32m    454\u001B[0m \u001B[38;5;66;03m# the correct key.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m     62\u001B[0m config \u001B[38;5;241m=\u001B[39m get_config()\n\u001B[0;32m     63\u001B[0m iterable_with_config \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     64\u001B[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001B[0;32m     65\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m delayed_func, args, kwargs \u001B[38;5;129;01min\u001B[39;00m iterable\n\u001B[0;32m     66\u001B[0m )\n\u001B[1;32m---> 67\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43miterable_with_config\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\joblib\\parallel.py:1952\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[1;34m(self, iterable)\u001B[0m\n\u001B[0;32m   1946\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[0;32m   1947\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[0;32m   1948\u001B[0m \u001B[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001B[39;00m\n\u001B[0;32m   1949\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[0;32m   1950\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[1;32m-> 1952\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\joblib\\parallel.py:1595\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[1;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[0;32m   1592\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[0;32m   1594\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[1;32m-> 1595\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[0;32m   1597\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[0;32m   1598\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[0;32m   1599\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[0;32m   1600\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[0;32m   1601\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\lib\\site-packages\\joblib\\parallel.py:1707\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1702\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[0;32m   1703\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[0;32m   1704\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[0;32m   1705\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[0;32m   1706\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[1;32m-> 1707\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1708\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m   1710\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[0;32m   1711\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[0;32m   1712\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "best_subgrade_model = compare_models(exclude=['svm', 'dummy', 'gbr'],\n",
    "                                     sort='MMAE')\n",
    "save_model(best_subgrade_model, 'model_subgrade_untuned_mmae')\n",
    "tuned_best_subgrade_model, tuner = tune_model(best_subgrade_model,\n",
    "                                              optimize='MMAE', tuner_verbose=3,\n",
    "                                              return_tuner=True)\n",
    "save_model(tuned_best_subgrade_model, 'model_subgrade_tuned_mmae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d91b7430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2d8fa th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_2d8fa_row0_col0, #T_2d8fa_row0_col6, #T_2d8fa_row1_col0, #T_2d8fa_row1_col1, #T_2d8fa_row1_col2, #T_2d8fa_row1_col3, #T_2d8fa_row1_col4, #T_2d8fa_row1_col5, #T_2d8fa_row2_col0, #T_2d8fa_row2_col1, #T_2d8fa_row2_col2, #T_2d8fa_row2_col3, #T_2d8fa_row2_col4, #T_2d8fa_row2_col5, #T_2d8fa_row2_col6, #T_2d8fa_row3_col0, #T_2d8fa_row3_col1, #T_2d8fa_row3_col2, #T_2d8fa_row3_col3, #T_2d8fa_row3_col4, #T_2d8fa_row3_col5, #T_2d8fa_row3_col6, #T_2d8fa_row4_col0, #T_2d8fa_row4_col1, #T_2d8fa_row4_col2, #T_2d8fa_row4_col3, #T_2d8fa_row4_col4, #T_2d8fa_row4_col5, #T_2d8fa_row4_col6, #T_2d8fa_row5_col0, #T_2d8fa_row5_col1, #T_2d8fa_row5_col2, #T_2d8fa_row5_col3, #T_2d8fa_row5_col4, #T_2d8fa_row5_col5, #T_2d8fa_row5_col6, #T_2d8fa_row6_col0, #T_2d8fa_row6_col1, #T_2d8fa_row6_col2, #T_2d8fa_row6_col3, #T_2d8fa_row6_col4, #T_2d8fa_row6_col5, #T_2d8fa_row6_col6, #T_2d8fa_row7_col0, #T_2d8fa_row7_col1, #T_2d8fa_row7_col2, #T_2d8fa_row7_col3, #T_2d8fa_row7_col4, #T_2d8fa_row7_col5, #T_2d8fa_row7_col6, #T_2d8fa_row8_col0, #T_2d8fa_row8_col1, #T_2d8fa_row8_col2, #T_2d8fa_row8_col3, #T_2d8fa_row8_col4, #T_2d8fa_row8_col5, #T_2d8fa_row8_col6, #T_2d8fa_row9_col0, #T_2d8fa_row9_col1, #T_2d8fa_row9_col2, #T_2d8fa_row9_col3, #T_2d8fa_row9_col4, #T_2d8fa_row9_col5, #T_2d8fa_row9_col6, #T_2d8fa_row10_col0, #T_2d8fa_row10_col1, #T_2d8fa_row10_col2, #T_2d8fa_row10_col3, #T_2d8fa_row10_col4, #T_2d8fa_row10_col5, #T_2d8fa_row10_col6, #T_2d8fa_row11_col0, #T_2d8fa_row11_col1, #T_2d8fa_row11_col2, #T_2d8fa_row11_col3, #T_2d8fa_row11_col4, #T_2d8fa_row11_col5, #T_2d8fa_row11_col6, #T_2d8fa_row12_col0, #T_2d8fa_row12_col1, #T_2d8fa_row12_col2, #T_2d8fa_row12_col3, #T_2d8fa_row12_col4, #T_2d8fa_row12_col5, #T_2d8fa_row12_col6, #T_2d8fa_row13_col0, #T_2d8fa_row13_col1, #T_2d8fa_row13_col2, #T_2d8fa_row13_col3, #T_2d8fa_row13_col4, #T_2d8fa_row13_col5, #T_2d8fa_row13_col6, #T_2d8fa_row14_col0, #T_2d8fa_row14_col1, #T_2d8fa_row14_col2, #T_2d8fa_row14_col3, #T_2d8fa_row14_col4, #T_2d8fa_row14_col5, #T_2d8fa_row14_col6, #T_2d8fa_row15_col0, #T_2d8fa_row15_col1, #T_2d8fa_row15_col2, #T_2d8fa_row15_col3, #T_2d8fa_row15_col4, #T_2d8fa_row15_col5, #T_2d8fa_row15_col6, #T_2d8fa_row16_col0, #T_2d8fa_row16_col1, #T_2d8fa_row16_col2, #T_2d8fa_row16_col3, #T_2d8fa_row16_col4, #T_2d8fa_row16_col5, #T_2d8fa_row16_col6 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_2d8fa_row0_col1, #T_2d8fa_row0_col2, #T_2d8fa_row0_col3, #T_2d8fa_row0_col4, #T_2d8fa_row0_col5, #T_2d8fa_row0_col7, #T_2d8fa_row1_col6, #T_2d8fa_row1_col7, #T_2d8fa_row2_col7, #T_2d8fa_row3_col7, #T_2d8fa_row4_col7, #T_2d8fa_row5_col7, #T_2d8fa_row6_col7, #T_2d8fa_row7_col7, #T_2d8fa_row8_col7, #T_2d8fa_row9_col7, #T_2d8fa_row10_col7, #T_2d8fa_row11_col7, #T_2d8fa_row12_col7, #T_2d8fa_row13_col7, #T_2d8fa_row14_col7, #T_2d8fa_row15_col7, #T_2d8fa_row16_col7 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_2d8fa_row0_col8, #T_2d8fa_row1_col8, #T_2d8fa_row2_col8, #T_2d8fa_row3_col8, #T_2d8fa_row4_col8, #T_2d8fa_row5_col8, #T_2d8fa_row6_col8, #T_2d8fa_row7_col8, #T_2d8fa_row8_col8, #T_2d8fa_row9_col8, #T_2d8fa_row10_col8, #T_2d8fa_row12_col8, #T_2d8fa_row13_col8, #T_2d8fa_row14_col8, #T_2d8fa_row15_col8, #T_2d8fa_row16_col8 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_2d8fa_row11_col8 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2d8fa\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2d8fa_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_2d8fa_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th id=\"T_2d8fa_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n",
       "      <th id=\"T_2d8fa_level0_col3\" class=\"col_heading level0 col3\" >RMSE</th>\n",
       "      <th id=\"T_2d8fa_level0_col4\" class=\"col_heading level0 col4\" >R2</th>\n",
       "      <th id=\"T_2d8fa_level0_col5\" class=\"col_heading level0 col5\" >RMSLE</th>\n",
       "      <th id=\"T_2d8fa_level0_col6\" class=\"col_heading level0 col6\" >MAPE</th>\n",
       "      <th id=\"T_2d8fa_level0_col7\" class=\"col_heading level0 col7\" >MMAE</th>\n",
       "      <th id=\"T_2d8fa_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row0\" class=\"row_heading level0 row0\" >lightgbm</th>\n",
       "      <td id=\"T_2d8fa_row0_col0\" class=\"data row0 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_2d8fa_row0_col1\" class=\"data row0 col1\" >1.1456</td>\n",
       "      <td id=\"T_2d8fa_row0_col2\" class=\"data row0 col2\" >1.7871</td>\n",
       "      <td id=\"T_2d8fa_row0_col3\" class=\"data row0 col3\" >1.3368</td>\n",
       "      <td id=\"T_2d8fa_row0_col4\" class=\"data row0 col4\" >0.9551</td>\n",
       "      <td id=\"T_2d8fa_row0_col5\" class=\"data row0 col5\" >0.1803</td>\n",
       "      <td id=\"T_2d8fa_row0_col6\" class=\"data row0 col6\" >0.1810</td>\n",
       "      <td id=\"T_2d8fa_row0_col7\" class=\"data row0 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row0_col8\" class=\"data row0 col8\" >2.5610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row1\" class=\"row_heading level0 row1\" >xgboost</th>\n",
       "      <td id=\"T_2d8fa_row1_col0\" class=\"data row1 col0\" >Extreme Gradient Boosting</td>\n",
       "      <td id=\"T_2d8fa_row1_col1\" class=\"data row1 col1\" >1.1474</td>\n",
       "      <td id=\"T_2d8fa_row1_col2\" class=\"data row1 col2\" >1.8138</td>\n",
       "      <td id=\"T_2d8fa_row1_col3\" class=\"data row1 col3\" >1.3468</td>\n",
       "      <td id=\"T_2d8fa_row1_col4\" class=\"data row1 col4\" >0.9544</td>\n",
       "      <td id=\"T_2d8fa_row1_col5\" class=\"data row1 col5\" >0.1814</td>\n",
       "      <td id=\"T_2d8fa_row1_col6\" class=\"data row1 col6\" >0.1803</td>\n",
       "      <td id=\"T_2d8fa_row1_col7\" class=\"data row1 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row1_col8\" class=\"data row1 col8\" >3.5790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row2\" class=\"row_heading level0 row2\" >rf</th>\n",
       "      <td id=\"T_2d8fa_row2_col0\" class=\"data row2 col0\" >Random Forest Regressor</td>\n",
       "      <td id=\"T_2d8fa_row2_col1\" class=\"data row2 col1\" >1.1573</td>\n",
       "      <td id=\"T_2d8fa_row2_col2\" class=\"data row2 col2\" >1.8280</td>\n",
       "      <td id=\"T_2d8fa_row2_col3\" class=\"data row2 col3\" >1.3520</td>\n",
       "      <td id=\"T_2d8fa_row2_col4\" class=\"data row2 col4\" >0.9541</td>\n",
       "      <td id=\"T_2d8fa_row2_col5\" class=\"data row2 col5\" >0.1832</td>\n",
       "      <td id=\"T_2d8fa_row2_col6\" class=\"data row2 col6\" >0.1843</td>\n",
       "      <td id=\"T_2d8fa_row2_col7\" class=\"data row2 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row2_col8\" class=\"data row2 col8\" >109.5160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row3\" class=\"row_heading level0 row3\" >et</th>\n",
       "      <td id=\"T_2d8fa_row3_col0\" class=\"data row3 col0\" >Extra Trees Regressor</td>\n",
       "      <td id=\"T_2d8fa_row3_col1\" class=\"data row3 col1\" >1.1587</td>\n",
       "      <td id=\"T_2d8fa_row3_col2\" class=\"data row3 col2\" >1.8416</td>\n",
       "      <td id=\"T_2d8fa_row3_col3\" class=\"data row3 col3\" >1.3571</td>\n",
       "      <td id=\"T_2d8fa_row3_col4\" class=\"data row3 col4\" >0.9537</td>\n",
       "      <td id=\"T_2d8fa_row3_col5\" class=\"data row3 col5\" >0.1832</td>\n",
       "      <td id=\"T_2d8fa_row3_col6\" class=\"data row3 col6\" >0.1840</td>\n",
       "      <td id=\"T_2d8fa_row3_col7\" class=\"data row3 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row3_col8\" class=\"data row3 col8\" >102.1220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row4\" class=\"row_heading level0 row4\" >lr</th>\n",
       "      <td id=\"T_2d8fa_row4_col0\" class=\"data row4 col0\" >Linear Regression</td>\n",
       "      <td id=\"T_2d8fa_row4_col1\" class=\"data row4 col1\" >1.1720</td>\n",
       "      <td id=\"T_2d8fa_row4_col2\" class=\"data row4 col2\" >1.8576</td>\n",
       "      <td id=\"T_2d8fa_row4_col3\" class=\"data row4 col3\" >1.3629</td>\n",
       "      <td id=\"T_2d8fa_row4_col4\" class=\"data row4 col4\" >0.9533</td>\n",
       "      <td id=\"T_2d8fa_row4_col5\" class=\"data row4 col5\" >0.1895</td>\n",
       "      <td id=\"T_2d8fa_row4_col6\" class=\"data row4 col6\" >0.1949</td>\n",
       "      <td id=\"T_2d8fa_row4_col7\" class=\"data row4 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row4_col8\" class=\"data row4 col8\" >2.2630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row5\" class=\"row_heading level0 row5\" >ridge</th>\n",
       "      <td id=\"T_2d8fa_row5_col0\" class=\"data row5 col0\" >Ridge Regression</td>\n",
       "      <td id=\"T_2d8fa_row5_col1\" class=\"data row5 col1\" >1.1719</td>\n",
       "      <td id=\"T_2d8fa_row5_col2\" class=\"data row5 col2\" >1.8575</td>\n",
       "      <td id=\"T_2d8fa_row5_col3\" class=\"data row5 col3\" >1.3629</td>\n",
       "      <td id=\"T_2d8fa_row5_col4\" class=\"data row5 col4\" >0.9533</td>\n",
       "      <td id=\"T_2d8fa_row5_col5\" class=\"data row5 col5\" >0.1895</td>\n",
       "      <td id=\"T_2d8fa_row5_col6\" class=\"data row5 col6\" >0.1949</td>\n",
       "      <td id=\"T_2d8fa_row5_col7\" class=\"data row5 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row5_col8\" class=\"data row5 col8\" >1.3390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row6\" class=\"row_heading level0 row6\" >br</th>\n",
       "      <td id=\"T_2d8fa_row6_col0\" class=\"data row6 col0\" >Bayesian Ridge</td>\n",
       "      <td id=\"T_2d8fa_row6_col1\" class=\"data row6 col1\" >1.1719</td>\n",
       "      <td id=\"T_2d8fa_row6_col2\" class=\"data row6 col2\" >1.8576</td>\n",
       "      <td id=\"T_2d8fa_row6_col3\" class=\"data row6 col3\" >1.3629</td>\n",
       "      <td id=\"T_2d8fa_row6_col4\" class=\"data row6 col4\" >0.9533</td>\n",
       "      <td id=\"T_2d8fa_row6_col5\" class=\"data row6 col5\" >0.1895</td>\n",
       "      <td id=\"T_2d8fa_row6_col6\" class=\"data row6 col6\" >0.1949</td>\n",
       "      <td id=\"T_2d8fa_row6_col7\" class=\"data row6 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row6_col8\" class=\"data row6 col8\" >2.1210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row7\" class=\"row_heading level0 row7\" >ada</th>\n",
       "      <td id=\"T_2d8fa_row7_col0\" class=\"data row7 col0\" >AdaBoost Regressor</td>\n",
       "      <td id=\"T_2d8fa_row7_col1\" class=\"data row7 col1\" >1.3323</td>\n",
       "      <td id=\"T_2d8fa_row7_col2\" class=\"data row7 col2\" >2.5199</td>\n",
       "      <td id=\"T_2d8fa_row7_col3\" class=\"data row7 col3\" >1.5844</td>\n",
       "      <td id=\"T_2d8fa_row7_col4\" class=\"data row7 col4\" >0.9367</td>\n",
       "      <td id=\"T_2d8fa_row7_col5\" class=\"data row7 col5\" >0.1996</td>\n",
       "      <td id=\"T_2d8fa_row7_col6\" class=\"data row7 col6\" >0.2004</td>\n",
       "      <td id=\"T_2d8fa_row7_col7\" class=\"data row7 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row7_col8\" class=\"data row7 col8\" >35.9320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row8\" class=\"row_heading level0 row8\" >dt</th>\n",
       "      <td id=\"T_2d8fa_row8_col0\" class=\"data row8 col0\" >Decision Tree Regressor</td>\n",
       "      <td id=\"T_2d8fa_row8_col1\" class=\"data row8 col1\" >1.4943</td>\n",
       "      <td id=\"T_2d8fa_row8_col2\" class=\"data row8 col2\" >3.6378</td>\n",
       "      <td id=\"T_2d8fa_row8_col3\" class=\"data row8 col3\" >1.9073</td>\n",
       "      <td id=\"T_2d8fa_row8_col4\" class=\"data row8 col4\" >0.9086</td>\n",
       "      <td id=\"T_2d8fa_row8_col5\" class=\"data row8 col5\" >0.2573</td>\n",
       "      <td id=\"T_2d8fa_row8_col6\" class=\"data row8 col6\" >0.2286</td>\n",
       "      <td id=\"T_2d8fa_row8_col7\" class=\"data row8 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row8_col8\" class=\"data row8 col8\" >3.5620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row9\" class=\"row_heading level0 row9\" >en</th>\n",
       "      <td id=\"T_2d8fa_row9_col0\" class=\"data row9 col0\" >Elastic Net</td>\n",
       "      <td id=\"T_2d8fa_row9_col1\" class=\"data row9 col1\" >3.5791</td>\n",
       "      <td id=\"T_2d8fa_row9_col2\" class=\"data row9 col2\" >21.2938</td>\n",
       "      <td id=\"T_2d8fa_row9_col3\" class=\"data row9 col3\" >4.6145</td>\n",
       "      <td id=\"T_2d8fa_row9_col4\" class=\"data row9 col4\" >0.4650</td>\n",
       "      <td id=\"T_2d8fa_row9_col5\" class=\"data row9 col5\" >0.4508</td>\n",
       "      <td id=\"T_2d8fa_row9_col6\" class=\"data row9 col6\" >0.5670</td>\n",
       "      <td id=\"T_2d8fa_row9_col7\" class=\"data row9 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row9_col8\" class=\"data row9 col8\" >10.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row10\" class=\"row_heading level0 row10\" >lasso</th>\n",
       "      <td id=\"T_2d8fa_row10_col0\" class=\"data row10 col0\" >Lasso Regression</td>\n",
       "      <td id=\"T_2d8fa_row10_col1\" class=\"data row10 col1\" >3.6868</td>\n",
       "      <td id=\"T_2d8fa_row10_col2\" class=\"data row10 col2\" >22.3707</td>\n",
       "      <td id=\"T_2d8fa_row10_col3\" class=\"data row10 col3\" >4.7297</td>\n",
       "      <td id=\"T_2d8fa_row10_col4\" class=\"data row10 col4\" >0.4379</td>\n",
       "      <td id=\"T_2d8fa_row10_col5\" class=\"data row10 col5\" >0.4618</td>\n",
       "      <td id=\"T_2d8fa_row10_col6\" class=\"data row10 col6\" >0.5867</td>\n",
       "      <td id=\"T_2d8fa_row10_col7\" class=\"data row10 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row10_col8\" class=\"data row10 col8\" >9.3590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row11\" class=\"row_heading level0 row11\" >llar</th>\n",
       "      <td id=\"T_2d8fa_row11_col0\" class=\"data row11 col0\" >Lasso Least Angle Regression</td>\n",
       "      <td id=\"T_2d8fa_row11_col1\" class=\"data row11 col1\" >3.6868</td>\n",
       "      <td id=\"T_2d8fa_row11_col2\" class=\"data row11 col2\" >22.3707</td>\n",
       "      <td id=\"T_2d8fa_row11_col3\" class=\"data row11 col3\" >4.7297</td>\n",
       "      <td id=\"T_2d8fa_row11_col4\" class=\"data row11 col4\" >0.4379</td>\n",
       "      <td id=\"T_2d8fa_row11_col5\" class=\"data row11 col5\" >0.4618</td>\n",
       "      <td id=\"T_2d8fa_row11_col6\" class=\"data row11 col6\" >0.5867</td>\n",
       "      <td id=\"T_2d8fa_row11_col7\" class=\"data row11 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row11_col8\" class=\"data row11 col8\" >1.3090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row12\" class=\"row_heading level0 row12\" >omp</th>\n",
       "      <td id=\"T_2d8fa_row12_col0\" class=\"data row12 col0\" >Orthogonal Matching Pursuit</td>\n",
       "      <td id=\"T_2d8fa_row12_col1\" class=\"data row12 col1\" >4.5831</td>\n",
       "      <td id=\"T_2d8fa_row12_col2\" class=\"data row12 col2\" >33.8224</td>\n",
       "      <td id=\"T_2d8fa_row12_col3\" class=\"data row12 col3\" >5.8141</td>\n",
       "      <td id=\"T_2d8fa_row12_col4\" class=\"data row12 col4\" >0.1503</td>\n",
       "      <td id=\"T_2d8fa_row12_col5\" class=\"data row12 col5\" >0.5760</td>\n",
       "      <td id=\"T_2d8fa_row12_col6\" class=\"data row12 col6\" >0.8491</td>\n",
       "      <td id=\"T_2d8fa_row12_col7\" class=\"data row12 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row12_col8\" class=\"data row12 col8\" >1.3370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row13\" class=\"row_heading level0 row13\" >knn</th>\n",
       "      <td id=\"T_2d8fa_row13_col0\" class=\"data row13 col0\" >K Neighbors Regressor</td>\n",
       "      <td id=\"T_2d8fa_row13_col1\" class=\"data row13 col1\" >4.7613</td>\n",
       "      <td id=\"T_2d8fa_row13_col2\" class=\"data row13 col2\" >36.7593</td>\n",
       "      <td id=\"T_2d8fa_row13_col3\" class=\"data row13 col3\" >6.0629</td>\n",
       "      <td id=\"T_2d8fa_row13_col4\" class=\"data row13 col4\" >0.0764</td>\n",
       "      <td id=\"T_2d8fa_row13_col5\" class=\"data row13 col5\" >0.5831</td>\n",
       "      <td id=\"T_2d8fa_row13_col6\" class=\"data row13 col6\" >0.8191</td>\n",
       "      <td id=\"T_2d8fa_row13_col7\" class=\"data row13 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row13_col8\" class=\"data row13 col8\" >6.9230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row14\" class=\"row_heading level0 row14\" >huber</th>\n",
       "      <td id=\"T_2d8fa_row14_col0\" class=\"data row14 col0\" >Huber Regressor</td>\n",
       "      <td id=\"T_2d8fa_row14_col1\" class=\"data row14 col1\" >5.3909</td>\n",
       "      <td id=\"T_2d8fa_row14_col2\" class=\"data row14 col2\" >79.5219</td>\n",
       "      <td id=\"T_2d8fa_row14_col3\" class=\"data row14 col3\" >8.8061</td>\n",
       "      <td id=\"T_2d8fa_row14_col4\" class=\"data row14 col4\" >-0.9977</td>\n",
       "      <td id=\"T_2d8fa_row14_col5\" class=\"data row14 col5\" >0.6229</td>\n",
       "      <td id=\"T_2d8fa_row14_col6\" class=\"data row14 col6\" >0.8508</td>\n",
       "      <td id=\"T_2d8fa_row14_col7\" class=\"data row14 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row14_col8\" class=\"data row14 col8\" >8.1880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row15\" class=\"row_heading level0 row15\" >par</th>\n",
       "      <td id=\"T_2d8fa_row15_col0\" class=\"data row15 col0\" >Passive Aggressive Regressor</td>\n",
       "      <td id=\"T_2d8fa_row15_col1\" class=\"data row15 col1\" >8.6709</td>\n",
       "      <td id=\"T_2d8fa_row15_col2\" class=\"data row15 col2\" >200.5125</td>\n",
       "      <td id=\"T_2d8fa_row15_col3\" class=\"data row15 col3\" >13.1302</td>\n",
       "      <td id=\"T_2d8fa_row15_col4\" class=\"data row15 col4\" >-4.0462</td>\n",
       "      <td id=\"T_2d8fa_row15_col5\" class=\"data row15 col5\" >0.8404</td>\n",
       "      <td id=\"T_2d8fa_row15_col6\" class=\"data row15 col6\" >1.4758</td>\n",
       "      <td id=\"T_2d8fa_row15_col7\" class=\"data row15 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row15_col8\" class=\"data row15 col8\" >1.9620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2d8fa_level0_row16\" class=\"row_heading level0 row16\" >lar</th>\n",
       "      <td id=\"T_2d8fa_row16_col0\" class=\"data row16 col0\" >Least Angle Regression</td>\n",
       "      <td id=\"T_2d8fa_row16_col1\" class=\"data row16 col1\" >4278.6178</td>\n",
       "      <td id=\"T_2d8fa_row16_col2\" class=\"data row16 col2\" >483353936.5085</td>\n",
       "      <td id=\"T_2d8fa_row16_col3\" class=\"data row16 col3\" >11210.3347</td>\n",
       "      <td id=\"T_2d8fa_row16_col4\" class=\"data row16 col4\" >-12167758.0035</td>\n",
       "      <td id=\"T_2d8fa_row16_col5\" class=\"data row16 col5\" >3.3020</td>\n",
       "      <td id=\"T_2d8fa_row16_col6\" class=\"data row16 col6\" >599.2209</td>\n",
       "      <td id=\"T_2d8fa_row16_col7\" class=\"data row16 col7\" >0.0000</td>\n",
       "      <td id=\"T_2d8fa_row16_col8\" class=\"data row16 col8\" >1.3250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16b84c45700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_21a99_row10_col0, #T_21a99_row10_col1, #T_21a99_row10_col2, #T_21a99_row10_col3, #T_21a99_row10_col4, #T_21a99_row10_col5, #T_21a99_row10_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_21a99\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_21a99_level0_col0\" class=\"col_heading level0 col0\" >MAE</th>\n",
       "      <th id=\"T_21a99_level0_col1\" class=\"col_heading level0 col1\" >MSE</th>\n",
       "      <th id=\"T_21a99_level0_col2\" class=\"col_heading level0 col2\" >RMSE</th>\n",
       "      <th id=\"T_21a99_level0_col3\" class=\"col_heading level0 col3\" >R2</th>\n",
       "      <th id=\"T_21a99_level0_col4\" class=\"col_heading level0 col4\" >RMSLE</th>\n",
       "      <th id=\"T_21a99_level0_col5\" class=\"col_heading level0 col5\" >MAPE</th>\n",
       "      <th id=\"T_21a99_level0_col6\" class=\"col_heading level0 col6\" >MMAE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_21a99_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_21a99_row0_col0\" class=\"data row0 col0\" >1.1632</td>\n",
       "      <td id=\"T_21a99_row0_col1\" class=\"data row0 col1\" >1.8284</td>\n",
       "      <td id=\"T_21a99_row0_col2\" class=\"data row0 col2\" >1.3522</td>\n",
       "      <td id=\"T_21a99_row0_col3\" class=\"data row0 col3\" >0.9533</td>\n",
       "      <td id=\"T_21a99_row0_col4\" class=\"data row0 col4\" >0.1843</td>\n",
       "      <td id=\"T_21a99_row0_col5\" class=\"data row0 col5\" >0.1876</td>\n",
       "      <td id=\"T_21a99_row0_col6\" class=\"data row0 col6\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_21a99_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_21a99_row1_col0\" class=\"data row1 col0\" >1.1646</td>\n",
       "      <td id=\"T_21a99_row1_col1\" class=\"data row1 col1\" >1.8388</td>\n",
       "      <td id=\"T_21a99_row1_col2\" class=\"data row1 col2\" >1.3560</td>\n",
       "      <td id=\"T_21a99_row1_col3\" class=\"data row1 col3\" >0.9539</td>\n",
       "      <td id=\"T_21a99_row1_col4\" class=\"data row1 col4\" >0.1860</td>\n",
       "      <td id=\"T_21a99_row1_col5\" class=\"data row1 col5\" >0.1888</td>\n",
       "      <td id=\"T_21a99_row1_col6\" class=\"data row1 col6\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_21a99_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_21a99_row2_col0\" class=\"data row2 col0\" >1.1621</td>\n",
       "      <td id=\"T_21a99_row2_col1\" class=\"data row2 col1\" >1.8232</td>\n",
       "      <td id=\"T_21a99_row2_col2\" class=\"data row2 col2\" >1.3503</td>\n",
       "      <td id=\"T_21a99_row2_col3\" class=\"data row2 col3\" >0.9538</td>\n",
       "      <td id=\"T_21a99_row2_col4\" class=\"data row2 col4\" >0.1848</td>\n",
       "      <td id=\"T_21a99_row2_col5\" class=\"data row2 col5\" >0.1880</td>\n",
       "      <td id=\"T_21a99_row2_col6\" class=\"data row2 col6\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_21a99_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_21a99_row3_col0\" class=\"data row3 col0\" >1.1575</td>\n",
       "      <td id=\"T_21a99_row3_col1\" class=\"data row3 col1\" >1.8110</td>\n",
       "      <td id=\"T_21a99_row3_col2\" class=\"data row3 col2\" >1.3457</td>\n",
       "      <td id=\"T_21a99_row3_col3\" class=\"data row3 col3\" >0.9541</td>\n",
       "      <td id=\"T_21a99_row3_col4\" class=\"data row3 col4\" >0.1864</td>\n",
       "      <td id=\"T_21a99_row3_col5\" class=\"data row3 col5\" >0.1906</td>\n",
       "      <td id=\"T_21a99_row3_col6\" class=\"data row3 col6\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_21a99_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_21a99_row4_col0\" class=\"data row4 col0\" >1.1568</td>\n",
       "      <td id=\"T_21a99_row4_col1\" class=\"data row4 col1\" >1.8103</td>\n",
       "      <td id=\"T_21a99_row4_col2\" class=\"data row4 col2\" >1.3455</td>\n",
       "      <td id=\"T_21a99_row4_col3\" class=\"data row4 col3\" >0.9547</td>\n",
       "      <td id=\"T_21a99_row4_col4\" class=\"data row4 col4\" >0.1815</td>\n",
       "      <td id=\"T_21a99_row4_col5\" class=\"data row4 col5\" >0.1824</td>\n",
       "      <td id=\"T_21a99_row4_col6\" class=\"data row4 col6\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_21a99_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_21a99_row5_col0\" class=\"data row5 col0\" >1.1526</td>\n",
       "      <td id=\"T_21a99_row5_col1\" class=\"data row5 col1\" >1.7983</td>\n",
       "      <td id=\"T_21a99_row5_col2\" class=\"data row5 col2\" >1.3410</td>\n",
       "      <td id=\"T_21a99_row5_col3\" class=\"data row5 col3\" >0.9554</td>\n",
       "      <td id=\"T_21a99_row5_col4\" class=\"data row5 col4\" >0.1836</td>\n",
       "      <td id=\"T_21a99_row5_col5\" class=\"data row5 col5\" >0.1862</td>\n",
       "      <td id=\"T_21a99_row5_col6\" class=\"data row5 col6\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_21a99_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_21a99_row6_col0\" class=\"data row6 col0\" >1.1611</td>\n",
       "      <td id=\"T_21a99_row6_col1\" class=\"data row6 col1\" >1.8165</td>\n",
       "      <td id=\"T_21a99_row6_col2\" class=\"data row6 col2\" >1.3478</td>\n",
       "      <td id=\"T_21a99_row6_col3\" class=\"data row6 col3\" >0.9545</td>\n",
       "      <td id=\"T_21a99_row6_col4\" class=\"data row6 col4\" >0.1833</td>\n",
       "      <td id=\"T_21a99_row6_col5\" class=\"data row6 col5\" >0.1873</td>\n",
       "      <td id=\"T_21a99_row6_col6\" class=\"data row6 col6\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_21a99_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_21a99_row7_col0\" class=\"data row7 col0\" >1.1643</td>\n",
       "      <td id=\"T_21a99_row7_col1\" class=\"data row7 col1\" >1.8278</td>\n",
       "      <td id=\"T_21a99_row7_col2\" class=\"data row7 col2\" >1.3520</td>\n",
       "      <td id=\"T_21a99_row7_col3\" class=\"data row7 col3\" >0.9543</td>\n",
       "      <td id=\"T_21a99_row7_col4\" class=\"data row7 col4\" >0.1805</td>\n",
       "      <td id=\"T_21a99_row7_col5\" class=\"data row7 col5\" >0.1817</td>\n",
       "      <td id=\"T_21a99_row7_col6\" class=\"data row7 col6\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_21a99_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_21a99_row8_col0\" class=\"data row8 col0\" >1.1557</td>\n",
       "      <td id=\"T_21a99_row8_col1\" class=\"data row8 col1\" >1.8094</td>\n",
       "      <td id=\"T_21a99_row8_col2\" class=\"data row8 col2\" >1.3451</td>\n",
       "      <td id=\"T_21a99_row8_col3\" class=\"data row8 col3\" >0.9542</td>\n",
       "      <td id=\"T_21a99_row8_col4\" class=\"data row8 col4\" >0.1833</td>\n",
       "      <td id=\"T_21a99_row8_col5\" class=\"data row8 col5\" >0.1853</td>\n",
       "      <td id=\"T_21a99_row8_col6\" class=\"data row8 col6\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_21a99_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_21a99_row9_col0\" class=\"data row9 col0\" >1.1586</td>\n",
       "      <td id=\"T_21a99_row9_col1\" class=\"data row9 col1\" >1.8133</td>\n",
       "      <td id=\"T_21a99_row9_col2\" class=\"data row9 col2\" >1.3466</td>\n",
       "      <td id=\"T_21a99_row9_col3\" class=\"data row9 col3\" >0.9550</td>\n",
       "      <td id=\"T_21a99_row9_col4\" class=\"data row9 col4\" >0.1822</td>\n",
       "      <td id=\"T_21a99_row9_col5\" class=\"data row9 col5\" >0.1851</td>\n",
       "      <td id=\"T_21a99_row9_col6\" class=\"data row9 col6\" >-0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_21a99_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_21a99_row10_col0\" class=\"data row10 col0\" >1.1596</td>\n",
       "      <td id=\"T_21a99_row10_col1\" class=\"data row10 col1\" >1.8177</td>\n",
       "      <td id=\"T_21a99_row10_col2\" class=\"data row10 col2\" >1.3482</td>\n",
       "      <td id=\"T_21a99_row10_col3\" class=\"data row10 col3\" >0.9543</td>\n",
       "      <td id=\"T_21a99_row10_col4\" class=\"data row10 col4\" >0.1836</td>\n",
       "      <td id=\"T_21a99_row10_col5\" class=\"data row10 col5\" >0.1863</td>\n",
       "      <td id=\"T_21a99_row10_col6\" class=\"data row10 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_21a99_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_21a99_row11_col0\" class=\"data row11 col0\" >0.0038</td>\n",
       "      <td id=\"T_21a99_row11_col1\" class=\"data row11 col1\" >0.0112</td>\n",
       "      <td id=\"T_21a99_row11_col2\" class=\"data row11 col2\" >0.0042</td>\n",
       "      <td id=\"T_21a99_row11_col3\" class=\"data row11 col3\" >0.0006</td>\n",
       "      <td id=\"T_21a99_row11_col4\" class=\"data row11 col4\" >0.0018</td>\n",
       "      <td id=\"T_21a99_row11_col5\" class=\"data row11 col5\" >0.0026</td>\n",
       "      <td id=\"T_21a99_row11_col6\" class=\"data row11 col6\" >0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16be4e9dee0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n",
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(memory=Memory(location=None),\n",
       "          steps=[('numerical_imputer',\n",
       "                  TransformerWrapper(include=['Number_Trades_24_Months',\n",
       "                                              'Annual_Income', 'Total_Open_Buy',\n",
       "                                              'Number_Delinquent_2_Years',\n",
       "                                              'Delinquent_Amount',\n",
       "                                              'Debt_To_Income_Ratio',\n",
       "                                              'Employment_Length_In_Years',\n",
       "                                              'Loan_Amount', 'Risk_Score',\n",
       "                                              'Inquiries_6_Months',\n",
       "                                              'Mortgage_Accounts', 'Open_Trades',\n",
       "                                              'Revolving_Acco...\n",
       "                                              'Verification_Status'],\n",
       "                                     transformer=OneHotEncoder(cols=['Grade',\n",
       "                                                                     'Home_Ownership',\n",
       "                                                                     'Loan_Purpose',\n",
       "                                                                     'Verification_Status'],\n",
       "                                                               handle_missing='return_nan',\n",
       "                                                               use_cat_names=True))),\n",
       "                 ('rest_encoding',\n",
       "                  TransformerWrapper(include=['State'],\n",
       "                                     transformer=TargetEncoder(cols=['State'],\n",
       "                                                               handle_missing='return_nan'))),\n",
       "                 ('trained_model', LGBMRegressor(n_jobs=-1, random_state=123))]),\n",
       " 'model_subgrade_tuned.pkl')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_subgrade_model = compare_models(exclude=['svm', 'dummy', 'gbr'],\n",
    "                                     sort='RMSE')\n",
    "save_model(best_subgrade_model, 'model_subgrade_untuned')\n",
    "tuned_best_subgrade_model, tuner = tune_model(best_subgrade_model,\n",
    "                                              optimize='RMSE', tuner_verbose=3,\n",
    "                                              return_tuner=True)\n",
    "save_model(tuned_best_subgrade_model, 'model_subgrade_tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "32b1d3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7db2d_row10_col0, #T_7db2d_row10_col1, #T_7db2d_row10_col2, #T_7db2d_row10_col3, #T_7db2d_row10_col4, #T_7db2d_row10_col5 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7db2d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7db2d_level0_col0\" class=\"col_heading level0 col0\" >MAE</th>\n",
       "      <th id=\"T_7db2d_level0_col1\" class=\"col_heading level0 col1\" >MSE</th>\n",
       "      <th id=\"T_7db2d_level0_col2\" class=\"col_heading level0 col2\" >RMSE</th>\n",
       "      <th id=\"T_7db2d_level0_col3\" class=\"col_heading level0 col3\" >R2</th>\n",
       "      <th id=\"T_7db2d_level0_col4\" class=\"col_heading level0 col4\" >RMSLE</th>\n",
       "      <th id=\"T_7db2d_level0_col5\" class=\"col_heading level0 col5\" >MAPE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7db2d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7db2d_row0_col0\" class=\"data row0 col0\" >1.1556</td>\n",
       "      <td id=\"T_7db2d_row0_col1\" class=\"data row0 col1\" >1.8253</td>\n",
       "      <td id=\"T_7db2d_row0_col2\" class=\"data row0 col2\" >1.3510</td>\n",
       "      <td id=\"T_7db2d_row0_col3\" class=\"data row0 col3\" >0.9539</td>\n",
       "      <td id=\"T_7db2d_row0_col4\" class=\"data row0 col4\" >0.1813</td>\n",
       "      <td id=\"T_7db2d_row0_col5\" class=\"data row0 col5\" >0.1818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db2d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7db2d_row1_col0\" class=\"data row1 col0\" >1.1526</td>\n",
       "      <td id=\"T_7db2d_row1_col1\" class=\"data row1 col1\" >1.8076</td>\n",
       "      <td id=\"T_7db2d_row1_col2\" class=\"data row1 col2\" >1.3445</td>\n",
       "      <td id=\"T_7db2d_row1_col3\" class=\"data row1 col3\" >0.9553</td>\n",
       "      <td id=\"T_7db2d_row1_col4\" class=\"data row1 col4\" >0.1784</td>\n",
       "      <td id=\"T_7db2d_row1_col5\" class=\"data row1 col5\" >0.1789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db2d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7db2d_row2_col0\" class=\"data row2 col0\" >1.1487</td>\n",
       "      <td id=\"T_7db2d_row2_col1\" class=\"data row2 col1\" >1.7965</td>\n",
       "      <td id=\"T_7db2d_row2_col2\" class=\"data row2 col2\" >1.3403</td>\n",
       "      <td id=\"T_7db2d_row2_col3\" class=\"data row2 col3\" >0.9554</td>\n",
       "      <td id=\"T_7db2d_row2_col4\" class=\"data row2 col4\" >0.1816</td>\n",
       "      <td id=\"T_7db2d_row2_col5\" class=\"data row2 col5\" >0.1841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db2d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7db2d_row3_col0\" class=\"data row3 col0\" >1.1479</td>\n",
       "      <td id=\"T_7db2d_row3_col1\" class=\"data row3 col1\" >1.7931</td>\n",
       "      <td id=\"T_7db2d_row3_col2\" class=\"data row3 col2\" >1.3391</td>\n",
       "      <td id=\"T_7db2d_row3_col3\" class=\"data row3 col3\" >0.9547</td>\n",
       "      <td id=\"T_7db2d_row3_col4\" class=\"data row3 col4\" >0.1784</td>\n",
       "      <td id=\"T_7db2d_row3_col5\" class=\"data row3 col5\" >0.1799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db2d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7db2d_row4_col0\" class=\"data row4 col0\" >1.1485</td>\n",
       "      <td id=\"T_7db2d_row4_col1\" class=\"data row4 col1\" >1.7959</td>\n",
       "      <td id=\"T_7db2d_row4_col2\" class=\"data row4 col2\" >1.3401</td>\n",
       "      <td id=\"T_7db2d_row4_col3\" class=\"data row4 col3\" >0.9547</td>\n",
       "      <td id=\"T_7db2d_row4_col4\" class=\"data row4 col4\" >0.1795</td>\n",
       "      <td id=\"T_7db2d_row4_col5\" class=\"data row4 col5\" >0.1804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db2d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7db2d_row5_col0\" class=\"data row5 col0\" >1.1497</td>\n",
       "      <td id=\"T_7db2d_row5_col1\" class=\"data row5 col1\" >1.8032</td>\n",
       "      <td id=\"T_7db2d_row5_col2\" class=\"data row5 col2\" >1.3429</td>\n",
       "      <td id=\"T_7db2d_row5_col3\" class=\"data row5 col3\" >0.9546</td>\n",
       "      <td id=\"T_7db2d_row5_col4\" class=\"data row5 col4\" >0.1845</td>\n",
       "      <td id=\"T_7db2d_row5_col5\" class=\"data row5 col5\" >0.1871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db2d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_7db2d_row6_col0\" class=\"data row6 col0\" >1.1561</td>\n",
       "      <td id=\"T_7db2d_row6_col1\" class=\"data row6 col1\" >1.8255</td>\n",
       "      <td id=\"T_7db2d_row6_col2\" class=\"data row6 col2\" >1.3511</td>\n",
       "      <td id=\"T_7db2d_row6_col3\" class=\"data row6 col3\" >0.9547</td>\n",
       "      <td id=\"T_7db2d_row6_col4\" class=\"data row6 col4\" >0.1853</td>\n",
       "      <td id=\"T_7db2d_row6_col5\" class=\"data row6 col5\" >0.1868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db2d_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_7db2d_row7_col0\" class=\"data row7 col0\" >1.1470</td>\n",
       "      <td id=\"T_7db2d_row7_col1\" class=\"data row7 col1\" >1.7979</td>\n",
       "      <td id=\"T_7db2d_row7_col2\" class=\"data row7 col2\" >1.3409</td>\n",
       "      <td id=\"T_7db2d_row7_col3\" class=\"data row7 col3\" >0.9545</td>\n",
       "      <td id=\"T_7db2d_row7_col4\" class=\"data row7 col4\" >0.1815</td>\n",
       "      <td id=\"T_7db2d_row7_col5\" class=\"data row7 col5\" >0.1828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db2d_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_7db2d_row8_col0\" class=\"data row8 col0\" >1.1510</td>\n",
       "      <td id=\"T_7db2d_row8_col1\" class=\"data row8 col1\" >1.8090</td>\n",
       "      <td id=\"T_7db2d_row8_col2\" class=\"data row8 col2\" >1.3450</td>\n",
       "      <td id=\"T_7db2d_row8_col3\" class=\"data row8 col3\" >0.9539</td>\n",
       "      <td id=\"T_7db2d_row8_col4\" class=\"data row8 col4\" >0.1816</td>\n",
       "      <td id=\"T_7db2d_row8_col5\" class=\"data row8 col5\" >0.1806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db2d_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_7db2d_row9_col0\" class=\"data row9 col0\" >1.1524</td>\n",
       "      <td id=\"T_7db2d_row9_col1\" class=\"data row9 col1\" >1.8007</td>\n",
       "      <td id=\"T_7db2d_row9_col2\" class=\"data row9 col2\" >1.3419</td>\n",
       "      <td id=\"T_7db2d_row9_col3\" class=\"data row9 col3\" >0.9547</td>\n",
       "      <td id=\"T_7db2d_row9_col4\" class=\"data row9 col4\" >0.1800</td>\n",
       "      <td id=\"T_7db2d_row9_col5\" class=\"data row9 col5\" >0.1814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db2d_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_7db2d_row10_col0\" class=\"data row10 col0\" >1.1510</td>\n",
       "      <td id=\"T_7db2d_row10_col1\" class=\"data row10 col1\" >1.8055</td>\n",
       "      <td id=\"T_7db2d_row10_col2\" class=\"data row10 col2\" >1.3437</td>\n",
       "      <td id=\"T_7db2d_row10_col3\" class=\"data row10 col3\" >0.9546</td>\n",
       "      <td id=\"T_7db2d_row10_col4\" class=\"data row10 col4\" >0.1812</td>\n",
       "      <td id=\"T_7db2d_row10_col5\" class=\"data row10 col5\" >0.1824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7db2d_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_7db2d_row11_col0\" class=\"data row11 col0\" >0.0030</td>\n",
       "      <td id=\"T_7db2d_row11_col1\" class=\"data row11 col1\" >0.0110</td>\n",
       "      <td id=\"T_7db2d_row11_col2\" class=\"data row11 col2\" >0.0041</td>\n",
       "      <td id=\"T_7db2d_row11_col3\" class=\"data row11 col3\" >0.0005</td>\n",
       "      <td id=\"T_7db2d_row11_col4\" class=\"data row11 col4\" >0.0022</td>\n",
       "      <td id=\"T_7db2d_row11_col5\" class=\"data row11 col5\" >0.0027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2bea77b5ee0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "Original model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n",
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(memory=Memory(location=None),\n",
       "          steps=[('numerical_imputer',\n",
       "                  TransformerWrapper(exclude=None,\n",
       "                                     include=['Number_Trades_24_Months',\n",
       "                                              'Annual_Income', 'Total_Open_Buy',\n",
       "                                              'Number_Delinquent_2_Years',\n",
       "                                              'Delinquent_Amount',\n",
       "                                              'Debt_To_Income_Ratio',\n",
       "                                              'Employment_Length_In_Years',\n",
       "                                              'Loan_Amount', 'Risk_Score',\n",
       "                                              'Inquiries_6_Months',\n",
       "                                              'Mortgage_Accounts', 'Open_Trades',\n",
       "                                              'R...\n",
       "                  LGBMRegressor(boosting_type='gbdt', class_weight=None,\n",
       "                                colsample_bytree=1.0, importance_type='split',\n",
       "                                learning_rate=0.1, max_depth=-1,\n",
       "                                min_child_samples=20, min_child_weight=0.001,\n",
       "                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n",
       "                                num_leaves=31, objective=None, random_state=125,\n",
       "                                reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,\n",
       "                                subsample_for_bin=200000, subsample_freq=0))],\n",
       "          verbose=False),\n",
       " 'model_subgrade_tuned_ord_reg.pkl')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_best_subgrade_pycaret_model, tuner = tune_model(\n",
    "    best_subgrade_pycaret_model, optimize='MSE', tuner_verbose=3,\n",
    "    return_tuner=True)\n",
    "save_model(tuned_best_subgrade_pycaret_model, 'model_subgrade_tuned_ord_reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "30630ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3727cddcce344014857cd4f3d3fa46af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelin"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqYAAAHVCAYAAAAjJTltAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdD0lEQVR4nO3df2zVd7348VdHaXZK4BLDAtFLbk1XRN2PdhSrNxJ0xeyODXCK89dNrjFOTTMc6Mi9Su7NhoLz191NVe4lGoOLOBeJzE03lmn2gyhS5ljmj7tZ2C3jhrgMvIw7WoRun+8fN/R7e4vaT3tsX/Q8HgnJzpv3OedFXoE8aVdaVxRFEQAAMMkumOwBAAAgQpgCAJCEMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhhzGH6u9/9Lt72trfF3r17/+CdRx55JFasWBGtra1x9dVXx0MPPTTWtwMAYIobU5j+/Oc/j3e/+93x7LPP/sE7fX19sWbNmrjpppviscceizVr1sTatWvjueeeG/OwAABMXaXDdOfOnXHzzTfHunXr/uS99vb2WLZsWdTX18fy5ctj8eLFcdddd415WAAApq76sk9485vfHCtWrIj6+vo/GqcHDhyIBQsWDDu7+OKL46mnnhrV++zfvz+Koojp06eXHREAgAlw5syZqKuri7a2tqq8Xukwveiii0Z17+TJk1GpVIadXXjhhdHf3z+q5xdFEUVRxOnTp8uOCADAeah0mI5WpVKJU6dODTs7depUzJgxY1TPnz59epw+fTqamppGBC5Tz8DAQPT19dl3jbDv2mLftcW+a0tvb29ccEH1/pGnP1uYLliwIH71q18NOztw4EBccsklpV6nUqlEY2NjNUcjMfuuLfZdW+y7tth3bairq6vq6/3Z/h3TlStXRk9PT9x3330xODgY9913X/T09MSqVav+XG8JAMB5rKph2tbWFvfcc09ERDQ3N8dXv/rV2Lp1ayxevDi2bNkSX/7yl+PVr351Nd8SAIApYlyfyn/66aeHPd6/f/+wx0uWLIklS5aM5y0AAKgRviUpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApFA6TI8dOxZdXV3R3t4eHR0dsWnTphgcHDzn3W9+85tx5ZVXxhVXXBErVqyIBx54YNwDAwAwNZUO07Vr10ZjY2Ps3r07duzYEXv27Ilt27aNuPfII4/E1q1b4+tf/3o8/vjjceONN8batWvjP//zP6sxNwAAU0x9mcuHDh2Knp6eePTRR6NSqcT8+fOjq6srvvCFL8SHPvShYXefeeaZKIpi6Me0adNi+vTpUV9f6i1jYGCg1H3OT2f3bN+1wb5ri33XFvuuLUVRRF1dXdVer1Ql9vb2xuzZs2Pu3LlDZ83NzXHkyJE4ceJEzJo1a+j8mmuuie9973uxfPnymDZtWtTV1cUXvvCFmDdvXqkB+/r6St3n/GbftcW+a4t91xb7rh0NDQ1Ve61SYXry5MmoVCrDzs4+7u/vHxamZ86ciYULF8amTZti4cKFce+998aGDRuiubk5XvOa14z6PZuamka8J1PPwMBA9PX12XeNsO/aYt+1xb5rS29vb1Vfr1SYNjY2jvjQ/NnHM2bMGHb+6U9/Oq644oq47LLLIiLine98Z/zgBz+InTt3xj/8wz+M+j0rlUo0NjaWGZPzmH3XFvuuLfZdW+y7NlTz0/gRJb/4qaWlJY4fPx5Hjx4dOjt48GDMmzcvZs6cOezukSNH4vTp08PO6uvrY/r06eMYFwCAqapUmDY1NcWiRYti8+bN8eKLL8bhw4djy5YtsXr16hF3r7zyyvjWt74Vv/rVr+Lll1+OXbt2xd69e2P58uVVGx4AgKmj3JfIR0R3d3ds3LgxOjs744ILLoi3v/3t0dXVFRERbW1tceutt8bKlSvjxhtvjGnTpsWaNWvihRdeiL/6q7+Kr371q/Ha17626r8IAADOf6XDdM6cOdHd3X3On9u/f///f+H6+lizZk2sWbNm7NMBAFAzfEtSAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKRQOkyPHTsWXV1d0d7eHh0dHbFp06YYHBw8592enp5417veFW1tbbF06dLYunXruAcGAGBqKh2ma9eujcbGxti9e3fs2LEj9uzZE9u2bRtx7+DBg/HhD3843ve+98Xjjz8eW7dujW984xuxa9euaswNAMAUUypMDx06FD09PbF+/fqoVCoxf/786Orqiu3bt4+4++1vfzs6Ozvjuuuui7q6uli4cGF85zvfiUWLFlVteAAApo76Mpd7e3tj9uzZMXfu3KGz5ubmOHLkSJw4cSJmzZo1dP7kk0/GX//1X8fHP/7x+MlPfhKveMUr4gMf+EC8+93vLjXgwMBAqfucn87u2b5rg33XFvuuLfZdW4qiiLq6uqq9XqkwPXnyZFQqlWFnZx/39/cPC9MXXngh7rjjjrj99tvj85//fOzfvz8+8pGPxF/8xV/E3/zN34z6Pfv6+sqMyHnOvmuLfdcW+64t9l07GhoaqvZapcK0sbFxxN+Azj6eMWPGsPOGhobo7OyMt7zlLRERsXjx4li1alXcf//9pcK0qalpRAwz9QwMDERfX5991wj7ri32XVvsu7b09vZW9fVKhWlLS0scP348jh49GnPmzImI//kip3nz5sXMmTOH3W1ubo7Tp08PO3vppZeiKIpSA1YqlWhsbCz1HM5f9l1b7Lu22Hdtse/aUM1P40eU/OKnpqamWLRoUWzevDlefPHFOHz4cGzZsiVWr1494u573vOe+PGPfxzf//73oyiK2LdvX9x7772xatWqqg0PAMDUUfqfi+ru7o7BwcHo7OyM66+/PpYsWRJdXV0REdHW1hb33HNPRES86U1vii1btsQdd9wRixYtik9+8pPx93//99HZ2VndXwEAAFNCqU/lR0TMmTMnuru7z/lz+/fvH/Z46dKlsXTp0rFNBgBATfEtSQEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACCF0mF67Nix6Orqivb29ujo6IhNmzbF4ODgH33Ob37zm7j88stj7969Yx4UAICprXSYrl27NhobG2P37t2xY8eO2LNnT2zbtu0P3h8YGIhPfOITcerUqfHMCQDAFFcqTA8dOhQ9PT2xfv36qFQqMX/+/Ojq6ort27f/wefceuutsWzZsnEPCgDA1FZf5nJvb2/Mnj075s6dO3TW3NwcR44ciRMnTsSsWbOG3b/77rvj0KFDsWnTptiyZcuYBhwYGBjT8zi/nN2zfdcG+64t9l1b7Lu2FEURdXV1VXu9UmF68uTJqFQqw87OPu7v7x8WpgcPHozbb7897rzzzpg2bdqYB+zr6xvzczn/2Hdtse/aYt+1xb5rR0NDQ9Veq1SYNjY2jvgb0NnHM2bMGDr7/e9/H+vWrYtPfepT8cpXvnJcAzY1NY2IYaaegYGB6Ovrs+8aYd+1xb5ri33Xlt7e3qq+XqkwbWlpiePHj8fRo0djzpw5EfE/HxmdN29ezJw5c+jeL37xi+jr64sNGzbEhg0bhs4/+tGPxqpVq+KWW24Z9XtWKpVobGwsMybnMfuuLfZdW+y7tth3bajmp/EjSoZpU1NTLFq0KDZv3hwbN26M//qv/4otW7bE6tWrh91rb2+PJ598ctjZa17zmvi3f/u36OjoGP/UAABMOaX/uaju7u4YHByMzs7OuP7662PJkiXR1dUVERFtbW1xzz33VH1IAACmvlIfMY2ImDNnTnR3d5/z5/bv3/8Hn/f000+XfSsAAGqIb0kKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKZQO02PHjkVXV1e0t7dHR0dHbNq0KQYHB895984774yrrroq2tra4qqrrort27ePe2AAAKam0mG6du3aaGxsjN27d8eOHTtiz549sW3bthH3fvSjH8U///M/x+c+97l4/PHH47bbbot/+Zd/iQceeKAacwMAMMWUCtNDhw5FT09PrF+/PiqVSsyfPz+6urrO+ZHQ5557Lm644YZobW2Nurq6aGtri46Ojti3b1/VhgcAYOqoL3O5t7c3Zs+eHXPnzh06a25ujiNHjsSJEydi1qxZQ+fvf//7hz332LFjsW/fvvjkJz9ZasCBgYFS9zk/nd2zfdcG+64t9l1b7Lu2FEURdXV1VXu9UmF68uTJqFQqw87OPu7v7x8Wpv/b888/Hx/5yEfikksuiWuvvbbUgH19faXuc36z79pi37XFvmuLfdeOhoaGqr1WqTBtbGwc8Tegs49nzJhxzuc88cQTcdNNN0V7e3t89rOfjfr6Um8ZTU1NI2KYqWdgYCD6+vrsu0bYd22x79pi37Wlt7e3qq9XqhJbWlri+PHjcfTo0ZgzZ05ERBw8eDDmzZsXM2fOHHF/x44d8ZnPfCY+9rGPxQc/+MExDVipVKKxsXFMz+X8Y9+1xb5ri33XFvuuDdX8NH5EyS9+ampqikWLFsXmzZvjxRdfjMOHD8eWLVti9erVI+4+8MADccstt8SXv/zlMUcpAAC1o/Q/F9Xd3R2Dg4PR2dkZ119/fSxZsiS6uroiIqKtrS3uueeeiIj4yle+Ei+99FJ87GMfi7a2tqEf//RP/1TdXwEAAFNCuf/hMyLmzJkT3d3d5/y5/fv3D/33vffeO/apAACoOb4lKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBIQZgCAJCCMAUAIAVhCgBACsIUAIAUhCkAACkIUwAAUhCmAACkIEwBAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBSEKQAAKQhTAABSKB2mx44di66urmhvb4+Ojo7YtGlTDA4OnvPuI488EitWrIjW1ta4+uqr46GHHhr3wAAATE2lw3Tt2rXR2NgYu3fvjh07dsSePXti27ZtI+719fXFmjVr4qabborHHnss1qxZE2vXro3nnnuuGnMDADDFlArTQ4cORU9PT6xfvz4qlUrMnz8/urq6Yvv27SPu7ty5M9rb22PZsmVRX18fy5cvj8WLF8ddd91VteEBAJg66stc7u3tjdmzZ8fcuXOHzpqbm+PIkSNx4sSJmDVr1tD5gQMHYsGCBcOef/HFF8dTTz01qvc6c+bM0HvW1dWVGZPzUFEUEWHftcK+a4t91xb7ri1nzpyp6p5LhenJkyejUqkMOzv7uL+/f1iYnuvuhRdeGP39/aN6r7O/yAsu8PVZtaCuri4aGhomewwmiH3XFvuuLfZdW+rq6iYvTBsbG2NgYGDY2dnHM2bMGHZeqVTi1KlTw85OnTo14t4f0tbWVmY0AADOc6U+HNnS0hLHjx+Po0ePDp0dPHgw5s2bFzNnzhx2d8GCBdHb2zvs7MCBA9HS0jKOcQEAmKpKhWlTU1MsWrQoNm/eHC+++GIcPnw4tmzZEqtXrx5xd+XKldHT0xP33XdfDA4Oxn333Rc9PT2xatWqqg0PAMDUUVec/b+UR+no0aOxcePG2Lt3b1xwwQXx9re/PW6++eaYNm1atLW1xa233horV66MiIjdu3fHF7/4xXj22WfjVa96Vaxfvz6WLl36Z/mFAABwfisdpgAA8OfgS94BAEhBmAIAkIIwBQAgBWEKAEAKwhQAgBQmNUyPHTsWXV1d0d7eHh0dHbFp06YYHBw8591HHnkkVqxYEa2trXH11VfHQw89NMHTMl5l9n3nnXfGVVddFW1tbXHVVVfF9u3bJ3haxqvMvs/6zW9+E5dffnns3bt3gqakWsrsu6enJ971rndFW1tbLF26NLZu3TrB0zJeZfb9zW9+M6688sq44oorYsWKFfHAAw9M8LRUy+9+97t429ve9kf/jB53rxWT6G//9m+LT3ziE0V/f3/x7LPPFtdcc03xta99bcS9//iP/yguvfTS4sEHHyzOnDlT/PCHPywuu+yy4re//e0kTM1YjXbfDz74YNHe3l7s37+/ePnll4vHH3+8aG9vL3bt2jUJUzNWo933Wf39/cW1115bLFiwoPjZz342gZNSDaPd94EDB4rLL7+8+N73vle8/PLLxb//+78Xb3jDG4r7779/EqZmrEa774cffrh405veVBw8eLAoiqLYtWtXsXDhwuLw4cMTPTLj9NhjjxXLli37o39GV6PXJu0jpocOHYqenp5Yv359VCqVmD9/fnR1dZ3zI2M7d+6M9vb2WLZsWdTX18fy5ctj8eLFcdddd03C5IxFmX0/99xzccMNN0Rra2vU1dVFW1tbdHR0xL59+yZhcsaizL7PuvXWW2PZsmUTOCXVUmbf3/72t6OzszOuu+66qKuri4ULF8Z3vvOdWLRo0SRMzliU2fczzzwTRVEM/Zg2bVpMnz496uvrJ2Fyxmrnzp1x8803x7p16/7kvfH22qSFaW9vb8yePTvmzp07dNbc3BxHjhyJEydODLt74MCBWLBgwbCziy++OJ566qkJmZXxK7Pv97///fHhD3946PGxY8di3759cckll0zYvIxPmX1HRNx9991x6NChuPHGGydyTKqkzL6ffPLJ+Mu//Mv4+Mc/Hh0dHXH11VdHT09PXHTRRRM9NmNUZt/XXHNNzJkzJ5YvXx6vf/3r46abborbbrst5s2bN9FjMw5vfvOb48EHH4zly5f/0XvV6LVJC9OTJ09GpVIZdnb2cX9//5+8e+GFF464R15l9v2/Pf/883HDDTfEJZdcEtdee+2fdUaqp8y+Dx48GLfffnt86UtfimnTpk3YjFRPmX2/8MILcccdd8TKlSvjJz/5SWzcuDE+97nPxa5duyZsXsanzL7PnDkTCxcujO9+97vxxBNPxMaNG2PDhg3x9NNPT9i8jN9FF100qo9yV6PXJi1MGxsbY2BgYNjZ2cczZswYdl6pVOLUqVPDzk6dOjXiHnmV2fdZTzzxRKxevTpe/epXx7/+67/61M95ZLT7/v3vfx/r1q2LT33qU/HKV75yQmekesr8/m5oaIjOzs54y1veEvX19bF48eJYtWpV3H///RM2L+NTZt+f/vSno6WlJS677LJoaGiId77zndHa2ho7d+6csHmZONXotUkL05aWljh+/HgcPXp06OzgwYMxb968mDlz5rC7CxYsiN7e3mFnBw4ciJaWlgmZlfErs++IiB07dsQHPvCB+Lu/+7v40pe+FA0NDRM5LuM02n3/4he/iL6+vtiwYUO0t7dHe3t7RER89KMfjVtuuWWix2aMyvz+bm5ujtOnTw87e+mll6IoigmZlfErs+8jR46M2Hd9fX1Mnz59QmZlYlWl16rxlVpj9d73vrdYt25d8d///d9DX9XX3d094t6BAweKSy+9tPjhD3849FVel156afHMM89MwtSM1Wj3vWvXruL1r3998eijj07ClFTLaPf9f/mq/PPTaPf905/+tHjd615X3H333cXLL79c9PT0FK2trcWPfvSjSZiasRrtvm+//faio6Oj+OUvf1m89NJLxf33319ceumlxa9//etJmJpq+GN/Rlej1yY1TJ9//vlizZo1xRve8IbijW98Y3HbbbcVg4ODRVEURWtra/H9739/6O6jjz5arFy5smhtbS2uueaa4uGHH56ssRmj0e772muvLRYuXFi0trYO+/GP//iPkzk+JZX5/f2/CdPzU5l9P/zww8U73vGOoq2trejs7CzuvPPOyRqbMRrtvs+cOVN0d3cXb33rW4srrriiuO6663zQ4Tz3f/+Mrnav1RWFz58AADD5fEtSAABSEKYAAKQgTAEASEGYAgCQgjAFACAFYQoAQArCFACAFIQpAAApCFMAAFIQpgAApCBMAQBI4f8BOpeYccsUcdIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_model(tuned_best_subgrade_pycaret_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7e7b878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_71570\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_71570_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_71570_level0_col1\" class=\"col_heading level0 col1\" >MAE</th>\n",
       "      <th id=\"T_71570_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n",
       "      <th id=\"T_71570_level0_col3\" class=\"col_heading level0 col3\" >RMSE</th>\n",
       "      <th id=\"T_71570_level0_col4\" class=\"col_heading level0 col4\" >R2</th>\n",
       "      <th id=\"T_71570_level0_col5\" class=\"col_heading level0 col5\" >RMSLE</th>\n",
       "      <th id=\"T_71570_level0_col6\" class=\"col_heading level0 col6\" >MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_71570_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_71570_row0_col0\" class=\"data row0 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_71570_row0_col1\" class=\"data row0 col1\" >1.1466</td>\n",
       "      <td id=\"T_71570_row0_col2\" class=\"data row0 col2\" >1.7890</td>\n",
       "      <td id=\"T_71570_row0_col3\" class=\"data row0 col3\" >1.3375</td>\n",
       "      <td id=\"T_71570_row0_col4\" class=\"data row0 col4\" >0.9549</td>\n",
       "      <td id=\"T_71570_row0_col5\" class=\"data row0 col5\" >0.1805</td>\n",
       "      <td id=\"T_71570_row0_col6\" class=\"data row0 col6\" >0.1815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2beb80feee0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_unseen = predict_model(tuned_best_subgrade_pycaret_model,\n",
    "                            pd.concat([X_test, y_test], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "857b29c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sub_Grade_Numerical</th>\n",
       "      <th>prediction_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1900085</th>\n",
       "      <td>2</td>\n",
       "      <td>1.786852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511459</th>\n",
       "      <td>4</td>\n",
       "      <td>3.055329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2115773</th>\n",
       "      <td>9</td>\n",
       "      <td>8.486023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049567</th>\n",
       "      <td>15</td>\n",
       "      <td>13.506929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678720</th>\n",
       "      <td>4</td>\n",
       "      <td>3.634170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542537</th>\n",
       "      <td>14</td>\n",
       "      <td>13.262981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1606177</th>\n",
       "      <td>6</td>\n",
       "      <td>8.061271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2975</th>\n",
       "      <td>8</td>\n",
       "      <td>8.037128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1228869</th>\n",
       "      <td>4</td>\n",
       "      <td>3.412991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078297</th>\n",
       "      <td>7</td>\n",
       "      <td>8.236234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>678201 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sub_Grade_Numerical  prediction_label\n",
       "1900085                    2          1.786852\n",
       "1511459                    4          3.055329\n",
       "2115773                    9          8.486023\n",
       "1049567                   15         13.506929\n",
       "678720                     4          3.634170\n",
       "...                      ...               ...\n",
       "542537                    14         13.262981\n",
       "1606177                    6          8.061271\n",
       "2975                       8          8.037128\n",
       "1228869                    4          3.412991\n",
       "1078297                    7          8.236234\n",
       "\n",
       "[678201 rows x 2 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_unseen[['Sub_Grade_Numerical', 'prediction_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1aaa20c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1546 12638 10000 ...     0     0     0]\n",
      " [  303  6022 10722 ...     0     0     0]\n",
      " [   97  3907 11020 ...     0     0     0]\n",
      " ...\n",
      " [    0     0     0 ...   222     0     0]\n",
      " [    0     0     0 ...   157     0     0]\n",
      " [    0     0     0 ...   180     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Define reverse mapping dictionary\n",
    "reverse_mapping = {\n",
    "    1: 'A1', 2: 'A2', 3: 'A3', 4: 'A4', 5: 'A5',\n",
    "    6: 'B1', 7: 'B2', 8: 'B3', 9: 'B4', 10: 'B5',\n",
    "    11: 'C1', 12: 'C2', 13: 'C3', 14: 'C4', 15: 'C5',\n",
    "    16: 'D1', 17: 'D2', 18: 'D3', 19: 'D4', 20: 'D5',\n",
    "    21: 'E1', 22: 'E2', 23: 'E3', 24: 'E4', 25: 'E5',\n",
    "    26: 'F1', 27: 'F2', 28: 'F3', 29: 'F4', 30: 'F5',\n",
    "    31: 'G1', 32: 'G2', 33: 'G3', 34: 'G4', 35: 'G5'\n",
    "}\n",
    "\n",
    "pred_unseen['rounded_prediction'] = pred_unseen[\n",
    "    'prediction_label'].round().astype(int)\n",
    "\n",
    "# Convert rounded numerical predictions back to subgrade labels\n",
    "pred_unseen['predicted_subgrade'] = pred_unseen['rounded_prediction'].map(\n",
    "    reverse_mapping)\n",
    "pred_unseen['Sub_Grade'] = pred_unseen['Sub_Grade_Numerical'].map(\n",
    "    reverse_mapping)\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(pred_unseen['Sub_Grade'],\n",
    "                            pred_unseen['predicted_subgrade']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a93a7e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1546 12638 10000 ...     0     0     0]\n",
      " [  303  6022 10722 ...     0     0     0]\n",
      " [   97  3907 11020 ...     0     0     0]\n",
      " ...\n",
      " [    0     0     0 ...   222     0     0]\n",
      " [    0     0     0 ...   157     0     0]\n",
      " [    0     0     0 ...   180     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(pred_unseen['Sub_Grade'],\n",
    "                       pred_unseen['predicted_subgrade']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b641366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_960ce th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_960ce_row0_col0, #T_960ce_row0_col4, #T_960ce_row1_col0, #T_960ce_row1_col1, #T_960ce_row1_col3, #T_960ce_row1_col5, #T_960ce_row1_col6, #T_960ce_row1_col7, #T_960ce_row2_col0, #T_960ce_row2_col1, #T_960ce_row2_col3, #T_960ce_row2_col4, #T_960ce_row2_col5, #T_960ce_row2_col6, #T_960ce_row2_col7, #T_960ce_row3_col0, #T_960ce_row3_col1, #T_960ce_row3_col3, #T_960ce_row3_col4, #T_960ce_row3_col5, #T_960ce_row3_col6, #T_960ce_row3_col7, #T_960ce_row4_col0, #T_960ce_row4_col1, #T_960ce_row4_col3, #T_960ce_row4_col4, #T_960ce_row4_col5, #T_960ce_row4_col6, #T_960ce_row4_col7, #T_960ce_row5_col0, #T_960ce_row5_col1, #T_960ce_row5_col3, #T_960ce_row5_col4, #T_960ce_row5_col5, #T_960ce_row5_col6, #T_960ce_row5_col7, #T_960ce_row6_col0, #T_960ce_row6_col1, #T_960ce_row6_col3, #T_960ce_row6_col4, #T_960ce_row6_col5, #T_960ce_row6_col6, #T_960ce_row6_col7, #T_960ce_row7_col0, #T_960ce_row7_col1, #T_960ce_row7_col3, #T_960ce_row7_col4, #T_960ce_row7_col5, #T_960ce_row7_col6, #T_960ce_row7_col7, #T_960ce_row8_col0, #T_960ce_row8_col1, #T_960ce_row8_col3, #T_960ce_row8_col4, #T_960ce_row8_col5, #T_960ce_row8_col6, #T_960ce_row8_col7, #T_960ce_row9_col0, #T_960ce_row9_col1, #T_960ce_row9_col3, #T_960ce_row9_col4, #T_960ce_row9_col5, #T_960ce_row9_col6, #T_960ce_row9_col7, #T_960ce_row10_col0, #T_960ce_row10_col1, #T_960ce_row10_col3, #T_960ce_row10_col4, #T_960ce_row10_col5, #T_960ce_row10_col6, #T_960ce_row10_col7 {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_960ce_row0_col1, #T_960ce_row0_col2, #T_960ce_row0_col3, #T_960ce_row0_col5, #T_960ce_row0_col6, #T_960ce_row0_col7, #T_960ce_row1_col2, #T_960ce_row1_col4, #T_960ce_row2_col2, #T_960ce_row3_col2, #T_960ce_row4_col2, #T_960ce_row5_col2, #T_960ce_row6_col2, #T_960ce_row7_col2, #T_960ce_row8_col2, #T_960ce_row9_col2, #T_960ce_row10_col2 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "}\n",
       "#T_960ce_row0_col8, #T_960ce_row1_col8, #T_960ce_row2_col8, #T_960ce_row3_col8, #T_960ce_row4_col8, #T_960ce_row5_col8, #T_960ce_row7_col8, #T_960ce_row8_col8, #T_960ce_row9_col8, #T_960ce_row10_col8 {\n",
       "  text-align: left;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "#T_960ce_row6_col8 {\n",
       "  text-align: left;\n",
       "  background-color: yellow;\n",
       "  background-color: lightgrey;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_960ce\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_960ce_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_960ce_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_960ce_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_960ce_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_960ce_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_960ce_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_960ce_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_960ce_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "      <th id=\"T_960ce_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_960ce_level0_row0\" class=\"row_heading level0 row0\" >lightgbm</th>\n",
       "      <td id=\"T_960ce_row0_col0\" class=\"data row0 col0\" >Light Gradient Boosting Machine</td>\n",
       "      <td id=\"T_960ce_row0_col1\" class=\"data row0 col1\" >0.3697</td>\n",
       "      <td id=\"T_960ce_row0_col2\" class=\"data row0 col2\" >0.0000</td>\n",
       "      <td id=\"T_960ce_row0_col3\" class=\"data row0 col3\" >0.3697</td>\n",
       "      <td id=\"T_960ce_row0_col4\" class=\"data row0 col4\" >0.3387</td>\n",
       "      <td id=\"T_960ce_row0_col5\" class=\"data row0 col5\" >0.3415</td>\n",
       "      <td id=\"T_960ce_row0_col6\" class=\"data row0 col6\" >0.1971</td>\n",
       "      <td id=\"T_960ce_row0_col7\" class=\"data row0 col7\" >0.2011</td>\n",
       "      <td id=\"T_960ce_row0_col8\" class=\"data row0 col8\" >2.9340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_960ce_level0_row1\" class=\"row_heading level0 row1\" >lda</th>\n",
       "      <td id=\"T_960ce_row1_col0\" class=\"data row1 col0\" >Linear Discriminant Analysis</td>\n",
       "      <td id=\"T_960ce_row1_col1\" class=\"data row1 col1\" >0.3510</td>\n",
       "      <td id=\"T_960ce_row1_col2\" class=\"data row1 col2\" >0.0000</td>\n",
       "      <td id=\"T_960ce_row1_col3\" class=\"data row1 col3\" >0.3510</td>\n",
       "      <td id=\"T_960ce_row1_col4\" class=\"data row1 col4\" >0.3414</td>\n",
       "      <td id=\"T_960ce_row1_col5\" class=\"data row1 col5\" >0.3398</td>\n",
       "      <td id=\"T_960ce_row1_col6\" class=\"data row1 col6\" >0.1837</td>\n",
       "      <td id=\"T_960ce_row1_col7\" class=\"data row1 col7\" >0.1854</td>\n",
       "      <td id=\"T_960ce_row1_col8\" class=\"data row1 col8\" >0.8320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_960ce_level0_row2\" class=\"row_heading level0 row2\" >xgboost</th>\n",
       "      <td id=\"T_960ce_row2_col0\" class=\"data row2 col0\" >Extreme Gradient Boosting</td>\n",
       "      <td id=\"T_960ce_row2_col1\" class=\"data row2 col1\" >0.3578</td>\n",
       "      <td id=\"T_960ce_row2_col2\" class=\"data row2 col2\" >0.0000</td>\n",
       "      <td id=\"T_960ce_row2_col3\" class=\"data row2 col3\" >0.3578</td>\n",
       "      <td id=\"T_960ce_row2_col4\" class=\"data row2 col4\" >0.3321</td>\n",
       "      <td id=\"T_960ce_row2_col5\" class=\"data row2 col5\" >0.3383</td>\n",
       "      <td id=\"T_960ce_row2_col6\" class=\"data row2 col6\" >0.1843</td>\n",
       "      <td id=\"T_960ce_row2_col7\" class=\"data row2 col7\" >0.1863</td>\n",
       "      <td id=\"T_960ce_row2_col8\" class=\"data row2 col8\" >4.8390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_960ce_level0_row3\" class=\"row_heading level0 row3\" >rf</th>\n",
       "      <td id=\"T_960ce_row3_col0\" class=\"data row3 col0\" >Random Forest Classifier</td>\n",
       "      <td id=\"T_960ce_row3_col1\" class=\"data row3 col1\" >0.3524</td>\n",
       "      <td id=\"T_960ce_row3_col2\" class=\"data row3 col2\" >0.0000</td>\n",
       "      <td id=\"T_960ce_row3_col3\" class=\"data row3 col3\" >0.3524</td>\n",
       "      <td id=\"T_960ce_row3_col4\" class=\"data row3 col4\" >0.3241</td>\n",
       "      <td id=\"T_960ce_row3_col5\" class=\"data row3 col5\" >0.3290</td>\n",
       "      <td id=\"T_960ce_row3_col6\" class=\"data row3 col6\" >0.1769</td>\n",
       "      <td id=\"T_960ce_row3_col7\" class=\"data row3 col7\" >0.1796</td>\n",
       "      <td id=\"T_960ce_row3_col8\" class=\"data row3 col8\" >3.6190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_960ce_level0_row4\" class=\"row_heading level0 row4\" >et</th>\n",
       "      <td id=\"T_960ce_row4_col0\" class=\"data row4 col0\" >Extra Trees Classifier</td>\n",
       "      <td id=\"T_960ce_row4_col1\" class=\"data row4 col1\" >0.3513</td>\n",
       "      <td id=\"T_960ce_row4_col2\" class=\"data row4 col2\" >0.0000</td>\n",
       "      <td id=\"T_960ce_row4_col3\" class=\"data row4 col3\" >0.3513</td>\n",
       "      <td id=\"T_960ce_row4_col4\" class=\"data row4 col4\" >0.3227</td>\n",
       "      <td id=\"T_960ce_row4_col5\" class=\"data row4 col5\" >0.3260</td>\n",
       "      <td id=\"T_960ce_row4_col6\" class=\"data row4 col6\" >0.1750</td>\n",
       "      <td id=\"T_960ce_row4_col7\" class=\"data row4 col7\" >0.1781</td>\n",
       "      <td id=\"T_960ce_row4_col8\" class=\"data row4 col8\" >3.6260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_960ce_level0_row5\" class=\"row_heading level0 row5\" >ada</th>\n",
       "      <td id=\"T_960ce_row5_col0\" class=\"data row5 col0\" >Ada Boost Classifier</td>\n",
       "      <td id=\"T_960ce_row5_col1\" class=\"data row5 col1\" >0.3473</td>\n",
       "      <td id=\"T_960ce_row5_col2\" class=\"data row5 col2\" >0.0000</td>\n",
       "      <td id=\"T_960ce_row5_col3\" class=\"data row5 col3\" >0.3473</td>\n",
       "      <td id=\"T_960ce_row5_col4\" class=\"data row5 col4\" >0.3187</td>\n",
       "      <td id=\"T_960ce_row5_col5\" class=\"data row5 col5\" >0.3216</td>\n",
       "      <td id=\"T_960ce_row5_col6\" class=\"data row5 col6\" >0.1698</td>\n",
       "      <td id=\"T_960ce_row5_col7\" class=\"data row5 col7\" >0.1729</td>\n",
       "      <td id=\"T_960ce_row5_col8\" class=\"data row5 col8\" >2.5680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_960ce_level0_row6\" class=\"row_heading level0 row6\" >ridge</th>\n",
       "      <td id=\"T_960ce_row6_col0\" class=\"data row6 col0\" >Ridge Classifier</td>\n",
       "      <td id=\"T_960ce_row6_col1\" class=\"data row6 col1\" >0.3597</td>\n",
       "      <td id=\"T_960ce_row6_col2\" class=\"data row6 col2\" >0.0000</td>\n",
       "      <td id=\"T_960ce_row6_col3\" class=\"data row6 col3\" >0.3597</td>\n",
       "      <td id=\"T_960ce_row6_col4\" class=\"data row6 col4\" >0.3251</td>\n",
       "      <td id=\"T_960ce_row6_col5\" class=\"data row6 col5\" >0.3168</td>\n",
       "      <td id=\"T_960ce_row6_col6\" class=\"data row6 col6\" >0.1864</td>\n",
       "      <td id=\"T_960ce_row6_col7\" class=\"data row6 col7\" >0.1943</td>\n",
       "      <td id=\"T_960ce_row6_col8\" class=\"data row6 col8\" >0.5940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_960ce_level0_row7\" class=\"row_heading level0 row7\" >dt</th>\n",
       "      <td id=\"T_960ce_row7_col0\" class=\"data row7 col0\" >Decision Tree Classifier</td>\n",
       "      <td id=\"T_960ce_row7_col1\" class=\"data row7 col1\" >0.2774</td>\n",
       "      <td id=\"T_960ce_row7_col2\" class=\"data row7 col2\" >0.0000</td>\n",
       "      <td id=\"T_960ce_row7_col3\" class=\"data row7 col3\" >0.2774</td>\n",
       "      <td id=\"T_960ce_row7_col4\" class=\"data row7 col4\" >0.2808</td>\n",
       "      <td id=\"T_960ce_row7_col5\" class=\"data row7 col5\" >0.2788</td>\n",
       "      <td id=\"T_960ce_row7_col6\" class=\"data row7 col6\" >0.0923</td>\n",
       "      <td id=\"T_960ce_row7_col7\" class=\"data row7 col7\" >0.0924</td>\n",
       "      <td id=\"T_960ce_row7_col8\" class=\"data row7 col8\" >1.1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_960ce_level0_row8\" class=\"row_heading level0 row8\" >lr</th>\n",
       "      <td id=\"T_960ce_row8_col0\" class=\"data row8 col0\" >Logistic Regression</td>\n",
       "      <td id=\"T_960ce_row8_col1\" class=\"data row8 col1\" >0.2874</td>\n",
       "      <td id=\"T_960ce_row8_col2\" class=\"data row8 col2\" >0.0000</td>\n",
       "      <td id=\"T_960ce_row8_col3\" class=\"data row8 col3\" >0.2874</td>\n",
       "      <td id=\"T_960ce_row8_col4\" class=\"data row8 col4\" >0.2602</td>\n",
       "      <td id=\"T_960ce_row8_col5\" class=\"data row8 col5\" >0.2531</td>\n",
       "      <td id=\"T_960ce_row8_col6\" class=\"data row8 col6\" >0.0926</td>\n",
       "      <td id=\"T_960ce_row8_col7\" class=\"data row8 col7\" >0.0968</td>\n",
       "      <td id=\"T_960ce_row8_col8\" class=\"data row8 col8\" >4.8440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_960ce_level0_row9\" class=\"row_heading level0 row9\" >knn</th>\n",
       "      <td id=\"T_960ce_row9_col0\" class=\"data row9 col0\" >K Neighbors Classifier</td>\n",
       "      <td id=\"T_960ce_row9_col1\" class=\"data row9 col1\" >0.2165</td>\n",
       "      <td id=\"T_960ce_row9_col2\" class=\"data row9 col2\" >0.0000</td>\n",
       "      <td id=\"T_960ce_row9_col3\" class=\"data row9 col3\" >0.2165</td>\n",
       "      <td id=\"T_960ce_row9_col4\" class=\"data row9 col4\" >0.2295</td>\n",
       "      <td id=\"T_960ce_row9_col5\" class=\"data row9 col5\" >0.2134</td>\n",
       "      <td id=\"T_960ce_row9_col6\" class=\"data row9 col6\" >0.0274</td>\n",
       "      <td id=\"T_960ce_row9_col7\" class=\"data row9 col7\" >0.0279</td>\n",
       "      <td id=\"T_960ce_row9_col8\" class=\"data row9 col8\" >0.8470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_960ce_level0_row10\" class=\"row_heading level0 row10\" >nb</th>\n",
       "      <td id=\"T_960ce_row10_col0\" class=\"data row10 col0\" >Naive Bayes</td>\n",
       "      <td id=\"T_960ce_row10_col1\" class=\"data row10 col1\" >0.1945</td>\n",
       "      <td id=\"T_960ce_row10_col2\" class=\"data row10 col2\" >0.0000</td>\n",
       "      <td id=\"T_960ce_row10_col3\" class=\"data row10 col3\" >0.1945</td>\n",
       "      <td id=\"T_960ce_row10_col4\" class=\"data row10 col4\" >0.2648</td>\n",
       "      <td id=\"T_960ce_row10_col5\" class=\"data row10 col5\" >0.1214</td>\n",
       "      <td id=\"T_960ce_row10_col6\" class=\"data row10 col6\" >0.0271</td>\n",
       "      <td id=\"T_960ce_row10_col7\" class=\"data row10 col7\" >0.0435</td>\n",
       "      <td id=\"T_960ce_row10_col8\" class=\"data row10 col8\" >0.6260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2bf1b17b2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_subgrade_pycaret_model = compare_models(sort='F1',\n",
    "                                             exclude=['dummy', 'gbc', 'svm',\n",
    "                                                      'qda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "333d8337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0e31a_row10_col0, #T_0e31a_row10_col1, #T_0e31a_row10_col2, #T_0e31a_row10_col3, #T_0e31a_row10_col4, #T_0e31a_row10_col5, #T_0e31a_row10_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0e31a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0e31a_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_0e31a_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_0e31a_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_0e31a_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_0e31a_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_0e31a_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_0e31a_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0e31a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0e31a_row0_col0\" class=\"data row0 col0\" >0.3664</td>\n",
       "      <td id=\"T_0e31a_row0_col1\" class=\"data row0 col1\" >0.0000</td>\n",
       "      <td id=\"T_0e31a_row0_col2\" class=\"data row0 col2\" >0.3664</td>\n",
       "      <td id=\"T_0e31a_row0_col3\" class=\"data row0 col3\" >0.3408</td>\n",
       "      <td id=\"T_0e31a_row0_col4\" class=\"data row0 col4\" >0.3456</td>\n",
       "      <td id=\"T_0e31a_row0_col5\" class=\"data row0 col5\" >0.1943</td>\n",
       "      <td id=\"T_0e31a_row0_col6\" class=\"data row0 col6\" >0.1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e31a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_0e31a_row1_col0\" class=\"data row1 col0\" >0.3689</td>\n",
       "      <td id=\"T_0e31a_row1_col1\" class=\"data row1 col1\" >0.0000</td>\n",
       "      <td id=\"T_0e31a_row1_col2\" class=\"data row1 col2\" >0.3689</td>\n",
       "      <td id=\"T_0e31a_row1_col3\" class=\"data row1 col3\" >0.3409</td>\n",
       "      <td id=\"T_0e31a_row1_col4\" class=\"data row1 col4\" >0.3476</td>\n",
       "      <td id=\"T_0e31a_row1_col5\" class=\"data row1 col5\" >0.1987</td>\n",
       "      <td id=\"T_0e31a_row1_col6\" class=\"data row1 col6\" >0.2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e31a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_0e31a_row2_col0\" class=\"data row2 col0\" >0.3639</td>\n",
       "      <td id=\"T_0e31a_row2_col1\" class=\"data row2 col1\" >0.0000</td>\n",
       "      <td id=\"T_0e31a_row2_col2\" class=\"data row2 col2\" >0.3639</td>\n",
       "      <td id=\"T_0e31a_row2_col3\" class=\"data row2 col3\" >0.3372</td>\n",
       "      <td id=\"T_0e31a_row2_col4\" class=\"data row2 col4\" >0.3444</td>\n",
       "      <td id=\"T_0e31a_row2_col5\" class=\"data row2 col5\" >0.1919</td>\n",
       "      <td id=\"T_0e31a_row2_col6\" class=\"data row2 col6\" >0.1938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e31a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_0e31a_row3_col0\" class=\"data row3 col0\" >0.3616</td>\n",
       "      <td id=\"T_0e31a_row3_col1\" class=\"data row3 col1\" >0.0000</td>\n",
       "      <td id=\"T_0e31a_row3_col2\" class=\"data row3 col2\" >0.3616</td>\n",
       "      <td id=\"T_0e31a_row3_col3\" class=\"data row3 col3\" >0.3321</td>\n",
       "      <td id=\"T_0e31a_row3_col4\" class=\"data row3 col4\" >0.3392</td>\n",
       "      <td id=\"T_0e31a_row3_col5\" class=\"data row3 col5\" >0.1885</td>\n",
       "      <td id=\"T_0e31a_row3_col6\" class=\"data row3 col6\" >0.1908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e31a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_0e31a_row4_col0\" class=\"data row4 col0\" >0.3553</td>\n",
       "      <td id=\"T_0e31a_row4_col1\" class=\"data row4 col1\" >0.0000</td>\n",
       "      <td id=\"T_0e31a_row4_col2\" class=\"data row4 col2\" >0.3553</td>\n",
       "      <td id=\"T_0e31a_row4_col3\" class=\"data row4 col3\" >0.3302</td>\n",
       "      <td id=\"T_0e31a_row4_col4\" class=\"data row4 col4\" >0.3369</td>\n",
       "      <td id=\"T_0e31a_row4_col5\" class=\"data row4 col5\" >0.1811</td>\n",
       "      <td id=\"T_0e31a_row4_col6\" class=\"data row4 col6\" >0.1828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e31a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_0e31a_row5_col0\" class=\"data row5 col0\" >0.3530</td>\n",
       "      <td id=\"T_0e31a_row5_col1\" class=\"data row5 col1\" >0.0000</td>\n",
       "      <td id=\"T_0e31a_row5_col2\" class=\"data row5 col2\" >0.3530</td>\n",
       "      <td id=\"T_0e31a_row5_col3\" class=\"data row5 col3\" >0.3309</td>\n",
       "      <td id=\"T_0e31a_row5_col4\" class=\"data row5 col4\" >0.3371</td>\n",
       "      <td id=\"T_0e31a_row5_col5\" class=\"data row5 col5\" >0.1796</td>\n",
       "      <td id=\"T_0e31a_row5_col6\" class=\"data row5 col6\" >0.1809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e31a_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_0e31a_row6_col0\" class=\"data row6 col0\" >0.3626</td>\n",
       "      <td id=\"T_0e31a_row6_col1\" class=\"data row6 col1\" >0.0000</td>\n",
       "      <td id=\"T_0e31a_row6_col2\" class=\"data row6 col2\" >0.3626</td>\n",
       "      <td id=\"T_0e31a_row6_col3\" class=\"data row6 col3\" >0.3412</td>\n",
       "      <td id=\"T_0e31a_row6_col4\" class=\"data row6 col4\" >0.3448</td>\n",
       "      <td id=\"T_0e31a_row6_col5\" class=\"data row6 col5\" >0.1905</td>\n",
       "      <td id=\"T_0e31a_row6_col6\" class=\"data row6 col6\" >0.1926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e31a_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_0e31a_row7_col0\" class=\"data row7 col0\" >0.3573</td>\n",
       "      <td id=\"T_0e31a_row7_col1\" class=\"data row7 col1\" >0.0000</td>\n",
       "      <td id=\"T_0e31a_row7_col2\" class=\"data row7 col2\" >0.3573</td>\n",
       "      <td id=\"T_0e31a_row7_col3\" class=\"data row7 col3\" >0.3355</td>\n",
       "      <td id=\"T_0e31a_row7_col4\" class=\"data row7 col4\" >0.3411</td>\n",
       "      <td id=\"T_0e31a_row7_col5\" class=\"data row7 col5\" >0.1846</td>\n",
       "      <td id=\"T_0e31a_row7_col6\" class=\"data row7 col6\" >0.1862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e31a_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_0e31a_row8_col0\" class=\"data row8 col0\" >0.3596</td>\n",
       "      <td id=\"T_0e31a_row8_col1\" class=\"data row8 col1\" >0.0000</td>\n",
       "      <td id=\"T_0e31a_row8_col2\" class=\"data row8 col2\" >0.3596</td>\n",
       "      <td id=\"T_0e31a_row8_col3\" class=\"data row8 col3\" >0.3322</td>\n",
       "      <td id=\"T_0e31a_row8_col4\" class=\"data row8 col4\" >0.3390</td>\n",
       "      <td id=\"T_0e31a_row8_col5\" class=\"data row8 col5\" >0.1871</td>\n",
       "      <td id=\"T_0e31a_row8_col6\" class=\"data row8 col6\" >0.1892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e31a_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_0e31a_row9_col0\" class=\"data row9 col0\" >0.3629</td>\n",
       "      <td id=\"T_0e31a_row9_col1\" class=\"data row9 col1\" >0.0000</td>\n",
       "      <td id=\"T_0e31a_row9_col2\" class=\"data row9 col2\" >0.3629</td>\n",
       "      <td id=\"T_0e31a_row9_col3\" class=\"data row9 col3\" >0.3405</td>\n",
       "      <td id=\"T_0e31a_row9_col4\" class=\"data row9 col4\" >0.3456</td>\n",
       "      <td id=\"T_0e31a_row9_col5\" class=\"data row9 col5\" >0.1913</td>\n",
       "      <td id=\"T_0e31a_row9_col6\" class=\"data row9 col6\" >0.1931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e31a_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_0e31a_row10_col0\" class=\"data row10 col0\" >0.3612</td>\n",
       "      <td id=\"T_0e31a_row10_col1\" class=\"data row10 col1\" >0.0000</td>\n",
       "      <td id=\"T_0e31a_row10_col2\" class=\"data row10 col2\" >0.3612</td>\n",
       "      <td id=\"T_0e31a_row10_col3\" class=\"data row10 col3\" >0.3362</td>\n",
       "      <td id=\"T_0e31a_row10_col4\" class=\"data row10 col4\" >0.3421</td>\n",
       "      <td id=\"T_0e31a_row10_col5\" class=\"data row10 col5\" >0.1888</td>\n",
       "      <td id=\"T_0e31a_row10_col6\" class=\"data row10 col6\" >0.1907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0e31a_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_0e31a_row11_col0\" class=\"data row11 col0\" >0.0047</td>\n",
       "      <td id=\"T_0e31a_row11_col1\" class=\"data row11 col1\" >0.0000</td>\n",
       "      <td id=\"T_0e31a_row11_col2\" class=\"data row11 col2\" >0.0047</td>\n",
       "      <td id=\"T_0e31a_row11_col3\" class=\"data row11 col3\" >0.0043</td>\n",
       "      <td id=\"T_0e31a_row11_col4\" class=\"data row11 col4\" >0.0037</td>\n",
       "      <td id=\"T_0e31a_row11_col5\" class=\"data row11 col5\" >0.0056</td>\n",
       "      <td id=\"T_0e31a_row11_col6\" class=\"data row11 col6\" >0.0059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2be851f6070>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    }
   ],
   "source": [
    "tuned_best_subgrade_pycaret_model = tune_model(best_subgrade_pycaret_model,\n",
    "                                               optimize='F1', tuner_verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5495a782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fc0ad660454f73a14a5f4b29f0bd91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButtons(description='Plot Type:', icons=('',), options=(('Pipeline Plot', 'pipelin"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_model(tuned_best_subgrade_pycaret_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "666f64c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e71a9_row10_col0, #T_e71a9_row10_col1, #T_e71a9_row10_col2, #T_e71a9_row10_col3, #T_e71a9_row10_col4, #T_e71a9_row10_col5, #T_e71a9_row10_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e71a9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e71a9_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_e71a9_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_e71a9_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_e71a9_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_e71a9_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_e71a9_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_e71a9_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e71a9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e71a9_row0_col0\" class=\"data row0 col0\" >0.1373</td>\n",
       "      <td id=\"T_e71a9_row0_col1\" class=\"data row0 col1\" >0.0000</td>\n",
       "      <td id=\"T_e71a9_row0_col2\" class=\"data row0 col2\" >0.1373</td>\n",
       "      <td id=\"T_e71a9_row0_col3\" class=\"data row0 col3\" >0.1271</td>\n",
       "      <td id=\"T_e71a9_row0_col4\" class=\"data row0 col4\" >0.1305</td>\n",
       "      <td id=\"T_e71a9_row0_col5\" class=\"data row0 col5\" >0.0925</td>\n",
       "      <td id=\"T_e71a9_row0_col6\" class=\"data row0 col6\" >0.0926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e71a9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e71a9_row1_col0\" class=\"data row1 col0\" >0.1429</td>\n",
       "      <td id=\"T_e71a9_row1_col1\" class=\"data row1 col1\" >0.0000</td>\n",
       "      <td id=\"T_e71a9_row1_col2\" class=\"data row1 col2\" >0.1429</td>\n",
       "      <td id=\"T_e71a9_row1_col3\" class=\"data row1 col3\" >0.1335</td>\n",
       "      <td id=\"T_e71a9_row1_col4\" class=\"data row1 col4\" >0.1366</td>\n",
       "      <td id=\"T_e71a9_row1_col5\" class=\"data row1 col5\" >0.0988</td>\n",
       "      <td id=\"T_e71a9_row1_col6\" class=\"data row1 col6\" >0.0989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e71a9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e71a9_row2_col0\" class=\"data row2 col0\" >0.1386</td>\n",
       "      <td id=\"T_e71a9_row2_col1\" class=\"data row2 col1\" >0.0000</td>\n",
       "      <td id=\"T_e71a9_row2_col2\" class=\"data row2 col2\" >0.1386</td>\n",
       "      <td id=\"T_e71a9_row2_col3\" class=\"data row2 col3\" >0.1293</td>\n",
       "      <td id=\"T_e71a9_row2_col4\" class=\"data row2 col4\" >0.1324</td>\n",
       "      <td id=\"T_e71a9_row2_col5\" class=\"data row2 col5\" >0.0941</td>\n",
       "      <td id=\"T_e71a9_row2_col6\" class=\"data row2 col6\" >0.0942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e71a9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e71a9_row3_col0\" class=\"data row3 col0\" >0.1404</td>\n",
       "      <td id=\"T_e71a9_row3_col1\" class=\"data row3 col1\" >0.0000</td>\n",
       "      <td id=\"T_e71a9_row3_col2\" class=\"data row3 col2\" >0.1404</td>\n",
       "      <td id=\"T_e71a9_row3_col3\" class=\"data row3 col3\" >0.1315</td>\n",
       "      <td id=\"T_e71a9_row3_col4\" class=\"data row3 col4\" >0.1343</td>\n",
       "      <td id=\"T_e71a9_row3_col5\" class=\"data row3 col5\" >0.0960</td>\n",
       "      <td id=\"T_e71a9_row3_col6\" class=\"data row3 col6\" >0.0961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e71a9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e71a9_row4_col0\" class=\"data row4 col0\" >0.1379</td>\n",
       "      <td id=\"T_e71a9_row4_col1\" class=\"data row4 col1\" >0.0000</td>\n",
       "      <td id=\"T_e71a9_row4_col2\" class=\"data row4 col2\" >0.1379</td>\n",
       "      <td id=\"T_e71a9_row4_col3\" class=\"data row4 col3\" >0.1281</td>\n",
       "      <td id=\"T_e71a9_row4_col4\" class=\"data row4 col4\" >0.1315</td>\n",
       "      <td id=\"T_e71a9_row4_col5\" class=\"data row4 col5\" >0.0935</td>\n",
       "      <td id=\"T_e71a9_row4_col6\" class=\"data row4 col6\" >0.0935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e71a9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_e71a9_row5_col0\" class=\"data row5 col0\" >0.1398</td>\n",
       "      <td id=\"T_e71a9_row5_col1\" class=\"data row5 col1\" >0.0000</td>\n",
       "      <td id=\"T_e71a9_row5_col2\" class=\"data row5 col2\" >0.1398</td>\n",
       "      <td id=\"T_e71a9_row5_col3\" class=\"data row5 col3\" >0.1307</td>\n",
       "      <td id=\"T_e71a9_row5_col4\" class=\"data row5 col4\" >0.1336</td>\n",
       "      <td id=\"T_e71a9_row5_col5\" class=\"data row5 col5\" >0.0953</td>\n",
       "      <td id=\"T_e71a9_row5_col6\" class=\"data row5 col6\" >0.0954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e71a9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_e71a9_row6_col0\" class=\"data row6 col0\" >0.1409</td>\n",
       "      <td id=\"T_e71a9_row6_col1\" class=\"data row6 col1\" >0.0000</td>\n",
       "      <td id=\"T_e71a9_row6_col2\" class=\"data row6 col2\" >0.1409</td>\n",
       "      <td id=\"T_e71a9_row6_col3\" class=\"data row6 col3\" >0.1308</td>\n",
       "      <td id=\"T_e71a9_row6_col4\" class=\"data row6 col4\" >0.1343</td>\n",
       "      <td id=\"T_e71a9_row6_col5\" class=\"data row6 col5\" >0.0965</td>\n",
       "      <td id=\"T_e71a9_row6_col6\" class=\"data row6 col6\" >0.0966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e71a9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_e71a9_row7_col0\" class=\"data row7 col0\" >0.1381</td>\n",
       "      <td id=\"T_e71a9_row7_col1\" class=\"data row7 col1\" >0.0000</td>\n",
       "      <td id=\"T_e71a9_row7_col2\" class=\"data row7 col2\" >0.1381</td>\n",
       "      <td id=\"T_e71a9_row7_col3\" class=\"data row7 col3\" >0.1296</td>\n",
       "      <td id=\"T_e71a9_row7_col4\" class=\"data row7 col4\" >0.1324</td>\n",
       "      <td id=\"T_e71a9_row7_col5\" class=\"data row7 col5\" >0.0937</td>\n",
       "      <td id=\"T_e71a9_row7_col6\" class=\"data row7 col6\" >0.0937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e71a9_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_e71a9_row8_col0\" class=\"data row8 col0\" >0.1417</td>\n",
       "      <td id=\"T_e71a9_row8_col1\" class=\"data row8 col1\" >0.0000</td>\n",
       "      <td id=\"T_e71a9_row8_col2\" class=\"data row8 col2\" >0.1417</td>\n",
       "      <td id=\"T_e71a9_row8_col3\" class=\"data row8 col3\" >0.1317</td>\n",
       "      <td id=\"T_e71a9_row8_col4\" class=\"data row8 col4\" >0.1350</td>\n",
       "      <td id=\"T_e71a9_row8_col5\" class=\"data row8 col5\" >0.0973</td>\n",
       "      <td id=\"T_e71a9_row8_col6\" class=\"data row8 col6\" >0.0974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e71a9_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_e71a9_row9_col0\" class=\"data row9 col0\" >0.1384</td>\n",
       "      <td id=\"T_e71a9_row9_col1\" class=\"data row9 col1\" >0.0000</td>\n",
       "      <td id=\"T_e71a9_row9_col2\" class=\"data row9 col2\" >0.1384</td>\n",
       "      <td id=\"T_e71a9_row9_col3\" class=\"data row9 col3\" >0.1290</td>\n",
       "      <td id=\"T_e71a9_row9_col4\" class=\"data row9 col4\" >0.1320</td>\n",
       "      <td id=\"T_e71a9_row9_col5\" class=\"data row9 col5\" >0.0939</td>\n",
       "      <td id=\"T_e71a9_row9_col6\" class=\"data row9 col6\" >0.0940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e71a9_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_e71a9_row10_col0\" class=\"data row10 col0\" >0.1396</td>\n",
       "      <td id=\"T_e71a9_row10_col1\" class=\"data row10 col1\" >0.0000</td>\n",
       "      <td id=\"T_e71a9_row10_col2\" class=\"data row10 col2\" >0.1396</td>\n",
       "      <td id=\"T_e71a9_row10_col3\" class=\"data row10 col3\" >0.1301</td>\n",
       "      <td id=\"T_e71a9_row10_col4\" class=\"data row10 col4\" >0.1333</td>\n",
       "      <td id=\"T_e71a9_row10_col5\" class=\"data row10 col5\" >0.0952</td>\n",
       "      <td id=\"T_e71a9_row10_col6\" class=\"data row10 col6\" >0.0952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e71a9_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_e71a9_row11_col0\" class=\"data row11 col0\" >0.0018</td>\n",
       "      <td id=\"T_e71a9_row11_col1\" class=\"data row11 col1\" >0.0000</td>\n",
       "      <td id=\"T_e71a9_row11_col2\" class=\"data row11 col2\" >0.0018</td>\n",
       "      <td id=\"T_e71a9_row11_col3\" class=\"data row11 col3\" >0.0018</td>\n",
       "      <td id=\"T_e71a9_row11_col4\" class=\"data row11 col4\" >0.0017</td>\n",
       "      <td id=\"T_e71a9_row11_col5\" class=\"data row11 col5\" >0.0019</td>\n",
       "      <td id=\"T_e71a9_row11_col6\" class=\"data row11 col6\" >0.0019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17e979fa220>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.9, device=&#x27;cpu&#x27;, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.15, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
       "              min_child_weight=3, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=290, n_jobs=-1,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.9, device=&#x27;cpu&#x27;, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.15, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
       "              min_child_weight=3, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=290, n_jobs=-1,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.9, device='cpu', early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.15, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=7, max_leaves=None,\n",
       "              min_child_weight=3, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=290, n_jobs=-1,\n",
       "              num_parallel_tree=None, objective='multi:softprob', ...)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_best_subgrade_pycaret_model = tune_model(best_subgrade_pycaret_model,\n",
    "                                               optimize='F1', tuner_verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43a4f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_F1_model = automl(optimize='F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e485aaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce MX450, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce MX450, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce MX450, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce MX450, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce MX450, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce MX450, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_66df7_row11_col1, #T_66df7_row17_col1, #T_66df7_row22_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_66df7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_66df7_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_66df7_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_66df7_row0_col0\" class=\"data row0 col0\" >Session id</td>\n",
       "      <td id=\"T_66df7_row0_col1\" class=\"data row0 col1\" >123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_66df7_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_66df7_row1_col1\" class=\"data row1 col1\" >Sub_Grade_Number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_66df7_row2_col0\" class=\"data row2 col0\" >Target type</td>\n",
       "      <td id=\"T_66df7_row2_col1\" class=\"data row2 col1\" >Multiclass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_66df7_row3_col0\" class=\"data row3 col0\" >Target mapping</td>\n",
       "      <td id=\"T_66df7_row3_col1\" class=\"data row3 col1\" >1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_66df7_row4_col0\" class=\"data row4 col0\" >Original data shape</td>\n",
       "      <td id=\"T_66df7_row4_col1\" class=\"data row4 col1\" >(303118, 104)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_66df7_row5_col0\" class=\"data row5 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_66df7_row5_col1\" class=\"data row5 col1\" >(354596, 124)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_66df7_row6_col0\" class=\"data row6 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_66df7_row6_col1\" class=\"data row6 col1\" >(263660, 124)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_66df7_row7_col0\" class=\"data row7 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_66df7_row7_col1\" class=\"data row7 col1\" >(90936, 124)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_66df7_row8_col0\" class=\"data row8 col0\" >Numeric features</td>\n",
       "      <td id=\"T_66df7_row8_col1\" class=\"data row8 col1\" >98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_66df7_row9_col0\" class=\"data row9 col0\" >Categorical features</td>\n",
       "      <td id=\"T_66df7_row9_col1\" class=\"data row9 col1\" >5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_66df7_row10_col0\" class=\"data row10 col0\" >Rows with missing values</td>\n",
       "      <td id=\"T_66df7_row10_col1\" class=\"data row10 col1\" >26.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_66df7_row11_col0\" class=\"data row11 col0\" >Preprocess</td>\n",
       "      <td id=\"T_66df7_row11_col1\" class=\"data row11 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_66df7_row12_col0\" class=\"data row12 col0\" >Imputation type</td>\n",
       "      <td id=\"T_66df7_row12_col1\" class=\"data row12 col1\" >simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_66df7_row13_col0\" class=\"data row13 col0\" >Numeric imputation</td>\n",
       "      <td id=\"T_66df7_row13_col1\" class=\"data row13 col1\" >mean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_66df7_row14_col0\" class=\"data row14 col0\" >Categorical imputation</td>\n",
       "      <td id=\"T_66df7_row14_col1\" class=\"data row14 col1\" >mode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_66df7_row15_col0\" class=\"data row15 col0\" >Maximum one-hot encoding</td>\n",
       "      <td id=\"T_66df7_row15_col1\" class=\"data row15 col1\" >25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_66df7_row16_col0\" class=\"data row16 col0\" >Encoding method</td>\n",
       "      <td id=\"T_66df7_row16_col1\" class=\"data row16 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_66df7_row17_col0\" class=\"data row17 col0\" >Fix imbalance</td>\n",
       "      <td id=\"T_66df7_row17_col1\" class=\"data row17 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_66df7_row18_col0\" class=\"data row18 col0\" >Fix imbalance method</td>\n",
       "      <td id=\"T_66df7_row18_col1\" class=\"data row18 col1\" >SMOTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_66df7_row19_col0\" class=\"data row19 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_66df7_row19_col1\" class=\"data row19 col1\" >StratifiedKFold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_66df7_row20_col0\" class=\"data row20 col0\" >Fold Number</td>\n",
       "      <td id=\"T_66df7_row20_col1\" class=\"data row20 col1\" >10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_66df7_row21_col0\" class=\"data row21 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_66df7_row21_col1\" class=\"data row21 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_66df7_row22_col0\" class=\"data row22 col0\" >Use GPU</td>\n",
       "      <td id=\"T_66df7_row22_col1\" class=\"data row22 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_66df7_row23_col0\" class=\"data row23 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_66df7_row23_col1\" class=\"data row23 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_66df7_row24_col0\" class=\"data row24 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_66df7_row24_col1\" class=\"data row24 col1\" >build_model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_66df7_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_66df7_row25_col0\" class=\"data row25 col0\" >USI</td>\n",
       "      <td id=\"T_66df7_row25_col1\" class=\"data row25 col1\" >2422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2bf1393f460>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce MX450, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 1, number of negative: 1\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 2, number of used features: 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA GeForce MX450, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 16 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Warning] GPU acceleration is disabled because no non-trivial dense features can be found\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    }
   ],
   "source": [
    "s = setup(pd.concat([X_train, y_train], axis=1), target='Sub_Grade_Number',\n",
    "          session_id=123, fix_imbalance=True, experiment_name='build_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e28094f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_67718_row10_col0, #T_67718_row10_col1, #T_67718_row10_col2, #T_67718_row10_col3, #T_67718_row10_col4, #T_67718_row10_col5, #T_67718_row10_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_67718\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_67718_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_67718_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_67718_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_67718_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_67718_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_67718_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_67718_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_67718_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_67718_row0_col0\" class=\"data row0 col0\" >0.3913</td>\n",
       "      <td id=\"T_67718_row0_col1\" class=\"data row0 col1\" >0.0000</td>\n",
       "      <td id=\"T_67718_row0_col2\" class=\"data row0 col2\" >0.3913</td>\n",
       "      <td id=\"T_67718_row0_col3\" class=\"data row0 col3\" >0.3564</td>\n",
       "      <td id=\"T_67718_row0_col4\" class=\"data row0 col4\" >0.3520</td>\n",
       "      <td id=\"T_67718_row0_col5\" class=\"data row0 col5\" >0.2228</td>\n",
       "      <td id=\"T_67718_row0_col6\" class=\"data row0 col6\" >0.2303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67718_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_67718_row1_col0\" class=\"data row1 col0\" >0.3874</td>\n",
       "      <td id=\"T_67718_row1_col1\" class=\"data row1 col1\" >0.0000</td>\n",
       "      <td id=\"T_67718_row1_col2\" class=\"data row1 col2\" >0.3874</td>\n",
       "      <td id=\"T_67718_row1_col3\" class=\"data row1 col3\" >0.3465</td>\n",
       "      <td id=\"T_67718_row1_col4\" class=\"data row1 col4\" >0.3457</td>\n",
       "      <td id=\"T_67718_row1_col5\" class=\"data row1 col5\" >0.2177</td>\n",
       "      <td id=\"T_67718_row1_col6\" class=\"data row1 col6\" >0.2251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67718_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_67718_row2_col0\" class=\"data row2 col0\" >0.3905</td>\n",
       "      <td id=\"T_67718_row2_col1\" class=\"data row2 col1\" >0.0000</td>\n",
       "      <td id=\"T_67718_row2_col2\" class=\"data row2 col2\" >0.3905</td>\n",
       "      <td id=\"T_67718_row2_col3\" class=\"data row2 col3\" >0.3499</td>\n",
       "      <td id=\"T_67718_row2_col4\" class=\"data row2 col4\" >0.3485</td>\n",
       "      <td id=\"T_67718_row2_col5\" class=\"data row2 col5\" >0.2218</td>\n",
       "      <td id=\"T_67718_row2_col6\" class=\"data row2 col6\" >0.2294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67718_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_67718_row3_col0\" class=\"data row3 col0\" >0.3885</td>\n",
       "      <td id=\"T_67718_row3_col1\" class=\"data row3 col1\" >0.0000</td>\n",
       "      <td id=\"T_67718_row3_col2\" class=\"data row3 col2\" >0.3885</td>\n",
       "      <td id=\"T_67718_row3_col3\" class=\"data row3 col3\" >0.3486</td>\n",
       "      <td id=\"T_67718_row3_col4\" class=\"data row3 col4\" >0.3463</td>\n",
       "      <td id=\"T_67718_row3_col5\" class=\"data row3 col5\" >0.2189</td>\n",
       "      <td id=\"T_67718_row3_col6\" class=\"data row3 col6\" >0.2266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67718_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_67718_row4_col0\" class=\"data row4 col0\" >0.3850</td>\n",
       "      <td id=\"T_67718_row4_col1\" class=\"data row4 col1\" >0.0000</td>\n",
       "      <td id=\"T_67718_row4_col2\" class=\"data row4 col2\" >0.3850</td>\n",
       "      <td id=\"T_67718_row4_col3\" class=\"data row4 col3\" >0.3454</td>\n",
       "      <td id=\"T_67718_row4_col4\" class=\"data row4 col4\" >0.3433</td>\n",
       "      <td id=\"T_67718_row4_col5\" class=\"data row4 col5\" >0.2143</td>\n",
       "      <td id=\"T_67718_row4_col6\" class=\"data row4 col6\" >0.2218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67718_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_67718_row5_col0\" class=\"data row5 col0\" >0.3900</td>\n",
       "      <td id=\"T_67718_row5_col1\" class=\"data row5 col1\" >0.0000</td>\n",
       "      <td id=\"T_67718_row5_col2\" class=\"data row5 col2\" >0.3900</td>\n",
       "      <td id=\"T_67718_row5_col3\" class=\"data row5 col3\" >0.3496</td>\n",
       "      <td id=\"T_67718_row5_col4\" class=\"data row5 col4\" >0.3471</td>\n",
       "      <td id=\"T_67718_row5_col5\" class=\"data row5 col5\" >0.2205</td>\n",
       "      <td id=\"T_67718_row5_col6\" class=\"data row5 col6\" >0.2284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67718_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_67718_row6_col0\" class=\"data row6 col0\" >0.3842</td>\n",
       "      <td id=\"T_67718_row6_col1\" class=\"data row6 col1\" >0.0000</td>\n",
       "      <td id=\"T_67718_row6_col2\" class=\"data row6 col2\" >0.3842</td>\n",
       "      <td id=\"T_67718_row6_col3\" class=\"data row6 col3\" >0.3488</td>\n",
       "      <td id=\"T_67718_row6_col4\" class=\"data row6 col4\" >0.3447</td>\n",
       "      <td id=\"T_67718_row6_col5\" class=\"data row6 col5\" >0.2134</td>\n",
       "      <td id=\"T_67718_row6_col6\" class=\"data row6 col6\" >0.2206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67718_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_67718_row7_col0\" class=\"data row7 col0\" >0.3901</td>\n",
       "      <td id=\"T_67718_row7_col1\" class=\"data row7 col1\" >0.0000</td>\n",
       "      <td id=\"T_67718_row7_col2\" class=\"data row7 col2\" >0.3901</td>\n",
       "      <td id=\"T_67718_row7_col3\" class=\"data row7 col3\" >0.3509</td>\n",
       "      <td id=\"T_67718_row7_col4\" class=\"data row7 col4\" >0.3481</td>\n",
       "      <td id=\"T_67718_row7_col5\" class=\"data row7 col5\" >0.2209</td>\n",
       "      <td id=\"T_67718_row7_col6\" class=\"data row7 col6\" >0.2287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67718_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_67718_row8_col0\" class=\"data row8 col0\" >0.3859</td>\n",
       "      <td id=\"T_67718_row8_col1\" class=\"data row8 col1\" >0.0000</td>\n",
       "      <td id=\"T_67718_row8_col2\" class=\"data row8 col2\" >0.3859</td>\n",
       "      <td id=\"T_67718_row8_col3\" class=\"data row8 col3\" >0.3449</td>\n",
       "      <td id=\"T_67718_row8_col4\" class=\"data row8 col4\" >0.3431</td>\n",
       "      <td id=\"T_67718_row8_col5\" class=\"data row8 col5\" >0.2156</td>\n",
       "      <td id=\"T_67718_row8_col6\" class=\"data row8 col6\" >0.2233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67718_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_67718_row9_col0\" class=\"data row9 col0\" >0.3878</td>\n",
       "      <td id=\"T_67718_row9_col1\" class=\"data row9 col1\" >0.0000</td>\n",
       "      <td id=\"T_67718_row9_col2\" class=\"data row9 col2\" >0.3878</td>\n",
       "      <td id=\"T_67718_row9_col3\" class=\"data row9 col3\" >0.3478</td>\n",
       "      <td id=\"T_67718_row9_col4\" class=\"data row9 col4\" >0.3462</td>\n",
       "      <td id=\"T_67718_row9_col5\" class=\"data row9 col5\" >0.2181</td>\n",
       "      <td id=\"T_67718_row9_col6\" class=\"data row9 col6\" >0.2254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67718_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_67718_row10_col0\" class=\"data row10 col0\" >0.3881</td>\n",
       "      <td id=\"T_67718_row10_col1\" class=\"data row10 col1\" >0.0000</td>\n",
       "      <td id=\"T_67718_row10_col2\" class=\"data row10 col2\" >0.3881</td>\n",
       "      <td id=\"T_67718_row10_col3\" class=\"data row10 col3\" >0.3489</td>\n",
       "      <td id=\"T_67718_row10_col4\" class=\"data row10 col4\" >0.3465</td>\n",
       "      <td id=\"T_67718_row10_col5\" class=\"data row10 col5\" >0.2184</td>\n",
       "      <td id=\"T_67718_row10_col6\" class=\"data row10 col6\" >0.2260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_67718_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_67718_row11_col0\" class=\"data row11 col0\" >0.0023</td>\n",
       "      <td id=\"T_67718_row11_col1\" class=\"data row11 col1\" >0.0000</td>\n",
       "      <td id=\"T_67718_row11_col2\" class=\"data row11 col2\" >0.0023</td>\n",
       "      <td id=\"T_67718_row11_col3\" class=\"data row11 col3\" >0.0031</td>\n",
       "      <td id=\"T_67718_row11_col4\" class=\"data row11 col4\" >0.0025</td>\n",
       "      <td id=\"T_67718_row11_col5\" class=\"data row11 col5\" >0.0030</td>\n",
       "      <td id=\"T_67718_row11_col6\" class=\"data row11 col6\" >0.0031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2bea5859e80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_subgrade_model = create_model('lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e853c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f8890_row10_col0, #T_f8890_row10_col1, #T_f8890_row10_col2, #T_f8890_row10_col3, #T_f8890_row10_col4, #T_f8890_row10_col5, #T_f8890_row10_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f8890\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f8890_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_f8890_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_f8890_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_f8890_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_f8890_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_f8890_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_f8890_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f8890_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f8890_row0_col0\" class=\"data row0 col0\" >0.3863</td>\n",
       "      <td id=\"T_f8890_row0_col1\" class=\"data row0 col1\" >0.0000</td>\n",
       "      <td id=\"T_f8890_row0_col2\" class=\"data row0 col2\" >0.3863</td>\n",
       "      <td id=\"T_f8890_row0_col3\" class=\"data row0 col3\" >0.3542</td>\n",
       "      <td id=\"T_f8890_row0_col4\" class=\"data row0 col4\" >0.3573</td>\n",
       "      <td id=\"T_f8890_row0_col5\" class=\"data row0 col5\" >0.2185</td>\n",
       "      <td id=\"T_f8890_row0_col6\" class=\"data row0 col6\" >0.2229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f8890_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f8890_row1_col0\" class=\"data row1 col0\" >0.3888</td>\n",
       "      <td id=\"T_f8890_row1_col1\" class=\"data row1 col1\" >0.0000</td>\n",
       "      <td id=\"T_f8890_row1_col2\" class=\"data row1 col2\" >0.3888</td>\n",
       "      <td id=\"T_f8890_row1_col3\" class=\"data row1 col3\" >0.3577</td>\n",
       "      <td id=\"T_f8890_row1_col4\" class=\"data row1 col4\" >0.3607</td>\n",
       "      <td id=\"T_f8890_row1_col5\" class=\"data row1 col5\" >0.2219</td>\n",
       "      <td id=\"T_f8890_row1_col6\" class=\"data row1 col6\" >0.2261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f8890_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f8890_row2_col0\" class=\"data row2 col0\" >0.3861</td>\n",
       "      <td id=\"T_f8890_row2_col1\" class=\"data row2 col1\" >0.0000</td>\n",
       "      <td id=\"T_f8890_row2_col2\" class=\"data row2 col2\" >0.3861</td>\n",
       "      <td id=\"T_f8890_row2_col3\" class=\"data row2 col3\" >0.3538</td>\n",
       "      <td id=\"T_f8890_row2_col4\" class=\"data row2 col4\" >0.3572</td>\n",
       "      <td id=\"T_f8890_row2_col5\" class=\"data row2 col5\" >0.2185</td>\n",
       "      <td id=\"T_f8890_row2_col6\" class=\"data row2 col6\" >0.2227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f8890_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f8890_row3_col0\" class=\"data row3 col0\" >0.3866</td>\n",
       "      <td id=\"T_f8890_row3_col1\" class=\"data row3 col1\" >0.0000</td>\n",
       "      <td id=\"T_f8890_row3_col2\" class=\"data row3 col2\" >0.3866</td>\n",
       "      <td id=\"T_f8890_row3_col3\" class=\"data row3 col3\" >0.3544</td>\n",
       "      <td id=\"T_f8890_row3_col4\" class=\"data row3 col4\" >0.3576</td>\n",
       "      <td id=\"T_f8890_row3_col5\" class=\"data row3 col5\" >0.2189</td>\n",
       "      <td id=\"T_f8890_row3_col6\" class=\"data row3 col6\" >0.2232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f8890_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f8890_row4_col0\" class=\"data row4 col0\" >0.3831</td>\n",
       "      <td id=\"T_f8890_row4_col1\" class=\"data row4 col1\" >0.0000</td>\n",
       "      <td id=\"T_f8890_row4_col2\" class=\"data row4 col2\" >0.3831</td>\n",
       "      <td id=\"T_f8890_row4_col3\" class=\"data row4 col3\" >0.3494</td>\n",
       "      <td id=\"T_f8890_row4_col4\" class=\"data row4 col4\" >0.3536</td>\n",
       "      <td id=\"T_f8890_row4_col5\" class=\"data row4 col5\" >0.2144</td>\n",
       "      <td id=\"T_f8890_row4_col6\" class=\"data row4 col6\" >0.2187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f8890_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f8890_row5_col0\" class=\"data row5 col0\" >0.3816</td>\n",
       "      <td id=\"T_f8890_row5_col1\" class=\"data row5 col1\" >0.0000</td>\n",
       "      <td id=\"T_f8890_row5_col2\" class=\"data row5 col2\" >0.3816</td>\n",
       "      <td id=\"T_f8890_row5_col3\" class=\"data row5 col3\" >0.3457</td>\n",
       "      <td id=\"T_f8890_row5_col4\" class=\"data row5 col4\" >0.3504</td>\n",
       "      <td id=\"T_f8890_row5_col5\" class=\"data row5 col5\" >0.2121</td>\n",
       "      <td id=\"T_f8890_row5_col6\" class=\"data row5 col6\" >0.2165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f8890_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_f8890_row6_col0\" class=\"data row6 col0\" >0.3824</td>\n",
       "      <td id=\"T_f8890_row6_col1\" class=\"data row6 col1\" >0.0000</td>\n",
       "      <td id=\"T_f8890_row6_col2\" class=\"data row6 col2\" >0.3824</td>\n",
       "      <td id=\"T_f8890_row6_col3\" class=\"data row6 col3\" >0.3506</td>\n",
       "      <td id=\"T_f8890_row6_col4\" class=\"data row6 col4\" >0.3541</td>\n",
       "      <td id=\"T_f8890_row6_col5\" class=\"data row6 col5\" >0.2133</td>\n",
       "      <td id=\"T_f8890_row6_col6\" class=\"data row6 col6\" >0.2174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f8890_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_f8890_row7_col0\" class=\"data row7 col0\" >0.3856</td>\n",
       "      <td id=\"T_f8890_row7_col1\" class=\"data row7 col1\" >0.0000</td>\n",
       "      <td id=\"T_f8890_row7_col2\" class=\"data row7 col2\" >0.3856</td>\n",
       "      <td id=\"T_f8890_row7_col3\" class=\"data row7 col3\" >0.3530</td>\n",
       "      <td id=\"T_f8890_row7_col4\" class=\"data row7 col4\" >0.3563</td>\n",
       "      <td id=\"T_f8890_row7_col5\" class=\"data row7 col5\" >0.2175</td>\n",
       "      <td id=\"T_f8890_row7_col6\" class=\"data row7 col6\" >0.2219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f8890_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_f8890_row8_col0\" class=\"data row8 col0\" >0.3838</td>\n",
       "      <td id=\"T_f8890_row8_col1\" class=\"data row8 col1\" >0.0000</td>\n",
       "      <td id=\"T_f8890_row8_col2\" class=\"data row8 col2\" >0.3838</td>\n",
       "      <td id=\"T_f8890_row8_col3\" class=\"data row8 col3\" >0.3502</td>\n",
       "      <td id=\"T_f8890_row8_col4\" class=\"data row8 col4\" >0.3535</td>\n",
       "      <td id=\"T_f8890_row8_col5\" class=\"data row8 col5\" >0.2152</td>\n",
       "      <td id=\"T_f8890_row8_col6\" class=\"data row8 col6\" >0.2197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f8890_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_f8890_row9_col0\" class=\"data row9 col0\" >0.3860</td>\n",
       "      <td id=\"T_f8890_row9_col1\" class=\"data row9 col1\" >0.0000</td>\n",
       "      <td id=\"T_f8890_row9_col2\" class=\"data row9 col2\" >0.3860</td>\n",
       "      <td id=\"T_f8890_row9_col3\" class=\"data row9 col3\" >0.3556</td>\n",
       "      <td id=\"T_f8890_row9_col4\" class=\"data row9 col4\" >0.3580</td>\n",
       "      <td id=\"T_f8890_row9_col5\" class=\"data row9 col5\" >0.2182</td>\n",
       "      <td id=\"T_f8890_row9_col6\" class=\"data row9 col6\" >0.2224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f8890_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_f8890_row10_col0\" class=\"data row10 col0\" >0.3850</td>\n",
       "      <td id=\"T_f8890_row10_col1\" class=\"data row10 col1\" >0.0000</td>\n",
       "      <td id=\"T_f8890_row10_col2\" class=\"data row10 col2\" >0.3850</td>\n",
       "      <td id=\"T_f8890_row10_col3\" class=\"data row10 col3\" >0.3525</td>\n",
       "      <td id=\"T_f8890_row10_col4\" class=\"data row10 col4\" >0.3559</td>\n",
       "      <td id=\"T_f8890_row10_col5\" class=\"data row10 col5\" >0.2168</td>\n",
       "      <td id=\"T_f8890_row10_col6\" class=\"data row10 col6\" >0.2212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f8890_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_f8890_row11_col0\" class=\"data row11 col0\" >0.0021</td>\n",
       "      <td id=\"T_f8890_row11_col1\" class=\"data row11 col1\" >0.0000</td>\n",
       "      <td id=\"T_f8890_row11_col2\" class=\"data row11 col2\" >0.0021</td>\n",
       "      <td id=\"T_f8890_row11_col3\" class=\"data row11 col3\" >0.0033</td>\n",
       "      <td id=\"T_f8890_row11_col4\" class=\"data row11 col4\" >0.0028</td>\n",
       "      <td id=\"T_f8890_row11_col5\" class=\"data row11 col5\" >0.0028</td>\n",
       "      <td id=\"T_f8890_row11_col6\" class=\"data row11 col6\" >0.0028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2be801b6c40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29428\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[CV] END actual_estimator__bagging_fraction=0.5, actual_estimator__bagging_freq=0, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=91, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=130, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=4, actual_estimator__reg_lambda=1e-06; total time=  44.1s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29438\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[CV] END actual_estimator__bagging_fraction=0.5, actual_estimator__bagging_freq=0, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=91, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=130, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=4, actual_estimator__reg_lambda=1e-06; total time=  42.8s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067739 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29441\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[CV] END actual_estimator__bagging_fraction=0.5, actual_estimator__bagging_freq=0, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=91, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=130, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=4, actual_estimator__reg_lambda=1e-06; total time=  42.4s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063773 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29447\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[CV] END actual_estimator__bagging_fraction=0.5, actual_estimator__bagging_freq=0, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=91, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=130, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=4, actual_estimator__reg_lambda=1e-06; total time=  40.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066043 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29437\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[CV] END actual_estimator__bagging_fraction=0.5, actual_estimator__bagging_freq=0, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=91, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=130, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=4, actual_estimator__reg_lambda=1e-06; total time=  38.7s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067126 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29422\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[CV] END actual_estimator__bagging_fraction=0.5, actual_estimator__bagging_freq=0, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=91, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=130, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=4, actual_estimator__reg_lambda=1e-06; total time=  44.3s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071819 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29426\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[CV] END actual_estimator__bagging_fraction=0.5, actual_estimator__bagging_freq=0, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=91, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=130, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=4, actual_estimator__reg_lambda=1e-06; total time=  43.3s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032161 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29421\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[CV] END actual_estimator__bagging_fraction=0.5, actual_estimator__bagging_freq=0, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=91, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=130, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=4, actual_estimator__reg_lambda=1e-06; total time=  46.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067733 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29418\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[CV] END actual_estimator__bagging_fraction=0.5, actual_estimator__bagging_freq=0, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=91, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=130, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=4, actual_estimator__reg_lambda=1e-06; total time=  43.9s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066006 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29445\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.5, subsample=1.0 will be ignored. Current value: bagging_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_freq is set=0, subsample_freq=0 will be ignored. Current value: bagging_freq=0\n",
      "[CV] END actual_estimator__bagging_fraction=0.5, actual_estimator__bagging_freq=0, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=91, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=130, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=4, actual_estimator__reg_lambda=1e-06; total time=  40.7s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29428\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=4, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=0.05, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.7, actual_estimator__n_estimators=180, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=0.0001, actual_estimator__reg_lambda=0.1; total time=  26.6s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031515 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29429\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=4, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=0.05, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.7, actual_estimator__n_estimators=180, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=0.0001, actual_estimator__reg_lambda=0.1; total time=  27.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.062255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29430\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=4, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=0.05, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.7, actual_estimator__n_estimators=180, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=0.0001, actual_estimator__reg_lambda=0.1; total time=  25.5s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067129 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29437\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=4, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=0.05, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.7, actual_estimator__n_estimators=180, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=0.0001, actual_estimator__reg_lambda=0.1; total time=  24.8s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067612 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29437\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=4, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=0.05, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.7, actual_estimator__n_estimators=180, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=0.0001, actual_estimator__reg_lambda=0.1; total time=  25.1s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034396 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29422\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=4, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=0.05, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.7, actual_estimator__n_estimators=180, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=0.0001, actual_estimator__reg_lambda=0.1; total time=  26.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29426\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=4, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=0.05, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.7, actual_estimator__n_estimators=180, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=0.0001, actual_estimator__reg_lambda=0.1; total time=  29.8s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064749 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29421\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=4, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=0.05, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.7, actual_estimator__n_estimators=180, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=0.0001, actual_estimator__reg_lambda=0.1; total time=  28.3s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067902 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29418\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=4, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=0.05, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.7, actual_estimator__n_estimators=180, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=0.0001, actual_estimator__reg_lambda=0.1; total time=  25.9s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29436\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=4, actual_estimator__feature_fraction=0.9, actual_estimator__learning_rate=0.05, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.7, actual_estimator__n_estimators=180, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=0.0001, actual_estimator__reg_lambda=0.1; total time=  27.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.067730 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29448\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.8, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=30, actual_estimator__num_leaves=256, actual_estimator__reg_alpha=3, actual_estimator__reg_lambda=0.001; total time=  26.8s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29451\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.8, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=30, actual_estimator__num_leaves=256, actual_estimator__reg_alpha=3, actual_estimator__reg_lambda=0.001; total time=  26.0s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069939 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29453\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.8, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=30, actual_estimator__num_leaves=256, actual_estimator__reg_alpha=3, actual_estimator__reg_lambda=0.001; total time=  28.4s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.075911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29459\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.8, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=30, actual_estimator__num_leaves=256, actual_estimator__reg_alpha=3, actual_estimator__reg_lambda=0.001; total time=  26.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064621 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29460\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.8, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=30, actual_estimator__num_leaves=256, actual_estimator__reg_alpha=3, actual_estimator__reg_lambda=0.001; total time=  25.7s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074695 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29444\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.8, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=30, actual_estimator__num_leaves=256, actual_estimator__reg_alpha=3, actual_estimator__reg_lambda=0.001; total time=  27.4s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066833 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29445\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.8, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=30, actual_estimator__num_leaves=256, actual_estimator__reg_alpha=3, actual_estimator__reg_lambda=0.001; total time=  26.9s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29440\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.8, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=30, actual_estimator__num_leaves=256, actual_estimator__reg_alpha=3, actual_estimator__reg_lambda=0.001; total time=  25.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.065070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29437\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.8, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=30, actual_estimator__num_leaves=256, actual_estimator__reg_alpha=3, actual_estimator__reg_lambda=0.001; total time=  27.4s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29456\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.8, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=30, actual_estimator__num_leaves=256, actual_estimator__reg_alpha=3, actual_estimator__reg_lambda=0.001; total time=  26.7s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29444\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.4, actual_estimator__min_child_samples=6, actual_estimator__min_split_gain=0.3, actual_estimator__n_estimators=20, actual_estimator__num_leaves=150, actual_estimator__reg_alpha=0.005, actual_estimator__reg_lambda=0.0005; total time=  20.9s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29449\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 122\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.4, actual_estimator__min_child_samples=6, actual_estimator__min_split_gain=0.3, actual_estimator__n_estimators=20, actual_estimator__num_leaves=150, actual_estimator__reg_alpha=0.005, actual_estimator__reg_lambda=0.0005; total time=  20.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031547 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29449\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.4, actual_estimator__min_child_samples=6, actual_estimator__min_split_gain=0.3, actual_estimator__n_estimators=20, actual_estimator__num_leaves=150, actual_estimator__reg_alpha=0.005, actual_estimator__reg_lambda=0.0005; total time=  21.3s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29455\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.4, actual_estimator__min_child_samples=6, actual_estimator__min_split_gain=0.3, actual_estimator__n_estimators=20, actual_estimator__num_leaves=150, actual_estimator__reg_alpha=0.005, actual_estimator__reg_lambda=0.0005; total time=  20.9s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29456\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.4, actual_estimator__min_child_samples=6, actual_estimator__min_split_gain=0.3, actual_estimator__n_estimators=20, actual_estimator__num_leaves=150, actual_estimator__reg_alpha=0.005, actual_estimator__reg_lambda=0.0005; total time=  21.9s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29440\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.4, actual_estimator__min_child_samples=6, actual_estimator__min_split_gain=0.3, actual_estimator__n_estimators=20, actual_estimator__num_leaves=150, actual_estimator__reg_alpha=0.005, actual_estimator__reg_lambda=0.0005; total time=  19.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29441\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.4, actual_estimator__min_child_samples=6, actual_estimator__min_split_gain=0.3, actual_estimator__n_estimators=20, actual_estimator__num_leaves=150, actual_estimator__reg_alpha=0.005, actual_estimator__reg_lambda=0.0005; total time=  20.0s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.056244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29436\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.4, actual_estimator__min_child_samples=6, actual_estimator__min_split_gain=0.3, actual_estimator__n_estimators=20, actual_estimator__num_leaves=150, actual_estimator__reg_alpha=0.005, actual_estimator__reg_lambda=0.0005; total time=  22.1s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.048623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29433\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.4, actual_estimator__min_child_samples=6, actual_estimator__min_split_gain=0.3, actual_estimator__n_estimators=20, actual_estimator__num_leaves=150, actual_estimator__reg_alpha=0.005, actual_estimator__reg_lambda=0.0005; total time=  23.6s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29452\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.4, actual_estimator__min_child_samples=6, actual_estimator__min_split_gain=0.3, actual_estimator__n_estimators=20, actual_estimator__num_leaves=150, actual_estimator__reg_alpha=0.005, actual_estimator__reg_lambda=0.0005; total time=  29.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046769 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29440\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.4, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=41, actual_estimator__min_split_gain=0.9, actual_estimator__n_estimators=260, actual_estimator__num_leaves=70, actual_estimator__reg_alpha=2, actual_estimator__reg_lambda=3; total time=  59.6s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069207 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29443\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.4, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=41, actual_estimator__min_split_gain=0.9, actual_estimator__n_estimators=260, actual_estimator__num_leaves=70, actual_estimator__reg_alpha=2, actual_estimator__reg_lambda=3; total time=  45.7s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034463 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29445\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.4, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=41, actual_estimator__min_split_gain=0.9, actual_estimator__n_estimators=260, actual_estimator__num_leaves=70, actual_estimator__reg_alpha=2, actual_estimator__reg_lambda=3; total time=  52.5s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034651 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29452\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.4, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=41, actual_estimator__min_split_gain=0.9, actual_estimator__n_estimators=260, actual_estimator__num_leaves=70, actual_estimator__reg_alpha=2, actual_estimator__reg_lambda=3; total time=  52.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.029970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29453\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.4, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=41, actual_estimator__min_split_gain=0.9, actual_estimator__n_estimators=260, actual_estimator__num_leaves=70, actual_estimator__reg_alpha=2, actual_estimator__reg_lambda=3; total time=  53.4s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.032554 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29437\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.4, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=41, actual_estimator__min_split_gain=0.9, actual_estimator__n_estimators=260, actual_estimator__num_leaves=70, actual_estimator__reg_alpha=2, actual_estimator__reg_lambda=3; total time=  53.0s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29438\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.4, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=41, actual_estimator__min_split_gain=0.9, actual_estimator__n_estimators=260, actual_estimator__num_leaves=70, actual_estimator__reg_alpha=2, actual_estimator__reg_lambda=3; total time=  55.4s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29434\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.4, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=41, actual_estimator__min_split_gain=0.9, actual_estimator__n_estimators=260, actual_estimator__num_leaves=70, actual_estimator__reg_alpha=2, actual_estimator__reg_lambda=3; total time=  52.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29431\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.4, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=41, actual_estimator__min_split_gain=0.9, actual_estimator__n_estimators=260, actual_estimator__num_leaves=70, actual_estimator__reg_alpha=2, actual_estimator__reg_lambda=3; total time=  51.4s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29450\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.4, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.4\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=2, subsample_freq=0 will be ignored. Current value: bagging_freq=2\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=2, actual_estimator__feature_fraction=0.4, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=41, actual_estimator__min_split_gain=0.9, actual_estimator__n_estimators=260, actual_estimator__num_leaves=70, actual_estimator__reg_alpha=2, actual_estimator__reg_lambda=3; total time=  51.7s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036339 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29440\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=46, actual_estimator__min_split_gain=0, actual_estimator__n_estimators=30, actual_estimator__num_leaves=2, actual_estimator__reg_alpha=0.2, actual_estimator__reg_lambda=0.0005; total time=  19.4s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069525 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29443\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=46, actual_estimator__min_split_gain=0, actual_estimator__n_estimators=30, actual_estimator__num_leaves=2, actual_estimator__reg_alpha=0.2, actual_estimator__reg_lambda=0.0005; total time=  19.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29445\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=46, actual_estimator__min_split_gain=0, actual_estimator__n_estimators=30, actual_estimator__num_leaves=2, actual_estimator__reg_alpha=0.2, actual_estimator__reg_lambda=0.0005; total time=  19.3s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.042012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29452\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=46, actual_estimator__min_split_gain=0, actual_estimator__n_estimators=30, actual_estimator__num_leaves=2, actual_estimator__reg_alpha=0.2, actual_estimator__reg_lambda=0.0005; total time=  19.5s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073064 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29453\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=46, actual_estimator__min_split_gain=0, actual_estimator__n_estimators=30, actual_estimator__num_leaves=2, actual_estimator__reg_alpha=0.2, actual_estimator__reg_lambda=0.0005; total time=  20.0s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070198 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29437\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=46, actual_estimator__min_split_gain=0, actual_estimator__n_estimators=30, actual_estimator__num_leaves=2, actual_estimator__reg_alpha=0.2, actual_estimator__reg_lambda=0.0005; total time=  19.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29438\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=46, actual_estimator__min_split_gain=0, actual_estimator__n_estimators=30, actual_estimator__num_leaves=2, actual_estimator__reg_alpha=0.2, actual_estimator__reg_lambda=0.0005; total time=  20.0s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068194 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29434\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=46, actual_estimator__min_split_gain=0, actual_estimator__n_estimators=30, actual_estimator__num_leaves=2, actual_estimator__reg_alpha=0.2, actual_estimator__reg_lambda=0.0005; total time=  19.5s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043476 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29431\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=46, actual_estimator__min_split_gain=0, actual_estimator__n_estimators=30, actual_estimator__num_leaves=2, actual_estimator__reg_alpha=0.2, actual_estimator__reg_lambda=0.0005; total time=  19.7s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29450\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 120\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.9, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.0005, actual_estimator__min_child_samples=46, actual_estimator__min_split_gain=0, actual_estimator__n_estimators=30, actual_estimator__num_leaves=2, actual_estimator__reg_alpha=0.2, actual_estimator__reg_lambda=0.0005; total time=  19.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.037035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29448\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=1, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=90, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=10, actual_estimator__reg_lambda=2; total time=  25.4s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.076657 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29451\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=1, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=90, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=10, actual_estimator__reg_lambda=2; total time=  24.8s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.038562 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29453\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=1, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=90, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=10, actual_estimator__reg_lambda=2; total time=  25.0s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.083546 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29459\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=1, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=90, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=10, actual_estimator__reg_lambda=2; total time=  24.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073499 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29460\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=1, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=90, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=10, actual_estimator__reg_lambda=2; total time=  24.9s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036525 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29444\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=1, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=90, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=10, actual_estimator__reg_lambda=2; total time=  25.0s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.074684 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29445\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=1, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=90, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=10, actual_estimator__reg_lambda=2; total time=  24.7s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.081793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29440\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=1, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=90, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=10, actual_estimator__reg_lambda=2; total time=  25.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29437\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=1, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=90, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=10, actual_estimator__reg_lambda=2; total time=  24.7s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29456\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 123\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.6, subsample=1.0 will be ignored. Current value: bagging_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_freq is set=1, subsample_freq=0 will be ignored. Current value: bagging_freq=1\n",
      "[CV] END actual_estimator__bagging_fraction=0.6, actual_estimator__bagging_freq=1, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=1e-07, actual_estimator__min_child_samples=1, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=90, actual_estimator__num_leaves=10, actual_estimator__reg_alpha=10, actual_estimator__reg_lambda=2; total time=  25.5s\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035725 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29428\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.8, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.005, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=210, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=0.001, actual_estimator__reg_lambda=1e-06; total time= 1.2min\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29429\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.8, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.005, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=210, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=0.001, actual_estimator__reg_lambda=1e-06; total time= 1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29430\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.8, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.005, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=210, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=0.001, actual_estimator__reg_lambda=1e-06; total time= 1.2min\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29437\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.8, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.005, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=210, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=0.001, actual_estimator__reg_lambda=1e-06; total time= 1.1min\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29437\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.8, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.005, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=210, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=0.001, actual_estimator__reg_lambda=1e-06; total time= 1.3min\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29422\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.8, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.005, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=210, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=0.001, actual_estimator__reg_lambda=1e-06; total time= 1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068991 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29426\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.8, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.005, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=210, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=0.001, actual_estimator__reg_lambda=1e-06; total time= 1.0min\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073250 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29421\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.8, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.005, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=210, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=0.001, actual_estimator__reg_lambda=1e-06; total time= 1.0min\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.068877 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29418\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.8, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.005, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=210, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=0.001, actual_estimator__reg_lambda=1e-06; total time= 1.1min\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29436\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 118\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.8, subsample=1.0 will be ignored. Current value: bagging_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[CV] END actual_estimator__bagging_fraction=0.8, actual_estimator__bagging_freq=3, actual_estimator__feature_fraction=1.0, actual_estimator__learning_rate=0.005, actual_estimator__min_child_samples=96, actual_estimator__min_split_gain=0.5, actual_estimator__n_estimators=210, actual_estimator__num_leaves=80, actual_estimator__reg_alpha=0.001, actual_estimator__reg_lambda=1e-06; total time= 1.1min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29436\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=6, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=66, actual_estimator__min_split_gain=0.4, actual_estimator__n_estimators=90, actual_estimator__num_leaves=90, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.1; total time=  34.0s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29438\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=6, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=66, actual_estimator__min_split_gain=0.4, actual_estimator__n_estimators=90, actual_estimator__num_leaves=90, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.1; total time=  39.9s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29441\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=6, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=66, actual_estimator__min_split_gain=0.4, actual_estimator__n_estimators=90, actual_estimator__num_leaves=90, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.1; total time=  34.8s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.036655 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29447\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=6, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=66, actual_estimator__min_split_gain=0.4, actual_estimator__n_estimators=90, actual_estimator__num_leaves=90, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.1; total time=  33.0s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29447\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=6, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=66, actual_estimator__min_split_gain=0.4, actual_estimator__n_estimators=90, actual_estimator__num_leaves=90, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.1; total time=  33.5s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.043840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29431\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=6, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=66, actual_estimator__min_split_gain=0.4, actual_estimator__n_estimators=90, actual_estimator__num_leaves=90, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.1; total time=  33.0s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29434\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=6, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=66, actual_estimator__min_split_gain=0.4, actual_estimator__n_estimators=90, actual_estimator__num_leaves=90, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.1; total time=  33.3s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058332 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29429\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=6, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=66, actual_estimator__min_split_gain=0.4, actual_estimator__n_estimators=90, actual_estimator__num_leaves=90, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.1; total time=  39.0s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29426\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=6, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=66, actual_estimator__min_split_gain=0.4, actual_estimator__n_estimators=90, actual_estimator__num_leaves=90, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.1; total time=  35.5s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.035973 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 29445\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 119\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.5, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.5\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.7, subsample=1.0 will be ignored. Current value: bagging_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_freq is set=6, subsample_freq=0 will be ignored. Current value: bagging_freq=6\n",
      "[CV] END actual_estimator__bagging_fraction=0.7, actual_estimator__bagging_freq=6, actual_estimator__feature_fraction=0.5, actual_estimator__learning_rate=0.1, actual_estimator__min_child_samples=66, actual_estimator__min_split_gain=0.4, actual_estimator__n_estimators=90, actual_estimator__num_leaves=90, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.1; total time=  40.3s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071163 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29444\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[CV] END actual_estimator__bagging_fraction=1.0, actual_estimator__bagging_freq=7, actual_estimator__feature_fraction=0.7, actual_estimator__learning_rate=0.01, actual_estimator__min_child_samples=11, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=80, actual_estimator__num_leaves=60, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.4; total time=  32.0s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082575 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29447\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[CV] END actual_estimator__bagging_fraction=1.0, actual_estimator__bagging_freq=7, actual_estimator__feature_fraction=0.7, actual_estimator__learning_rate=0.01, actual_estimator__min_child_samples=11, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=80, actual_estimator__num_leaves=60, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.4; total time=  34.3s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.070368 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29449\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[CV] END actual_estimator__bagging_fraction=1.0, actual_estimator__bagging_freq=7, actual_estimator__feature_fraction=0.7, actual_estimator__learning_rate=0.01, actual_estimator__min_child_samples=11, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=80, actual_estimator__num_leaves=60, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.4; total time=  36.1s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.072191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29455\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[CV] END actual_estimator__bagging_fraction=1.0, actual_estimator__bagging_freq=7, actual_estimator__feature_fraction=0.7, actual_estimator__learning_rate=0.01, actual_estimator__min_child_samples=11, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=80, actual_estimator__num_leaves=60, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.4; total time=  38.9s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082343 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29456\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[CV] END actual_estimator__bagging_fraction=1.0, actual_estimator__bagging_freq=7, actual_estimator__feature_fraction=0.7, actual_estimator__learning_rate=0.01, actual_estimator__min_child_samples=11, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=80, actual_estimator__num_leaves=60, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.4; total time=  35.1s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.086670 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29440\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[CV] END actual_estimator__bagging_fraction=1.0, actual_estimator__bagging_freq=7, actual_estimator__feature_fraction=0.7, actual_estimator__learning_rate=0.01, actual_estimator__min_child_samples=11, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=80, actual_estimator__num_leaves=60, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.4; total time=  38.1s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.063789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29441\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[CV] END actual_estimator__bagging_fraction=1.0, actual_estimator__bagging_freq=7, actual_estimator__feature_fraction=0.7, actual_estimator__learning_rate=0.01, actual_estimator__min_child_samples=11, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=80, actual_estimator__num_leaves=60, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.4; total time=  31.2s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.069453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29436\n",
      "[LightGBM] [Info] Number of data points in the train set: 237295, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[CV] END actual_estimator__bagging_fraction=1.0, actual_estimator__bagging_freq=7, actual_estimator__feature_fraction=0.7, actual_estimator__learning_rate=0.01, actual_estimator__min_child_samples=11, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=80, actual_estimator__num_leaves=60, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.4; total time=  31.3s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.079196 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29433\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[CV] END actual_estimator__bagging_fraction=1.0, actual_estimator__bagging_freq=7, actual_estimator__feature_fraction=0.7, actual_estimator__learning_rate=0.01, actual_estimator__min_child_samples=11, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=80, actual_estimator__num_leaves=60, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.4; total time=  30.9s\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078480 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 29452\n",
      "[LightGBM] [Info] Number of data points in the train set: 237290, number of used features: 121\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n",
      "[LightGBM] [Info] Start training from score -1.609438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.7, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.7\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=7, subsample_freq=0 will be ignored. Current value: bagging_freq=7\n",
      "[CV] END actual_estimator__bagging_fraction=1.0, actual_estimator__bagging_freq=7, actual_estimator__feature_fraction=0.7, actual_estimator__learning_rate=0.01, actual_estimator__min_child_samples=11, actual_estimator__min_split_gain=0.2, actual_estimator__n_estimators=80, actual_estimator__num_leaves=60, actual_estimator__reg_alpha=0.0005, actual_estimator__reg_lambda=0.4; total time=  31.6s\n"
     ]
    }
   ],
   "source": [
    "tuned_best_subgrade_model_A = tune_model(best_subgrade_pycaret_model,\n",
    "                                         optimize='F1', tuner_verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e57bfc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_adaf7_row2_col0, #T_adaf7_row2_col1, #T_adaf7_row2_col2, #T_adaf7_row2_col3, #T_adaf7_row2_col4, #T_adaf7_row2_col5, #T_adaf7_row2_col6 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_adaf7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_adaf7_level0_col0\" class=\"col_heading level0 col0\" >Accuracy</th>\n",
       "      <th id=\"T_adaf7_level0_col1\" class=\"col_heading level0 col1\" >AUC</th>\n",
       "      <th id=\"T_adaf7_level0_col2\" class=\"col_heading level0 col2\" >Recall</th>\n",
       "      <th id=\"T_adaf7_level0_col3\" class=\"col_heading level0 col3\" >Prec.</th>\n",
       "      <th id=\"T_adaf7_level0_col4\" class=\"col_heading level0 col4\" >F1</th>\n",
       "      <th id=\"T_adaf7_level0_col5\" class=\"col_heading level0 col5\" >Kappa</th>\n",
       "      <th id=\"T_adaf7_level0_col6\" class=\"col_heading level0 col6\" >MCC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_adaf7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_adaf7_row0_col0\" class=\"data row0 col0\" >0.1547</td>\n",
       "      <td id=\"T_adaf7_row0_col1\" class=\"data row0 col1\" >0.0000</td>\n",
       "      <td id=\"T_adaf7_row0_col2\" class=\"data row0 col2\" >0.1547</td>\n",
       "      <td id=\"T_adaf7_row0_col3\" class=\"data row0 col3\" >0.1432</td>\n",
       "      <td id=\"T_adaf7_row0_col4\" class=\"data row0 col4\" >0.1462</td>\n",
       "      <td id=\"T_adaf7_row0_col5\" class=\"data row0 col5\" >0.1114</td>\n",
       "      <td id=\"T_adaf7_row0_col6\" class=\"data row0 col6\" >0.1115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_adaf7_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_adaf7_row1_col0\" class=\"data row1 col0\" >0.1560</td>\n",
       "      <td id=\"T_adaf7_row1_col1\" class=\"data row1 col1\" >0.0000</td>\n",
       "      <td id=\"T_adaf7_row1_col2\" class=\"data row1 col2\" >0.1560</td>\n",
       "      <td id=\"T_adaf7_row1_col3\" class=\"data row1 col3\" >0.1444</td>\n",
       "      <td id=\"T_adaf7_row1_col4\" class=\"data row1 col4\" >0.1474</td>\n",
       "      <td id=\"T_adaf7_row1_col5\" class=\"data row1 col5\" >0.1129</td>\n",
       "      <td id=\"T_adaf7_row1_col6\" class=\"data row1 col6\" >0.1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_adaf7_level0_row2\" class=\"row_heading level0 row2\" >Mean</th>\n",
       "      <td id=\"T_adaf7_row2_col0\" class=\"data row2 col0\" >0.1553</td>\n",
       "      <td id=\"T_adaf7_row2_col1\" class=\"data row2 col1\" >0.0000</td>\n",
       "      <td id=\"T_adaf7_row2_col2\" class=\"data row2 col2\" >0.1553</td>\n",
       "      <td id=\"T_adaf7_row2_col3\" class=\"data row2 col3\" >0.1438</td>\n",
       "      <td id=\"T_adaf7_row2_col4\" class=\"data row2 col4\" >0.1468</td>\n",
       "      <td id=\"T_adaf7_row2_col5\" class=\"data row2 col5\" >0.1121</td>\n",
       "      <td id=\"T_adaf7_row2_col6\" class=\"data row2 col6\" >0.1123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_adaf7_level0_row3\" class=\"row_heading level0 row3\" >Std</th>\n",
       "      <td id=\"T_adaf7_row3_col0\" class=\"data row3 col0\" >0.0007</td>\n",
       "      <td id=\"T_adaf7_row3_col1\" class=\"data row3 col1\" >0.0000</td>\n",
       "      <td id=\"T_adaf7_row3_col2\" class=\"data row3 col2\" >0.0007</td>\n",
       "      <td id=\"T_adaf7_row3_col3\" class=\"data row3 col3\" >0.0006</td>\n",
       "      <td id=\"T_adaf7_row3_col4\" class=\"data row3 col4\" >0.0006</td>\n",
       "      <td id=\"T_adaf7_row3_col5\" class=\"data row3 col5\" >0.0008</td>\n",
       "      <td id=\"T_adaf7_row3_col6\" class=\"data row3 col6\" >0.0008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17e979efd00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "    'actual_estimator__colsample_bytree': [0.9],\n",
    "    'actual_estimator__learning_rate': [0.15],\n",
    "    'actual_estimator__max_depth': [7],\n",
    "    'actual_estimator__min_child_weight': [3],\n",
    "    'actual_estimator__n_estimators': [290],\n",
    "    'actual_estimator__reg_alpha': [2],\n",
    "    'actual_estimator__reg_lambda': [0.7],\n",
    "    'actual_estimator__scale_pos_weight': [37.1],\n",
    "    'actual_estimator__subsample': [0.7]\n",
    "}\n",
    "\n",
    "tuned_best_subgrade_model = tune_model(best_subgrade_model, optimize='F1',\n",
    "                                       custom_grid=hyperparams, n_iter=1,\n",
    "                                       fold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee58b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Pipeline(memory=Memory(location=None),\n",
       "          steps=[('label_encoding',\n",
       "                  TransformerWrapperWithInverse(exclude=None, include=None,\n",
       "                                                transformer=LabelEncoder())),\n",
       "                 ('numerical_imputer',\n",
       "                  TransformerWrapper(exclude=None,\n",
       "                                     include=['Number_Trades_24_Months',\n",
       "                                              'Annual_Income', 'Total_Open_Buy',\n",
       "                                              'Number_Delinquent_2_Years',\n",
       "                                              'Delinquent_Amount',\n",
       "                                              'Debt_To_Income_Ratio',\n",
       "                                              'Employmen...\n",
       "                                importance_type=None,\n",
       "                                interaction_constraints=None, learning_rate=0.15,\n",
       "                                max_bin=None, max_cat_threshold=None,\n",
       "                                max_cat_to_onehot=None, max_delta_step=None,\n",
       "                                max_depth=7, max_leaves=None, min_child_weight=3,\n",
       "                                missing=nan, monotone_constraints=None,\n",
       "                                multi_strategy=None, n_estimators=290, n_jobs=-1,\n",
       "                                num_parallel_tree=None,\n",
       "                                objective='multi:softprob', ...))],\n",
       "          verbose=False),\n",
       " 'tuned_best_subgrade_model.pkl')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(tuned_best_subgrade_model, 'tuned_best_subgrade_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "77d1eb78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_3df9d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3df9d_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_3df9d_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_3df9d_level0_col2\" class=\"col_heading level0 col2\" >AUC</th>\n",
       "      <th id=\"T_3df9d_level0_col3\" class=\"col_heading level0 col3\" >Recall</th>\n",
       "      <th id=\"T_3df9d_level0_col4\" class=\"col_heading level0 col4\" >Prec.</th>\n",
       "      <th id=\"T_3df9d_level0_col5\" class=\"col_heading level0 col5\" >F1</th>\n",
       "      <th id=\"T_3df9d_level0_col6\" class=\"col_heading level0 col6\" >Kappa</th>\n",
       "      <th id=\"T_3df9d_level0_col7\" class=\"col_heading level0 col7\" >MCC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3df9d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_3df9d_row0_col0\" class=\"data row0 col0\" >Extreme Gradient Boosting</td>\n",
       "      <td id=\"T_3df9d_row0_col1\" class=\"data row0 col1\" >0.1621</td>\n",
       "      <td id=\"T_3df9d_row0_col2\" class=\"data row0 col2\" >0.7807</td>\n",
       "      <td id=\"T_3df9d_row0_col3\" class=\"data row0 col3\" >0.1621</td>\n",
       "      <td id=\"T_3df9d_row0_col4\" class=\"data row0 col4\" >0.1503</td>\n",
       "      <td id=\"T_3df9d_row0_col5\" class=\"data row0 col5\" >0.1525</td>\n",
       "      <td id=\"T_3df9d_row0_col6\" class=\"data row0 col6\" >0.1193</td>\n",
       "      <td id=\"T_3df9d_row0_col7\" class=\"data row0 col7\" >0.1195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17e908ded00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_unseen = predict_model(tuned_best_subgrade_model,\n",
    "                            pd.concat([X_test, y_test], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9952eea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
